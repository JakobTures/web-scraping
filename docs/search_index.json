[["index.html", "Web Scraping using R Introduction What is web scraping? Course contents Technical requirements Conventions Acknowledgements Colophon", " Web Scraping using R Jakob Tures 2023-10-16 Introduction This is an introduction to web scraping using R, designed to accompany a seminar held at the University of Potsdam in winter 2023/24. What is web scraping? Web scraping encompasses methods of data collection from online sources. This introduction will focus on scraping websites, but other forms on online data, e.g. databases or files, are scrapable as well. Instead of copying and pasting content from a browser window, in web scraping the process is automated through a programming language as R or Python. This is less error prone than manual collection and enables us to collect large amounts of data efficiently or regularly repeat the scraping process to update the data without investing much time. In web scraping your time is invested in planning the scraping process, programming your software and analysing the retrieved data – aka the fun stuff – rather than in time consuming and repetitive manual work. Web scraping enables us to access data, that we could not retrieve using traditional methods of data collection. Mainly because some types of data would not exist without the internet. Think of consumer reviews or social media posts. In other cases, web scraping enables us to collect data much more efficiently compared to traditional methods. Imagine you want to create a data file with demographic information of parliament members. This was obviously possible and done before web scraping became a possibility but included a lot of manual work using printed almanacs and typing the information into a table. Time consuming and error prone work. With web scraping we can set up scripts that gather this data directly from the website of a parliament including the possibility to rerun the collection process after the next election. But we don’t have to stop there. Maybe we want to analyse the sentiment of twitter messages written by parliament members by some socio-demographic variables as gender, age or educational background. We could retrieve both the tweets and the socio-demographic information using web scraping and analyse the gathered data in combination. At this point, your alarm bells should hopefully be ringing. Collecting data from social media profiles sounds like an Orwellian nightmare, and if it is done inconsiderate, it is. We should not and will not collect data for the sake of data collection. Before scraping data on the internet we have to think about what data we actually need to answer our specific research question. Next we have to check if we are actually allowed to collect certain types of data and even if we are, if we should collect it. Do we need the names of people or users if we are collecting data? Do we actually need all socio-demographic variables available or can we keep it at a minimum, ensuring that individuals in our data remain unidentifiable? We are not web spiders, collecting every bit of data that is available, whether we need it or not. We are scientists and have to behave in a responsible and reasonable manner when collecting data. Course contents Part I serves as an introduction to R, the programming language we will use for web scraping as well as analysis of the data gathered. This introduction can necessarily only cover the basics of R, as our focus lies on introducing the concepts of web scraping. The introduction to R will be problem-oriented, i.e.  we learn what we need for web scraping, but there will be a lot left out or only covered cursorily. See this as a starting point for your R journey, not its destination. Additional resources for deepening your R knowledge will be provided. Part II forms the main part of the course and introduces basic and intermediate web scraping concepts. First we have to understand the structure of websites before we can start picking out their specific parts. We will learn how to identify data of interest on HTML pages, select the data using CSS selectors and extract it using the R package rvest. We will also look at scraping tables, content generated by dynamic websites and scraping of multi-page websites. We close with a closer look on web scraping ethics and good practice, already briefly discussed above, and what we can do to make sure we scrape responsibly. Part III will cover the practical application of the acquired scraping knowledge to a small real world data analysis project, spanning the process from generating a research question over scraping appropriate data to cleaning and analysing the data with the goal of answering the formulated question. In this process, we will also deepen our R knowledge by looking at data cleaning, data transformation and graphical analysis in R using the tidyverse packages. Part IV is a basic outlook into regular expressions. These are used for handling, transforming and analysing text data, a data type we will commonly encounter in web scraping. Some starting points for working with regular expressions, as well as pointers to additional external resources, are given. The internet is an ever changing landscape. Where once every page we encountered was a simple HTML document, now many modern pages are less simplistic. Content is generated dynamically for each individual user and displayed information is not actually “present” in the HTML files but loaded through JavaScript or database connections. This also makes scraping harder. While still being possible, this requires advanced techniques that go beyond the scope of this introduction. You will still receive some pointers for where to look, should you want to follow these paths further. Also, when we begin scraping raw text from websites, we will need to learn about advanced handling of strings and regular expressions, which can also only be covered briefly here. This introduction focuses on using rvest for scraping in R. This will suffice for many intermediate and advanced scraping projects, but alternatives exist within R and in other languages. While these also can not be covered here, you will gain a solid basis to pursue alternatives, should you so desire. Technical requirements All scraping in this introduction is done in R and RStudio using the tidyverse packages. Information on how to install the software and handle packages will be provided in chapters 1 and 2. We will also need a browser for scraping. While in principle any popular browser should do, I would recommend using Chrome or Chromium, as all examples using the build in Web Developer Tools, are done in Chromium. You should make sure, that the URL bar in your browser shows the full URLs. In Chrome/Chromium this is done by right-clicking the URL bar and choosing “Always show full URLs” in the context menu. While not strictly necessary, I would also advice using a decent text editor for opening HTML files. The options that come with your operating systems work, but why not do yourselves a favour and use something you can actually work with. Options are aplenty, but some of the more accessible while still powerful text editors I personally would recommend are Notepad++ on Windows (https://notepad-plus-plus.org/) and Atom on all operating systems (https://atom.io/). If your feel you are up to it, you can also dip your feet into Emacs, also available for every OS (https://www.gnu.org/software/emacs/), but note that this is an advanced option. Proceed at your own risk. Conventions All code on the website is set in this code font. This is true for R as well as HTML code. R code that is included in text paragraphs will not always be runnable and often serve illustrative purposes. All R code written in code blocks is runnable and look like this: print(&quot;Hello World!&quot;) ## [1] &quot;Hello World!&quot; If an R command produces relevant output, you will see the output following directly after the command, written in red and introduced by ##. You should in general try to run the code blocks yourself in RStudio. The only way to learn what is happening in R and web scraping, is doing it yourself. Run the examples and experiment with them. Every problem you will run into while changing the code is a learning opportunity. Notice, that code blocks are copyable. Mouse over the code block and you will see a copy symbol, or simply copy &amp; paste like you are used to. But, I would strongly advice on writing as much code as you can yourself. Copying code examples by hand may not be intellectually challenging, but if your aim is to learn the R functions and syntax you have to write a lot of it before it can become second nature. Names of packages are written in bold and exactly as they are named, e.g. tidyverse. Acknowledgements I would like to thank the authors of bookdown, the tidyverse and all R packages used in the creation of this course. Very special thanks go to Lukas Höttges for his support in creating the website and in organizing the seminar at the University of Potsdam. To Hannah Gehrmann for the invaluable feedback on an early draft. To Fabian Class for technical support and numerous answers to my many questions. And to Sophia Varma for translating the original German draft and the great support. Colophon The website was built with: sessioninfo::session_info() ## ─ Session info ─────────────────────────────── ## setting value ## version R version 4.3.1 (2023-06-16) ## os Ubuntu 22.04.3 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Europe/Berlin ## date 2023-10-16 ## pandoc 3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────── ## package * version date (UTC) lib source ## bookdown 0.34 2023-05-09 [1] CRAN (R 4.3.1) ## bslib 0.5.0 2023-06-09 [1] CRAN (R 4.3.0) ## cachem 1.0.8 2023-05-01 [1] CRAN (R 4.3.0) ## cli 3.6.1 2023-03-23 [1] CRAN (R 4.3.0) ## colorspace 2.1-0 2023-01-23 [1] CRAN (R 4.3.0) ## digest 0.6.31 2022-12-11 [1] CRAN (R 4.3.0) ## dplyr * 1.1.2 2023-04-20 [1] CRAN (R 4.3.0) ## echarts4r * 0.4.5 2023-06-16 [1] CRAN (R 4.3.1) ## ellipsis 0.3.2 2021-04-29 [1] CRAN (R 4.3.0) ## evaluate 0.21 2023-05-05 [1] CRAN (R 4.3.0) ## fansi 1.0.4 2023-01-22 [1] CRAN (R 4.3.0) ## fastmap 1.1.1 2023-02-24 [1] CRAN (R 4.3.0) ## forcats * 1.0.0 2023-01-29 [1] CRAN (R 4.3.0) ## generics 0.1.3 2022-07-05 [1] CRAN (R 4.3.0) ## ggplot2 * 3.4.2 2023-04-03 [1] CRAN (R 4.3.0) ## glue 1.6.2 2022-02-24 [1] CRAN (R 4.3.0) ## gtable 0.3.3 2023-03-21 [1] CRAN (R 4.3.0) ## hms 1.1.3 2023-03-21 [1] CRAN (R 4.3.0) ## htmltools 0.5.5 2023-03-23 [1] CRAN (R 4.3.0) ## htmlwidgets 1.6.2 2023-03-17 [1] CRAN (R 4.3.0) ## httpuv 1.6.11 2023-05-11 [1] CRAN (R 4.3.0) ## httr 1.4.6 2023-05-08 [1] CRAN (R 4.3.0) ## jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.3.0) ## jsonlite 1.8.5 2023-06-05 [1] CRAN (R 4.3.0) ## knitr * 1.43 2023-05-25 [1] CRAN (R 4.3.0) ## later 1.3.1 2023-05-02 [1] CRAN (R 4.3.0) ## lifecycle 1.0.3 2022-10-07 [1] CRAN (R 4.3.0) ## lubridate * 1.9.2 2023-02-10 [1] CRAN (R 4.3.0) ## magrittr 2.0.3 2022-03-30 [1] CRAN (R 4.3.0) ## mime 0.12 2021-09-28 [1] CRAN (R 4.3.0) ## munsell 0.5.0 2018-06-12 [1] CRAN (R 4.3.0) ## pillar 1.9.0 2023-03-22 [1] CRAN (R 4.3.0) ## pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 4.3.0) ## promises 1.2.0.1 2021-02-11 [1] CRAN (R 4.3.0) ## purrr * 1.0.1 2023-01-10 [1] CRAN (R 4.3.0) ## R6 2.5.1 2021-08-19 [1] CRAN (R 4.3.0) ## Rcpp 1.0.10 2023-01-22 [1] CRAN (R 4.3.0) ## readr * 2.1.4 2023-02-10 [1] CRAN (R 4.3.0) ## rlang 1.1.1 2023-04-28 [1] CRAN (R 4.3.0) ## rmarkdown 2.22 2023-06-01 [1] CRAN (R 4.3.0) ## rstudioapi 0.14 2022-08-22 [1] CRAN (R 4.3.0) ## rvest * 1.0.3 2022-08-19 [1] CRAN (R 4.3.0) ## sass 0.4.6 2023-05-03 [1] CRAN (R 4.3.0) ## scales 1.2.1 2022-08-20 [1] CRAN (R 4.3.0) ## sessioninfo 1.2.2 2021-12-06 [1] CRAN (R 4.3.0) ## shiny 1.7.4 2022-12-15 [1] CRAN (R 4.3.0) ## stringi 1.7.12 2023-01-11 [1] CRAN (R 4.3.0) ## stringr * 1.5.0 2022-12-02 [1] CRAN (R 4.3.0) ## tibble * 3.2.1 2023-03-20 [1] CRAN (R 4.3.0) ## tidyr * 1.3.0 2023-01-24 [1] CRAN (R 4.3.0) ## tidyselect 1.2.0 2022-10-10 [1] CRAN (R 4.3.0) ## tidyverse * 2.0.0 2023-02-22 [1] CRAN (R 4.3.0) ## timechange 0.2.0 2023-01-11 [1] CRAN (R 4.3.0) ## tzdb 0.4.0 2023-05-12 [1] CRAN (R 4.3.0) ## utf8 1.2.3 2023-01-31 [1] CRAN (R 4.3.0) ## vctrs 0.6.3 2023-06-14 [1] CRAN (R 4.3.0) ## withr 2.5.0 2022-03-03 [1] CRAN (R 4.3.0) ## xfun 0.39 2023-04-20 [1] CRAN (R 4.3.0) ## xml2 1.3.4 2023-04-27 [1] CRAN (R 4.3.0) ## xtable 1.8-4 2019-04-21 [1] CRAN (R 4.3.0) ## yaml 2.3.7 2023-01-23 [1] CRAN (R 4.3.0) ## ## [1] /home/jtures/R/x86_64-pc-linux-gnu-library/4.3 ## [2] /usr/local/lib/R/site-library ## [3] /usr/lib/R/site-library ## [4] /usr/lib/R/library ## ## ────────────────────────────────────────────── "],["R1.html", "1 R &amp; R Studio 1.1 Installing R 1.2 RStudio 1.3 Hello World! 1.4 Objects 1.5 Vectors 1.6 Functions 1.7 RStudio Workflow", " 1 R &amp; R Studio 1.1 Installing R R is a freely available programming language used predominantly for data science and statistical computations. For more information on the language and to access the documentation, visit: https://www.r-project.org/. From there you can also follow the link to CRAN, the Comprehensive R Archive Network, or access it directly by visiting https://cran.r-project.org/. The latest versions of R will always be hosted at CRAN. At the top of the landing page, you will find links to the installers for each operating system. If you are using Windows, please choose “base” after following the link and then download the offered file. In the case of Mac OS download the first file listed under “Latest release”. In both cases, execute the file and install R to a directory of your choice. If you are using Linux, the link on CRAN will offer installation advice for some of the more popular distributions. In any case, you can check the package manager of your choice for the latest available release for your system. All examples used on this Website were written and tested with R version 4.1.3 “One Push-Up”. While it is not to be expected, they might, nonetheless, return errors in newer or older versions of R. 1.2 RStudio The basic R installation provides a terminal that could in principle be used to follow the contents of this introduction to Web Scraping. The widely more common approach is to use an external IDE – Integrated Development Environment –, the most popular being RStudio. Using an IDE will dramatically improve your workflow and I would strongly recommend using RStudio for this purpose. RStudio is also freely available and can be found at https://www.rstudio.com/. Following the “Download” link and scrolling down (ignoring the different versions offered for professional usage) you will find the latest installers for several operating systems offered as downloads. In most cases, simply installing RStudio after R has been installed, will work “out of the box”. 1.2.1 Overview The RStudio interface consists of four sub-areas. The bottom-left shows the “Console” as well as additional tabs which you will rarely need in the beginning. The console can be used to evaluate R code live. We will begin working with the console soon, so this will make more sense to you in a bit. The top-left shows opened files, e.g. R scripts. This is where you will actually spend most of your time. This introduction proceeds from using one-time commands in the console to writing your code in scripts that can be re-opened, re-run and shared. The top-right has several tabs of which “Environment” should be our main concern at this point. Here you will see all data objects created in your RStudio session. More on this later. Finally, the bottom-right shows us, amongst other things, the “Files” in a selected folder, graphical output under “Plots” and requested “Help” on packages and functions. 1.3 Hello World! So, let’s begin with putting R &amp; RStudio to use. For now, we will write our commands directly into the console. You will notice a &gt; sign at the beginning of the last line in the console. This is a prompt, as in “write commands here”. Try writing this command and executing it with the “Enter” key: print(&quot;Hello World!&quot;) ## [1] &quot;Hello World!&quot; You just entered your first R command, received your first output and also used your first function. We will address functions in more detail later. For now, it is enough to know that the command print() prints everything that is enclosed in its parentheses to the output. The output begins with [1], indicating that this is the first, and in this case the only, element of the output generated by the executed command. Please note, that RStudio will not print ## before the output. In the shown code segments on this website, ## is inserted before the output to allow copying the code directly to RStudio, as one or mutliple # indicate that a line is a comment, and thus is not evaluated as a command by R. 1.3.1 Calculating with R R understands the basic arithmetic symbols + - * / and thus the console can be used as a calculator. Many functions for more involved calculations, e.g.  sqrt() for taking a square root of the content enclosed in the parenthesis, are available. x^y can be used to write \\(x^y\\). For now, you should write the code below line for line into the R console and execute each line with the “Enter” key. 17 + 25 ## [1] 42 99 - 57 ## [1] 42 4 * 10.5 ## [1] 42 84 / 2 ## [1] 42 sqrt(1764) ## [1] 42 6.480741 ^ 2 ## [1] 42 1.3.2 Comparison operators We can use comparison operators to compare two values and receive the test result as output. To test if two values are equal, we write ==. To test if they are not equal, we can use != 42 == 42 ## [1] TRUE 42 != 42 ## [1] FALSE We can also compare if the first value is less &lt;, less or equal &lt;=, larger &gt; or larger or equal &gt;=, compared to the second value. 10 &lt; 42 ## [1] TRUE 42 &lt;= 42 ## [1] TRUE 10 &gt; 42 ## [1] FALSE 90 &gt;= 42 ## [1] TRUE 1.3.3 Logical operators We can also combine tests by applying logical operators to create more complex conditions. &amp; – AND – checks if both tests combined by it are TRUE and only returns TRUE if both are at the same time. | – OR – checks if at least on of the tests is TRUE and returns TRUE if one or both are. When combining two tests, we thus have these possibilities: TRUE &amp; TRUE returns TRUE TRUE &amp; FALSE, FALSE &amp; TRUE and FALSE &amp; FALSE all return FALSE TRUE | TRUE, TRUE | FALSE and FALSE | TRUE all return TRUE FALSE | FALSE returns FALSE 1 &lt; 2 &amp; 3 != 4 # TRUE &amp; TRUE ## [1] TRUE 1 &gt; 2 &amp; 3 != 4 # FALSE &amp; TRUE ## [1] FALSE 1 &gt; 2 | 3 != 4 # FALSE | TRUE ## [1] TRUE 1 &gt; 2 | 3 == 4 # FALSE | FALSE ## [1] FALSE 1.4 Objects Some of the power of using a language like R for computation, comes from the ability to store data or results for later use and further analysis. In R, all types of data are stored in objects. On a basic level, an object is a name that we define that has some form of data assigned to it. To assign data to a name, we use the assignment operator &lt;-. the_answer &lt;- 42 A handy keyboard shortcut for writing the assignment operator is pressing the “Alt” and “-” keys simultaneously. Learning this shortcut early, will safe you on a lot of typing and keyboard gymnastics. After we assigned a value to an object, we can recall that value, by writing the object’s name. the_answer ## [1] 42 We can also use defined objects in calculations and function calls (more on those later). Note, that if we assign a value to an already defined object, the stored value is overwritten by the new one. the_answer &lt;- the_answer / 2 the_answer ## [1] 21 a &lt;- 17 b &lt;- 4 the_answer &lt;- (a + b) * 2 the_answer ## [1] 42 All objects we define are listed in the “Environment” tab, seen in the upper right of RStudio. If we ever want to remove objects from the environment, we can use the rm() function. In general, this is not necessary, but it can help with keeping the list from getting cluttered. rm(the_answer) 1.5 Vectors When we assigned a number to an object, we actually created a vector. A vector is a one-dimensional data structure that can contain multiple elements. The number of elements determine the length of the vector. So a vector with only one element is still a vector, but with a length of 1. To assign multiple elements to a vector, we use the combine function c(). All values inside the parentheses, separated by ,, are combined as elements to form the vector. v &lt;- c(7, 8, 9) v ## [1] 7 8 9 1.5.1 Subsetting If we want to access certain elements of a vector, we have to use subsetting. This is achieved by adding square brackets to the object’s name, containing the position of the element in its vector. In order to access the first or third element, we can write: v[1] ## [1] 7 v[3] ## [1] 9 We can also access multiple elements at once, using c() inside the brackets or by defining a range of positions using :. v[c(1, 3)] ## [1] 7 9 v[2:3] ## [1] 8 9 1.5.2 Types of vectors Observing the vector v we created in the environment, we notice that RStudio writes num [1:3] before listing the values of the elements. The second part, indicates the length of 3, while the first part shows the type of the vector we created. In this case the type is numeric. Numeric vectors, as you might have guessed, contain numbers. We can also use str() to receive info on type, length and content of a vector. str(v) ## num [1:3] 7 8 9 There are a number of other types of vectors, the two most important – besides numeric vectors – being logical and character vectors. Logical vectors can only contain the values TRUE and FALSE. Strictly speaking, they – as the other types of vectors – can also contain NA, indicating a missing value. We will talk more about NAs later on. Logical vectors are often created when we test for something. For example, we can test, if the elements in a numerical vector are larger or equal to 5 and receive a logical vector containing the test results. x &lt;- c(1, 7, 3, 5) x &gt;= 5 ## [1] FALSE TRUE FALSE TRUE Character vectors contain strings of characters. When assigning strings, they have to be enclosed in quotation marks. char_v &lt;- c(&quot;This&quot;, &quot;is&quot;, &quot;a&quot;, &quot;character&quot;, &quot;vector!&quot;) char_v ## [1] &quot;This&quot; &quot;is&quot; &quot;a&quot; &quot;character&quot; &quot;vector!&quot; char_v_2 &lt;- c(&quot;This is also&quot;, &quot;a character vector!&quot;) char_v_2 ## [1] &quot;This is also&quot; &quot;a character vector!&quot; We can compare character vectors only for (non-)equality, not for being smaller or larger. &quot;same&quot; == &quot;same&quot; ## [1] TRUE &quot;same&quot; == &quot;not the same&quot; ## [1] FALSE &quot;same&quot; != &quot;not the same&quot; ## [1] TRUE Character vectors also cannot be used to calculate. This can get problematic, if numbers are stored as characters, which arises frequently when Web Scraping. a &lt;- c(1, 2, 3) b &lt;- c(&quot;7&quot;, &quot;8&quot;, &quot;9&quot;) str(a) ## num [1:3] 1 2 3 str(b) ## chr [1:3] &quot;7&quot; &quot;8&quot; &quot;9&quot; a + b ## Error in a + b: non-numeric argument to binary operator As we enclosed the elements of vector b in quotation marks, R interprets the data as characters instead of numbers. Since characters cannot be used for calculations, we received an error message. But we can make R interpret the characters as numbers by using as.numeric(). a + as.numeric(b) ## [1] 8 10 12 1.5.3 A brief look at lists Note that a vector of a certain type, can only contain elements of that type. So we cannot mix data types in the same vector. If we want to mix data types, we can use lists instead of vectors. l &lt;-list(1, TRUE, &quot;Hello World!&quot;) str(l) ## List of 3 ## $ : num 1 ## $ : logi TRUE ## $ : chr &quot;Hello World!&quot; Lists can also contain other lists to represent hierarchical data structures. We will see lists “in action” later on in this course. 1.6 Functions Functions provide an easy and concise way of performing more or less complex tasks using predefined bits of R code that are provided in “base R” – i.e. that come with the basic R installation – or in the various additional packages that are available for installation. We have already used a number of functions up to this point, e.g. print(). To “call” a function, we write its name, followed by parentheses. Inside the parentheses additional arguments are provided to R. In most cases, some data has to be entered as the first argument. For example, print() writes the text provided as argument to the output. More complex functions often allow for more than one argument. Sometimes these are required, but more often these additional arguments are optional and can be used to change some options from the default value to the one desired. 1.6.1 Help But how do we know which arguments can or have to be provide to use a function and what their effects are? We can check the documentation on CRAN or use Google to find additional information. Another often more convenient way, is to use the help functionality build into R. By writing ? in front of the function name into the console and executing the line by pressing “Enter”, the help file is opened in the lower right of the RStudio window. Let’s try this for the function rnorm(). ?rnorm() The help file tells us several things. rnorm() is part of a family of functions that are related to the normal distribution, each providing a distinct functionality. The functionality of rnorm() being the generation of random numbers stemming from the normal distribution. We also learn, that three arguments can be provided. n, the number of observations to be generated, as well as mean and sd, the mean and the standard deviation of the normal distribution to be drawn from. We also see that mean and sd are provided with the standard values 0 and 1 respectively, indicated by the =. We also see that n has no standard value. So we have to provide a value for n, but not for mean and sd. Just writing rnorm() will result in an error. rnorm() ## Error in rnorm(): argument &quot;n&quot; is missing, with no default To provide an argument to a function, we write the name of the argument, followed by = and the value to be provided. Note that, since rnorm() draws random numbers, your output will differ from the output presented here. rnorm(n = 10) ## [1] -0.4825981 -0.1270875 -1.3090852 0.7795997 0.2651731 0.8898682 ## [7] 0.4773540 0.1411927 0.3071022 -0.8846552 In the same vein, additional arguments that are allowed by the function can be defined, instead of using their default values. rnorm(n = 10, mean = 10, sd = 0.5) ## [1] 9.403351 10.580990 10.447558 10.223672 9.916762 9.357506 10.537004 ## [8] 8.619759 10.252021 9.831583 We can also skip writing the names of arguments in many cases. As the n argument is the first listed in the function’s parentheses, R also understands the call, if we just provide the value to be used as the first argument. You will often encounter the convention that the first argument is written without its name and any further arguments are written in full. rnorm(10, mean = 10, sd = 0.5) ## [1] 10.155440 8.682634 10.205253 9.339411 10.166834 10.121625 9.889124 ## [8] 9.715910 10.972416 9.446602 1.6.2 Examples: Basic statistical functions Base R provides us with some basic statistical functions that are used for data analysis. We should start with defining a numerical vector that contains some data to be analysed. data &lt;- c(4, 8, 15, 16, 23, 42) We could be interested in describing this data by its arithmetic mean, median and standard deviation. For this purpose we can use the functions mean(), median(), and sd() provided by base R. All three do not require additional arguments besides the data to be analysed which we can provide using the object data we created beforehand. mean(data) ## [1] 18 median(data) ## [1] 15.5 sd(data) ## [1] 13.49074 1.7 RStudio Workflow Up until now, we wrote our code directly into the RStudio console, pressed “Enter” and received the desired output. This works but will not satisfy our needs in the long run. The main problem is, that the code we wrote essentially disappears after running it. Imagine that you want to rerun your code a week from now or even tomorrow. Maybe you took notes and can recreate it, but that means a lot of unsatisfying and error prone work. Also, maybe at some point you want to share code with colleagues, fellow students, or the R community in general. At the same time, as our code gets more complex, spans multiple lines and consists of many interdependent blocks of code, you will inevitably run into the situation where you realise you made a mistake or have to change some code at the very beginning of your R session. This would mean, recreating and rerunning most or all of the code you have already written. These are some of the reasons why we should start writing our code into so called R Scripts. 1.7.1 R Scripts To create a new R Script, you can click on “File” &gt; “New File” &gt; “R Script”, or more conveniently press “CTRL” + “Shift” + “N” simultaneously. This creates an untitled script that we can write our code into. Let’s start with something simple by recreating some of the code from last week. a &lt;- 17 b &lt;- 4 the_answer &lt;- (a + b) * 2 the_answer ## [1] 42 We assign two numerical values two the objects a and b, assign a calculation based on these objects to the new object the_answer and prompt R to return its value to us. Instead of writing the code line by line into the console, now we write the whole block into the newly created script. We can now run the complete script by clicking on “Source” or, to also get output, “Source with Echo” in the upper toolbar attached to the script’s tab. In most cases I prefer running the script line by line though. This allows full control of the process and enables you to stop in certain lines to e.g. contemplate what the code is doing, check for errors or change details of the code before moving on to the next line. You can do this either by clicking on “Run” in the toolbar or pressing “CTRL” + “Enter” simultaneously. In both cases, RStudio copies the line of code where your text cursor is currently residing into the console and runs it for you. The text cursor then conveniently jumps to the next line in the script. In this way you can quickly run your script line by line, while having full control over when to stop. You can decide for yourself what the right approach to running your code is, based on any given situation. But remember that R always assumes that you know what you’re doing. There will be no warning prompts if you are about to overwrite work you have previously done. When you are done writing your script, you might want to save it to the hard drive, preserving your work for later re-runs or for sharing. By clicking on “File” &gt; “Save” or presing “CTRL” + “S” you can save the file with a name of your choosing. The file extension for R Scripts is always “.R”. One problem – that you will run into sooner or later – is that you will try to run incomplete code from a script, most commonly a missing closing bracket. In this case, RStudio puts the code to be run into the console and begins a new line, starting with +, and then nothing happens. R assumes that your code will continue in a further line and waits for you to enter it after the +. In most cases the right approach is to cancel the entered command, fix your code and re-run it afterwards. To cancel an already entered command, you have to click into the “Console” tab and press “Esc” on your keyboard. The &gt; prompt will reappear in the console and you can continue with your work. 1.7.2 Projects In many cases, your work will consist of multiple scripts, data files, graphics saved to the disk or additional output. So it makes sense to assign your files to a place on your hard drive. You can do this “by hand” but a convenient approach might be to use RStudio’s project functionality. By clicking on “File” &gt; “New Project”, you can start the project creation wizard. If you have already created a folder on your hard drive that shall contain the project, you can click on “Existing Directory”, select the folder and click on “Create Project”. You can also create the folder on the fly by clicking on “New Directory” &gt; “New Project” and then choosing a folder name and the sub-folder where it should be placed, before creating the project. RStudio will now close all files currently open and switch to your newly created project. The name you chose for the project’s folder will also be its name, seen in RStudio’s title bar. When you look at the “File” tab (lower right), you will also see that you are now in the project’s folder. This is your current working directory, a concept we will talk about momentarily. All scripts you create while working in your project will become a part of it. So when you want to return to continuing your work, you can now click on “File” &gt; “Open Project”. All files opened the last time you worked on the project will be reopened and you will again be in the project’s working directory. This is an easy and convenient way to keep your work tidy. At this point, I would advise you to create a project for this introduction to web scraping and create R scripts for each chapter as parts of the project. The name and sub-folder you choose is not important from the point of view of functionality, but it should make sense to you. We should now briefly talk about the working directory. If you try to open or save a file directly from an R script – without specifying a complete path – R will always assume you refer to your working directory. If you created a project, this automatically set the project’s folder as the working directory. You can always check for your current working directory by entering getwd() into the console. You can change your current working directory by clicking on “Session” &gt; “Set Working Directory” &gt; “Choose Directory…” or by using the function setwd() with the desired path enclosed by \" as the function’s argument. 1.7.3 Comments You should get into the habit of commenting your code as early as possible. Comments are started with one or multiple #. All code following the # will not be evaluated by R and thus serves as the perfect place to comment on what you were doing and thinking while writing the code. Why do this? When you reopen a script that you have not been working on in a while, it can be hard to understand what you tried to do in the first place. Commented code makes this much easier. This is even more true if you share your code with other people. They may have very different approaches to certain R problems and clearly commented code will help them to quickly understand it. You should see this as a sign of respect towards the time your peers may invest in helping you with your coding problems. # assigning objects a &lt;- 17 b &lt;- 4 # calculating the answer the_answer &lt;- (a + b) * 2 the_answer ## [1] 42 # but what is the question? If you plan on using setwd() in your script, it is a good idea to comment this line before sharing your script. Other people will have different folder structures and will want to decide for themselves. The same goes for all lines that will save something to the hard drive, e.g. data sets or exported graphics. The R and RStudio communities are very welcoming and you will always find people that are willing to lend you their help, so you should return the favour and be polite in your code. This includes writing clear comments and not cluttering anyone’s hard drive with files they may not want to have. "],["R2.html", "2 Packages, the tidyverse and R Markdown 2.1 R packages 2.2 Tidyverse 2.3 Additional R resources 2.4 Intro to R Markdown", " 2 Packages, the tidyverse and R Markdown 2.1 R packages The R world is open and collaborative by nature. Besides the packages that come with your R installation – base R – an ever growing number of additional packages, written by professionals and users, is available for download by anyone. Every package is focussed on a specific use case and brings with it a number of functions that enable R to be used for tasks that the original software designers did not have in mind or at the very least provide a smoother user experience in cases where the original base R solutions are more complicated. The packages, its documentation and various other related information are hosted at CRAN – “Comprehensive R Archive Network”– which you already got to know during the installation of R. If you install a package directly from RStudio, it uses CRAN to find and download the package and the associated files. 2.1.1 Installing and loading packages To install a package we can use the R function install.packages() where the name of the package to be installed is written enclosed by \" between the parentheses. Normally we do this using the console. Installing packages from an R script works as well, but as we only need to perform the installation once, there is no benefit in it. It actually slows things down if we repeat the installation every time we run a script. At the same time, if we share our script, it is impolite to force an (re)installation on somebody else. For this introduction we will focus on the packages of the tidyverse – more on them below. To install the core tidyverse package, you should type: install.packages(&quot;tidyverse&quot;) R will output a lot of information concerning the installation process, and close with a satisfying DONE (tidyverse) if everything went according to plan. Please note, that R is case sensitive. This means that “Tidyverse” is not the same as “tidyverse”. To R, these are to different words. When installing packages, their names have to be written exactly as they are named or R will not find the package. The same principle applies to object names, strings, functions, arguments and work with R in general. Now that the installation is complete, we can load the package. This should normally be done in the first lines of a script. This way all necessary packages are loaded at the beginning of running a script and other users that see your code also immediately see which packages are required. Loading a package is done with library() with the name of the package in the parentheses, this time without the need for enclosing it in \". library(tidyverse) Loading the tidyverse package returns a lot of information to us, some of which we will look at in more detail during the course of this chapter. Please note that not all packages are that verbose in their loading process. Often you will get no output at all which is a good sign, as this also means that the package loaded correctly. If anything goes wrong, R will return an error message. 2.1.2 Namespaces Looking at the last lines of the returned message when loading the tidyverse package, we’re informed that there are two conflicts. These arise when two or more loaded packages include functions with the same name. Here we can see that the tidyverse package dplyr masks the functions filter() and lag() from the base R package stats. If we would have used filter() without loading dplyr, the function from the stats package would have been used. After loading it, the function from dplyr masks the function from stats and is used instead. If we had a case where we want to load dplyr, but still use filter() from stats, we can still do this by explicitly declaring the namespace which we are referring to. The namespace basically is a reference for R where to look up the function we have called. If we just write the function’s name, R looks for it in the list of loaded packages, which would result in applying filter() from dplyr here. But we can tell R to look up the function in another namespace, by using the notation namespace::function. So to call filter() from stats while the function is masked by the similarly named function from dplyr, we could write stats::filter(). As the function will not work without further arguments, we can’t try this out directly, but the same principle applies to loading the help files: ?dplyr::filter() ?stats::filter() 2.2 Tidyverse While we will use some base R functions throughout this course, our main focus will lie on the tidyverse packages. The tidyverse is a collection of R packages, all following a shared philosophy concerning the syntax of their functions and the way in which data is represented. We will see how the philosophy underlying the tidyverse can lead to more intuitive R code, especially when using the pipe (%&gt;%), in the next chapter. If you want to learn more about the concept of tidy data, the structure of data representation underlying the tidyverse, a read of the chapter on this concept from “R for Data Science” by Wickham &amp; Grolemund is highly recommended: https://r4ds.had.co.nz/tidy-data.html. Right now, the core tidyverse consists of eight packages. These are the packages that are loaded when we type library(tidyverse) and that are listed in the corresponding output under “Attaching packages”. As the name suggests, the packages comprise the core functionalities that define the tidyverse. This includes reading, cleaning and transforming data, handling certain data types, plotting graphs and more. Over the course of this introduction to web scraping, we will make use of several of these packages, so in most chapters we will begin our scripts with loading the tidyverse package. Besides the core tidyverse, a number of additional and more specialised packages are part of the tidyverse and were already installed when you ran install.packages(\"tidyverse\") above. Among them, the package rvest is of special importance to us, as it will be our main tool for web scraping throughout the course. For a full list of tidyverse packages and the corresponding descriptions of their functionality, you can visit: https://www.tidyverse.org/packages/ 2.2.1 Tibbles The tibble package is part of the core tidyverse and offers an alternative to data frames that are used in base R to represent data in tabular form. The differences between data frames and tibbles are relatively minor. If you are interested in the details, you can read up on them in this section from “R for Data Science” and the chapter on tibbles in general: https://r4ds.had.co.nz/tibbles.html#tibbles-vs.-data.frame. For now, it will suffice to know that tibbles are used throughout this introduction, but that all examples will also work with the classic data frames. The syntax to create a tibble is simple. Every column represents a variable, every row an observation. You should think of the columns as vectors, where the first position in each vector corresponds to the first observation (row), the second position in each vector to the second observation, and so on. In this way, we can create tibbles vector by vector or variable by variable, using the function tibble(). We assign a name to the variable followed by = and the data to be assigned to the variable. The variable-data pairs are separated by ,: tibble(numbers = c(0, 1, 2), strings = c(&quot;zero&quot;, &quot;one&quot;, &quot;two&quot;), logicals = c(FALSE, TRUE, TRUE)) ## # A tibble: 3 × 3 ## numbers strings logicals ## &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 0 zero FALSE ## 2 1 one TRUE ## 3 2 two TRUE For longer code like this, it is advisable to use multiple lines and a more clear formatting to create code that is readable and intuitive: tibble( numbers = c(0, 1, 2), strings = c(&quot;zero&quot;, &quot;one&quot;, &quot;two&quot;), logicals = c(FALSE, TRUE, TRUE) ) ## # A tibble: 3 × 3 ## numbers strings logicals ## &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 0 zero FALSE ## 2 1 one TRUE ## 3 2 two TRUE R understands that all five lines are part of one command as it evaluates everything between the opening and closing bracket of the tibbles() function together. We just have to make sure, that we don’t miss the closing bracket or a , that separates the variable-data pairs. This actually is a main source of errors and will be high on your list of things to check if something does not work as planned. We can also use calculations and functions directly in tibble creation, circumventing the need to assign the results to an object first: tibble( numbers = c(1, 2, 3), roots = sqrt(numbers), rounded = round(roots) ) ## # A tibble: 3 × 3 ## numbers roots rounded ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 ## 2 2 1.41 1 ## 3 3 1.73 2 2.2.1.1 Subsetting tibbles When subsetting two dimensional objects like data frames and tibbles, we have to supply an index for the row(s) as well as for the column(s) we want to subset. Those are written in the form object[row_index, column_index] Let us first assign a sample tibble to an object. exmpl_tbl &lt;- tibble( numbers = c(0, 1, 2), strings = c(&quot;zero&quot;, &quot;one&quot;, &quot;two&quot;), logicals = c(FALSE, TRUE, TRUE) ) We can now subset this tibble. Note that if we want to subset a complete row or column, we can leave the place before or after the , empty to indicate that we want to see all rows or columns. exmpl_tbl[1, 2] # first row, second column ## # A tibble: 1 × 1 ## strings ## &lt;chr&gt; ## 1 zero exmpl_tbl[1, ] # first row, all columns ## # A tibble: 1 × 3 ## numbers strings logicals ## &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 0 zero FALSE exmpl_tbl[, 2] # all rows, second column ## # A tibble: 3 × 1 ## strings ## &lt;chr&gt; ## 1 zero ## 2 one ## 3 two You may note that in each of those cases, R returns a tibble to us. Even when there is only one value, as in the first example, we get a 1x1 tibble. This is because subsetting tibbles with [] always returns a tibble. If we are interested in extracting an actual value from a cell in a tibble, we have to use [[]] subsetting instead. exmpl_tbl[[1, 2]] # first row, second column ## [1] &quot;zero&quot; exmpl_tbl[[3, 1]] # third row, first column ## [1] 2 If our goal is to extract a column as a vector, we have several options. We can write object[[column_index]]. In this case, R knows that we are only interested in columns as we have only provided one index to a two dimensional object. We couls also use the column’s name instead of the index, enclosed in \"\". Another popular option is to use the \\(-notation. Here we also use the column&#39;s name but write it like this: `object\\)column_name`. Which option to use is up to taste, as the results are identical: exmpl_tbl[[2]] ## [1] &quot;zero&quot; &quot;one&quot; &quot;two&quot; exmpl_tbl[[&quot;strings&quot;]] ## [1] &quot;zero&quot; &quot;one&quot; &quot;two&quot; exmpl_tbl$strings ## [1] &quot;zero&quot; &quot;one&quot; &quot;two&quot; 2.3 Additional R resources When learning R and when using functions and packages that are new to you, you will regularly run into situations where you need help in understanding what is happening and what you can do. Luckily, there a lot of resources that will help you on your R journey. You have already learned about the built-in help functionalities of R. Many packages also come with so called vignettes which offer more in-depth introductions to the packages. Let’s see if the tibble package comes with vignettes. To do this we can write: vignette(package = &quot;tibble&quot;) We get a list of all vignettes available for the specific package. To access a specific vignette, we also use the vignette() function, this time with the specific name of the vignette as the function’s argument: vignette(&quot;types&quot;) You can also always check the CRAN page for the package in question. Here you can access the documentation as well as available vignettes, e.g.: https://cran.r-project.org/web/packages/tibble/index.html. Another highly recommended resource are the RStudio cheatsheets found at: https://www.rstudio.com/resources/cheatsheets/. These are available for many popular packages and present a comprehensive list of the functions offered by the packages. The RStudio homepage also offers many more resources for learning R and specific packages, including a number of webinars and tutorial videos available under the menu “Resources”: https://www.rstudio.com/ In general, the internet offers a lot of resources that you can access. One of the most important skills you have to develop as an aspiring R user is to understand the problem you are facing to the best of your abilities and formulate a short but precise google search. In most cases you can assume, that you are not the first or last person to have a specific problem. Someone will have written a blogpost, asked a question on https://stackoverflow.com/, made a video tutorial, and so on. If you can find these resources, you are already halfway there. There are also a lot of books available on R and RStudio in general, as well as on more specific applications in R. I want to recommend three of them in particular, both available as paperback or online: Intro to R for Social Scientists by Jasper Tjaden. An accessible introduction to R that expands on the concepts only touched here. Written for a seminar at the University of Potsdam in summer 2021. Available under: https://jaspertjaden.github.io/course-intro2r/ R Cookbook, 2nd Edition by J.D. Long &amp; Paul Teetor. The book is comprised of recipes for specific tasks you might want to perform. It is not designed as a course but rather as reference for concrete questions. Available under: https://rc2e.com/ R for Data Science by Hadley Wickham and Garrett Grolemund. An introduction to data science using (almost) exclusively the tidyverse packages. Available under: https://r4ds.had.co.nz/ 2.4 Intro to R Markdown The following is an excerpt from another seminar that I have written with Prof. Jasper Tjaden and Niaz Morshed, Data Analysis with R for Social Scientists, available under: https://jaspertjaden.github.io/DataAnalysisR/ It is a short introduction to using R Markdown for documenting your work and writing reports and papers with it. 2.4.1 What is R Markdown? R Markdown allows you to combine written text that is easy to format with R Code that is executed when knitting or compiling the document into the chosen output format. This allows us to describe our research, analyse our data, display results as tables or plots and interpret these, all in one file. In this way we can not only create reports on seminar exercises but also write websites - like the one your are looking at in this very moment -, seminar papers, articles or create presentations. It is also a great notebook for projects you are working on. More often than not, our work on a specific analysis will span multiple days, weeks or even months and it is often hard to remember what we were thinking the last time we worked on our code. “I am sure I had my reasons for writing this piece of code, but I can not for the life of me remember any of them…” — Anonymous Coder 2023 If we use R Markdown to document our work we can add text that explains our reasons, thoughts, ideas and plans at that very moment and pick up our work from there the next time we open the file. R Markdown allows output to different file formats, including html, docx, pptx and pdf. Note that you need a LaTeX installation to knit to pdf. LaTeX is a typesetting language and used for producing high quality pdf documents. For simple pdf reports or presentations - sidenote: if you bring a pptx to a talk something will most probably go wrong or stop working… - you do not really need to know how LaTeX works, you just need an installed distribution. For these purposes the package tinytex gives you all you need and is easy to install from within R. This site explains how to install it. You can also get an overview of all possible output formats here. 2.4.2 Creating a R Markdown file Before you can create and compile R Markdown documents, you first have to install the package by writing install.packages(\"rmarkdown\") in your console. Creating a new R Markdown file is as straightforward as it can be. In RStudio you can click of File &gt; New File &gt; R Markdown.... In the new window you can set up some basic information on the document - which will be displayed in the output - and chose your desired format. You can basically write R Markdown files in any text editor, just make sure that the file extension is saved as .Rmd. We still recommend using RStudio because it gives you some convenient options that a text editor will not, e.g. displaying a preview of your document and easy knitting of the final file. 2.4.3 Writing in R Markdown 2.4.3.1 Document components When you followed the steps above, a new R Markdown file will have been created. It basically consists of two main parts: A YAML header - surrounded by three dashes --- - where options for the document can be set. The good news is that you do not have to do anything here until you get more profound with using R Markdown. For now it is enough that all the options you set when creating the new file - the title, author, date and format of the output - are present and will be included in your output file. A body that contains the actual content of your document. Text is directly written in the body at the location where it is to be displayed in the output. We can use the simple Markdown syntax for formatting using a set of symbols, some of which we will explain below. We can also include R code in so called chunks, specifying if we also want it and/or the results to be displayed or “just” to be executed in the background. The code chunks will be executed when we compile the final document and everything that we want to include in the output - e.g. tables, plots or code examples - will be displayed where it occurs. 2.4.3.2 Formatting Here are some of the more common formatting elements you will need when starting out using R Markdown: 2.4.3.2.1 Headers To include sections in a document we use # followed by the header we want to be displayed. We can define levels for sections by using multiple # in this way: # Section 1 ## Section 1.1 ## Section 1.2 ### Section 1.2.1 ### Section 1.2.2 ## Section 1.3 # Section 2 2.4.3.2.2 Text We write the text between the section headers at the place where it should be displayed in the final document. We can insert line breaks at any point but these will not be rendered in the output. To include an actual paragraph we will have to include a blank line between between two blocks of text. Two emphasize certain words or phrases, we can wrap them in * for italics or ** for bold face. Consider this Markdown code: This is the first paragraph. This still is the first paragraph. Here begins the second paragraph. It includes emphasis, by using *italics* and also **bold face** words. It is rendered as: This is the first paragraph. This still is the first paragraph. Here begins the second paragraph. It includes emphasis, by using italics and also bold face words. 2.4.3.2.3 Lists Unordered lists or bullet points can be inserted by adding a -, * or + at the beginning of a line. To create levels, we have to indent lines using tab stops. * Level 1 * Level 1 * Level 2 * Level 3 * Level 2 * Level 3 * Level 1 The above will be rendered as: Level 1 Level 1 Level 2 Level 3 Level 2 Level 3 Level 1 We can also create ordered lists by using numbers followed by a . instead of the * etc. 1. Bulletpoint 1 2. Bulletpoint 2 3. Bulletpoint 3 Bulletpoint 1 Bulletpoint 2 Bulletpoint 3 2.4.3.2.4 Hyperlinks Hyperlinks can be included as &lt;url&gt; or [text](url). To include a plain url we can use &lt;https://jaspertjaden.github.io/DataAnalysisR/&gt;. We can also [link](https://jaspertjaden.github.io/DataAnalysisR/) in this way. To include a plain url we can use https://jaspertjaden.github.io/DataAnalysisR/. We can also link in this way. 2.4.3.3 Code chunks Codechunks have to be started and ended with three backticks ```. After the first set of backticks we also have to include {r} to let Markdown know that we want to run the code as R code. The code that is written after this and up to the second set of backticks will be executed when knitting the file. You can see some examples of this in the newly created R Markdown file if you followed the steps above. We can also always run the code in a chunk before knitting by clicking on the green arrow in the upper right corner of the chunk. We can also execute individual lines of code by placing our keyboard cursor in the line and pressing Shift + Enter. 2.4.3.3.1 Chunk options We can change the way code chunks are handled when knitting by adding one or multiple chunk options between the curly brackets like this: {r option=value}. If we want to use multiple options they have to be written like this: {r option1=value1, option2=value2}. There are many options available but most are not needed when starting out. The ones that may be of interest to you are: {r echo=FALSE}: This prevents the code to be displayed in the output while the results will be included. This is useful if you want to show the results of a computation or a plot but do not want the document to be cluttered with the underlying code. {r include=FALSE}: This prevents the code as well as the output from being displayed. The code is still run in the background. {r eval=FALSE}: This prevents the code from being run but displays it. This can be useful if you want to show code examples for illustrative purposes. "],["html.html", "3 HTML as a cornerstone of the internet 3.1 HTML-Tags 3.2 Attributes 3.3 Entities", " 3 HTML as a cornerstone of the internet What happens when we call up a URL such as https://jakobtures.github.io/web-scraping/ in a browser? We will get a visualisation of the page in our browser window. From the perspective of the user of a website, this is everything we need to know. Our goal – calling up the website – has already been reached at this point. From the perspective of a web scraper, we need to understand however, what is happening behind the scenes. The link https://jakobtures.github.io/web-scraping/ does nothing but call up an HTML-file in which the content of the website is recorded in form of a specific code. This code is then interpreted by your browser and translated into a visual representation, which you are finding yourselves in front of now. Try it yourself. With a right-click into this area of the text and a further click on “View Page Source”, the HTML-code that the website is based on, is shown. At this point it is completely legitimate to be overwhelmed by the flood of unfamiliar symbols and terminology. Who would have thought that a relatively simple website such as this one, can be so complex and complicated on the backend? But the good news is that we do not need to be able to understand every word and every symbol in an HTML file. Our goal is identifying the parts of a website that are relevant for our data collection and extract them precisely from the HTML-code. This can possibly be only a single line of code in an HTML-file with thousands of lines of code. Do we need to understand every single line? No, but we have to be able to understand the structure of an HTML-file to be able to identify that one line that is of interest to us. Until we reach this point, we still have a ways to go. At the end of this first section, you will have a basic understanding of the structure and components of an HTML document and the source code will already seem way less intimidating. 3.1 HTML-Tags The “language” that HTML-files have been written in, is the Hypertext Markup Language, HTML for short. The basics of this language are the so-called Tags, the HTML “vocabulary”. These terms are used to structure the HTML-document, format it, insert links and images or create lists and tables. The browser knows the meaning of these key phrases, can interpret them and present the website visually according to the coded HTML-Tags. As with any language in the IT-world, HTML also follows certain rules of “grammar”, the Syntax. Fortunately for us, in the case of HTML this syntax is rather simple. In the following we will take a closer look at the tags and syntax rules that are important to us. Contemplate the following example: &lt;b&gt;Hello World!&lt;/b&gt; &lt;b&gt; is a tag. The b stands for bold. Tags always follow the same pattern. They begin with a &lt;, followed by the name of the tag – b – and end with a &gt;. It is important to note that an opened tag will need to be closed as well, under normal circumstances. To do this, the same tag is written again with a forward slash, &lt;/b&gt; in our case. Everything that is contained within the opening and closing tag in an HTML-document will be interpreted according to the meaning of the tag. With this knowledge we understand what will happen in our example. The tag &lt;b&gt; means bold, and the opening and closing tag &lt;b&gt;…&lt;/b&gt; include the text Hello World!. The text will be interpreted according to the tag, which means in bold: Hello World! 3.1.1 hello_world.html Let us have a look at a full HTML-document now: &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Hello World!&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;b&gt;Hello World!&lt;/b&gt; &lt;/body&gt; &lt;/html&gt; The interpretation of this document by the browser can be seen under https://jakobtures.github.io/web-scraping/hello_world.html. Let us take a look at the single elements of the code: The first line, the document type declaration, &lt;!DOCTYPE html&gt;, informs the browser, which version of HTML is being used in the document. &lt;!DOCTYPE html&gt; stands for the current HTML5 standard. This is the place to declare if an older version of HTML was used, to ensure the most accurate visual representation – in spite of possible changes or the omission of certain standards in the current version. From the point of view of a web-scraper the HTML-version does not necessarily play a massive role. Interestingly enough, the tag &lt;!DOCTYPE html&gt; is one of the few exceptions from the rule that a once-opened tag has to be closed again; it is not necessary here. The actual content of the HTML-document starts in line 3 with the &lt;html&gt; tag. The tag is closed again in the last line and thus contains the total contents of the document. So the tag tells the browser that all it encompasses, is HTML. Next is the tag &lt;head&gt;. What we see the tag encompassing here, is not what we see in the browser window yet. What this means in practice is that here we mainly find meta-information, advanced functionality (JavaScript) and definitions of design choices (Cascading Style Sheets – CSS). This should not distract us too much in this introduction to web scraping, but we should be aware that references to .js and .css files can appear in the &lt;head&gt; tag. In our example the &lt;head&gt; tag exclusively contains another tag called &lt;title&gt;, which in turn contains the text Hello World!. The &lt;title&gt; tag determines what we will see in the title bar of our browser. In this case Hello World! Finally, everything that the &lt;body&gt; tag includes, describes the content we can see in the browser window. In this simple example, only one line is included. The already known &lt;b&gt; tag includes the text Hello World!, which it displays in bold for us to see in the browser window. So now you already know the basic structure of any HTML file. While looking at the sample code above, you may have noticed that certain lines are indented to the right. This is not a requirement for functional HTML code, but a convention that makes it easier for reading and understanding. Indented lines represent the different hierarchical levels of the code. &lt;head&gt; is hierarchically subordinate to &lt;html&gt; and therefore single indented. &lt;title&gt;, in turn, is subordinate to &lt;head&gt; and therefore doubly indented. By writing it this way, it is also obvious at a first glance that &lt;body&gt; is subordinate to &lt;html&gt; but not to &lt;title&gt;, since &lt;body&gt; and &lt;title&gt; are each only single indented. You will often – but not always – encounter this convention in “real” HTML documents on the Internet. One more note on the technical side: HTML documents can basically be written by hand in any editor and must be saved with the extension .html. You can test this by starting any text editor yourself, copying the HTML code above, saving the file with the extension .html and opening it in a browser of your choice. However, due to their complexity, websites are not usually written by hand nowadays. A variety of professional tools now offer much more efficient ways to design websites. For example, the page you are looking at was written directly in RStudio using the bookdown package, which automates most of the layout decisions. 3.1.2 Important tags We cannot look at all the tags available in HTML at this point, but will initially limit ourselves to those that we encounter very frequently and that will be particularly relevant for our first web scraping projects. 3.1.2.1 Page structure One tag that relates to the structure of the page, we have already met above. The &lt;body&gt; tag communicates that everything encompassed by it is part of the content displayed in the browser window. One way to further structure the content is to use the maximum of six levels of headings that HTML offers. The tags &lt;h1&gt; &lt;h2&gt; ... &lt;h6&gt; allow this in a simple way. The h stands for header. The text encompassed by the tag is automatically numbered and displayed in different font sizes depending on the level of the heading. As an example, you can see the structure of the headings on this page as HTML code: &lt;h1&gt;HTML as a cornerstone of the internet&lt;/h1&gt; &lt;h2&gt;HTML-Tags&lt;/h2&gt; &lt;h3&gt;hello_world.html&lt;/h3&gt; &lt;h3&gt;Important tags&lt;/h3&gt; &lt;h4&gt;Page structure&lt;/h4&gt; &lt;h4&gt;Formatting&lt;/h4&gt; &lt;h4&gt;Lists&lt;/h4&gt; &lt;h4&gt;Tables&lt;/h4&gt; &lt;h2&gt;Attributes&lt;/h3&gt; &lt;h3&gt;Links&lt;/h4&gt; &lt;h3&gt;Images&lt;/h4&gt; &lt;h2&gt;Entities&lt;/h3&gt; Another frequently occurring form of structuring in HTML documents, are the groupings defined via &lt;div&gt; (division) and &lt;span&gt;. Both tags basically work the same way, with &lt;div&gt; referring to one or more lines and &lt;span&gt; referring to one line or part of a line. Neither have any direct effect on the display of the website at first, but they are often applied in combination with classes that are defined in Cascading Style Sheets – CSS, to adjust the visual interpretation. Normally, we do not care about how the CSS classes are defined and how they affect rendering. You will learn later in this seminar why the combination of &lt;div&gt; or &lt;span&gt; and CSS classes are often a very practical starting point for our web scraping endeavours and how we can exploit this in our work. Here is a simplified example of how both tags can appear in HTML code: &lt;div&gt; This sentence is part of the div tag. This sentence is part of the div tag, &lt;span&gt; while this sentence is part of the div and the span Tags. &lt;/span&gt; &lt;/div&gt; This sentence is not part of the div tag. 3.1.2.2 Formatting HTML provides a variety of tags for formatting the displayed text. In the following we will look at some of the most common ones. The &lt;p&gt; tag defines the enclosed text as a paragraph and is accordingly automatically terminated in the display with a line break. &lt;p&gt;This sentence is part of the paragraph. So is this. And this one.&lt;/p&gt; This sentence is not part of the paragraph. This is represented as: This sentence is part of the paragraph. So is this. And this one. This sentence is not part of the paragraph. The tag &lt;br&gt; introduces a line break. This tag is another exception to the rule that an opened tag must also be closed again. In this special case, using opening and closing tags &lt;br&gt;&lt;/br&gt; stands for two line breaks, so it is not equivalent to &lt;br&gt;. Unlike the line break inserted by &lt;p&gt;...&lt;/p&gt; at the end of the paragraph, no further spacing is inserted after &lt;br&gt;: Here comes some text, which is now broken up in two lines.&lt;br&gt; After the break tag, in contrast to the paragraph tag, no line spacing is inserted.&lt;br&gt; If the break tag is also explicitly closed again, two line breaks are inserted.&lt;br&gt;&lt;/br&gt; As can be seen here. This is represented as: Here comes some text, which is now broken up in two lines. After the break tag, in contrast to the paragraph tag, no line spacing is inserted. If the break tag is also explicitly closed again, two line breaks are inserted. As can be seen here. The typeface can be adjusted by tags like the already known &lt;b&gt; (bold) or &lt;i&gt; (italics) similar to the known options in common text editing programs: These tags can be used to render words, sentences and paragraphs &lt;b&gt;bold&lt;/b&gt; or &lt;i&gt;italic&lt;/i&gt;. This is represented as: These tags can be used to render words, sentences and paragraphs bold or italic. 3.1.2.3 Lists We will often encounter lists in HTML documents. The two most common variants being the unordered list, introduced by &lt;ul&gt;, and the ordered list, &lt;ol&gt;. The opening and closing list-tag covers the entire list, while each individual list element is enclosed by a &lt;li&gt; tag in both variants. Here are two short examples: &lt;ul&gt; &lt;li&gt;First unordered list element&lt;/li&gt; &lt;li&gt;Second unordered list element&lt;/li&gt; &lt;li&gt;Third unordered list element&lt;/li&gt; &lt;/ul&gt; This is represented as: First unordered list element Second unordered list element Third unordered list element &lt;ol&gt; &lt;li&gt;First ordered list element&lt;/li&gt; &lt;li&gt;Second ordered list element&lt;/li&gt; &lt;li&gt;Third ordered list element&lt;/li&gt; &lt;/ol&gt; This is represented as: First ordered list element Second ordered list element Third ordered list element 3.1.2.4 Tables HTML can also be used to display tables, without further adjustments of the display via CSS admittedly not very attractive tables. These are opened by a &lt;table&gt; tag and closed accordingly. Within the table, lines are defined by &lt;tr&gt;...&lt;/tr&gt; (table row). Within the line, table headers can be defined by &lt;th&gt; (table header) and cell contents by &lt;td&gt; (table data). Content encompassed by &lt;th&gt;...&lt;/th&gt; and &lt;td&gt;...&lt;/td&gt; are not only formatted differently in their presentation, from the web scraper’s point of view, these tags also allow us to clearly distinguish the table content to be read. Here is a simple example: &lt;table&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;Tag&lt;/th&gt; &lt;th&gt;Effect&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;&quot;b&quot;&lt;/td&gt; &lt;td&gt;bold&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&quot;i&quot;&lt;/td&gt; &lt;td&gt;italics&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; This is displayed as: # Tag Effect 1 “b” bold 2 “i” italics Formatting the HTML code in a kind of “table form” as in the example above is not necessary but increases intuitive readability. In fact, the following manner of writing it, is equivalent in result and actually more common in practice: &lt;table&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;Tag&lt;/th&gt; &lt;th&gt;Effect&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;&quot;b&quot;&lt;/td&gt; &lt;td&gt;bold&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;&quot;i&quot;&lt;/td&gt; &lt;td&gt;italics&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; This will also be displayed as: # Tag Effect 1 “b” bold 1 “i” italics 3.2 Attributes Many HTML tags can be further adapted in their functionality and presentation by using so-called attributes. The basic syntax is: &lt;tag attribute=\"value\"&gt;...&lt;/tag&gt;. In the opening (never in the closing) tag, the name of the tag is followed by the name of the attribute = the value to be assigned, enclosed in single or double inverted commas. In HTML, &lt;tag attribute=\"value\"&gt; is not equal to &lt;tag attribute = \"value\"&gt;. The pair of attribute name and value must be connected with a = without spaces in order to be interpreted correctly. A variety of tags can be modified with a multitude of attributes. Two of the most common and illustrative applications are the inclusion of links and images, two further frequently encountered HTML tags. 3.2.1 Links Links are included using the &lt;a&gt; (anchor) tag. The first intuitive attempt &lt;a&gt;This is a link&lt;/a&gt; is unfortunately unsuccessful: This is (not) a link Although the text is displayed and marked as a link – i.e. blue and underlined – it does not lead to any destination, since it was not defined in the HTML document what this destination should be. This is where the first attribute comes into play. With &lt;a href=\"url\"&gt; the target of the link is defined. href stands for hypertext reference and its assigned value can be, among other things, a website, an email address or even a file. For example, &lt;a href=\"https://jakobtures.github.io/web-scraping/html.html\"&gt;This is a link&lt;/a&gt; links to the page you are viewing. This is a link You may have noticed that you had to scroll to this point again to continue reading. A second attribute can remedy this. With &lt;a href=\"https://jakobtures.github.io/web-scraping/html.html\" target=\"_blank\"&gt;This is a link&lt;/a&gt; we instruct the browser to open the link in a new tab. The assigned value of target here is “_blank”, which stands for a new tab, but it can also take on a number of other values. This is a link Links are of particular interest in web scraping when we collect the links to all sub-pages from a parent page in order to scrape all of them in one step. But more about that later. One more note: the &lt;link&gt; tag is not to be confused with &lt;a&gt; and is used to integrate external files, such as the JavaScript or CSS files already mentioned. 3.2.2 Images Images and graphics are integrated in HTML with &lt;img&gt;, another tag that does not have to be explicitly closed. The URL of the image to be included is specified via the src (source) attribute of the tag. Thus &lt;img src=\"https://jakobtures.github.io/web-scraping/Rlogo.png\"&gt; includes the following image: Using further attributes, it is also possible, for example, to adjust the size of the image in pixels. &lt;img src=\"https://jakobtures.github.io/web-scraping/Rlogo.png\" width=\"100\" height=\"100\"&gt; results in a resized display of the image. Images can also be combined with links. So &lt;a href=\"https://www.r-project.org/\" target=\"_blank\"&gt;&lt;img src=\"https://jakobtures.github.io/web-scraping/Rlogo.png\"&gt;&lt;/a&gt; defines the image as a link, where a click on the image takes you to the specified link: 3.3 Entities A number of characters are reserved for the HTML code. We have already seen that the characters &lt; &gt; \" are part of the code to define tags and values of attributes. In many cases, more current HTML versions allow for the usage of reserved characters directly in continuous text. For the time being, however, we will regularly encounter so-called entities instead of the actual characters in web scraping. Entities are coded representations of certain characters. They are always introduced with &amp; and ended with ;. Between the two characters is either the name or the number of the entity. For example, &amp;lt; stands for less than, i.e. &lt; and &amp;gt; for greater than, i.e. &gt;. A text with reserved characters like &amp;lt; und &amp;gt; or the so-called &amp;quot;ampersand&amp;quot; &amp;amp;. Is displayed as: A text with reserved characters like &lt; und &gt; or the so-called \"ampersand\" &amp;. Sidenote: If you are interested in the origin of the term ampersand, I recommend its Wikipedia article, which makes for an interesting read: https://en.wikipedia.org/wiki/Ampersand Another entity we will encounter regularly is &amp;nbsp; (non-breaking space), which can be used instead of a simple space. The advantage of this is that there is never a line break in the browser, and it allows the use of more than one space: Displayed with one space Displayed with four&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;spaces Displayed with one space Displayed with four    spaces An overview of the most common entities can be found here: https://www.w3schools.com/html/html_entities.asp The entities of all Unicode characters can be found here: https://unicode-table.com/ "],["rvest1.html", "4 First scraping with rvest 4.1 The rvest package 4.2 hello_world.html 4.3 Countries of the World", " 4 First scraping with rvest With the knowledge of how an HTML file is constructed and how R and RStudio work in basic terms, we are equipped with the necessary tools to take our first steps in web scraping. In this session we will learn how to use the R package rvest to read HTML source code into RStudio, extract targeted content we are interested in, and transfer the collected data into an R object for further analysis in the future. 4.1 The rvest package Part of the tidyverse is a package called rvest, which provides us with all the basic functions for a variety of typical web scraping tasks. This package was included in the installation of the tidyverse package, but it is not part of the core tidyverse and thus is not loaded into the current R session with library(tidyverse). Therefore, we have to do this explicitly: library(rvest) 4.2 hello_world.html As a first exercise, it is a good idea to scrape the Hello World example already described in chapter 3. As a reminder, here is the HTML source code: &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Hello World!&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;b&gt;Hello World!&lt;/b&gt; &lt;/body&gt; &lt;/html&gt; 4.2.1 read_html() The first step in web scraping is to convert the page we are interested in into an R object. This is made possible by the function read_html() from the rvest package. read_html() “parses” the website, i.e. it reads the HTML, understands its source code and transforms it into a representation R can understand. This function needs the URL, i.e. the address of the website we want to read in, as its first argument. The URL must be given as a string, so we have to enclose it in \". The function also allows you to specify other options. In most cases, however, the default settings are sufficient. So we read in the hello_world.html file, assign it to a new R object at the same time and have this object put out in the next step: hello_world &lt;- read_html(&quot;https://jakobtures.github.io/web-scraping/hello_world.html&quot;) hello_world ## {html_document} ## &lt;html&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body&gt;\\n &lt;b&gt;Hello World!&lt;/b&gt;\\n &lt;/body&gt; As we can see in the output, the R object hello_world is a list with two entries. The first entry contains everything enclosed by the &lt;head&gt; tag, the second entry everything enclosed by the &lt;body&gt; tag. The opening and closing &lt;html&gt; tag is not part of the object. Remembering that HTML code is hierarchically structured, the list is thus organised based on the highest remaining levels – &lt;head&gt; and &lt;body&gt;. We have thus successfully created a representation of the website in an R object. But what do we do with it now? In the case of this simple example, we might be interested in extracting the title of the website or the text displayed on the page. 4.2.2 html_elements() The function html_elements() from rvest allows us to extract individual elements of the HTML code. To do this, it needs the object to be extracted from as the first argument and a selector as well. In this introduction, we will concentrate exclusively on the so-called CSS selectors. The alternative XPath is a bit more flexible, but CSS selectors are sufficient in most cases and have a shorter and more intuitive syntax, which clearly makes them the tool of choice here. We will discuss the possibilities offered by CSS selectors in more detail in chapter 5.1 and will limit ourselves to the basics for now. A selector in the form \"tag\", selects all HTML tags of the specified name. If we want to extract the &lt;title&gt; tag, we can do so in this way: element_title &lt;- html_elements(hello_world, css = &quot;title&quot;) element_title ## {xml_nodeset (1)} ## [1] &lt;title&gt;Hello World!&lt;/title&gt; If we want to extract the text Hello World! shown on the website, one possibility would be to select the complete &lt;body&gt; tag, since in this case no other text is displayed on the page. element_body &lt;- html_elements(hello_world, css = &quot;body&quot;) element_body ## {xml_nodeset (1)} ## [1] &lt;body&gt;\\n &lt;b&gt;Hello World!&lt;/b&gt;\\n &lt;/body&gt; This works in principle, but we also extracted the &lt;b&gt; tags as well as multiple new lines (\\n), which we do not need both. It would be more efficient to directly select the &lt;b&gt; tag enclosing the text. element_b &lt;- html_elements(hello_world, css = &quot;b&quot;) element_b ## {xml_nodeset (1)} ## [1] &lt;b&gt;Hello World!&lt;/b&gt; 4.2.3 html_text() In this case, we are interested in the text in the title and on the website, i.e. the content of the tags. We can extract this from the selected HTML elements in an additional step. This is made possible by the rvest function html_text(). This requires the previously extracted HTML element as the only argument. html_text(element_title) ## [1] &quot;Hello World!&quot; html_text(element_b) ## [1] &quot;Hello World!&quot; With this, we have successfully completed our first web scraping goal, the extraction of the title and the text displayed on the page. One more thing about the application of html_text() to elements that themselves contain further tags: Further above we extracted the object element_body, which contains the &lt;b&gt; tags as well as several line breaks in addition to the displayed text. Here, too, we can extract the pure text. html_text(element_body) ## [1] &quot;\\n Hello World!\\n &quot; We see that the function has conveniently removed the &lt;b&gt; tags we were not interested in for us. However, the line breaks and several spaces, so-called whitespace, remain. Both can be removed with the additional argument trim = TRUE. html_text(element_body, trim = TRUE) ## [1] &quot;Hello World!&quot; 4.3 Countries of the World Let us now look at a somewhat more realistic application. The website https://scrapethissite.com/pages/simple/ lists the names of 250 countries, as well as their flag, capital, population and size in square kilometres. Our goal could be to read this information into R for each country so that we can potentially analyse it further. Before we start, we should load the required packages (we will also need the tidyverse package this time) and read the website with the function read_html() and assign it to an R object. library(tidyverse) library(rvest) website &lt;- read_html(&quot;https://scrapethissite.com/pages/simple/&quot;) To understand the structure of the HTML file, the first step is to look at the source code. As always, we can open it by right-clicking in the browser window and then clicking on “View Page Source”. The first 100 or so lines of HTML code mainly contain information on the design of the website, which should not distract us further at this point. We are purely interested in the data of the countries. The first country listed on the website is Andorra. It therefore makes sense to search the source code specifically for “Andorra”. The key combination CTRL+F opens the search mask in your browser. We find what we are looking for in line 128. Since this source code, designed for practice purposes, is formatted in a very structured way, we quickly realise that lines 125-135 are code blocks related to Andorra. Let’s look at these more closely: &lt;div class=&quot;col-md-4 country&quot;&gt; &lt;h3 class=&quot;country-name&quot;&gt; &lt;i class=&quot;flag-icon flag-icon-ad&quot;&gt;&lt;/i&gt; Andorra &lt;/h3&gt; &lt;div class=&quot;country-info&quot;&gt; &lt;strong&gt;Capital:&lt;/strong&gt; &lt;span class=&quot;country-capital&quot;&gt;Andorra la Vella&lt;/span&gt;&lt;br&gt; &lt;strong&gt;Population:&lt;/strong&gt; &lt;span class=&quot;country-population&quot;&gt;84000&lt;/span&gt;&lt;br&gt; &lt;strong&gt;Area (km&lt;sup&gt;2&lt;/sup&gt;):&lt;/strong&gt; &lt;span class=&quot;country-area&quot;&gt;468.0&lt;/span&gt;&lt;br&gt; &lt;/div&gt; &lt;/div&gt;&lt;!--.col--&gt; All the information about Andorra is enclosed in a &lt;div&gt; tag. As a reminder, a &lt;div&gt; defines a grouping of code across multiple lines. In web design practice, these groupings are mainly used to assign a certain CSS style to the following code via the argument class=, for example to define the typeface. From a web scraping perspective, we generally don’t care how the styles are defined. We just need to know that we can exploit these CSS assignments of classes for our purposes. At the next level down, we find two blocks, one containing, among other things, the name of the country and another containing information about that country. Let’s begin with examining the first block. 4.3.1 Country names &lt;h3 class=&quot;country-name&quot;&gt; &lt;i class=&quot;flag-icon flag-icon-ad&quot;&gt;&lt;/i&gt; Andorra &lt;/h3&gt; The name “Andorra” is enclosed in an &lt;h3&gt; tag, i.e. a third-level heading. In addition to the name, we also find another tag within the tag that includes the image of the flag. Since we are not interested in the graphics here, we can ignore this. On this website, all &lt;h3&gt; tags are used exclusively to display the names of the countries. Thus, we can use the &lt;h3&gt; tag as a CSS selector to read out the enclosed text analogous to the first example. element_country &lt;- html_elements(website, css = &quot;h3&quot;) text_country &lt;- html_text(element_country, trim = TRUE) head(text_country, n = 10) ## [1] &quot;Andorra&quot; &quot;United Arab Emirates&quot; &quot;Afghanistan&quot; ## [4] &quot;Antigua and Barbuda&quot; &quot;Anguilla&quot; &quot;Albania&quot; ## [7] &quot;Armenia&quot; &quot;Angola&quot; &quot;Antarctica&quot; ## [10] &quot;Argentina&quot; The result looks promising. Since the structure of the code block is the same for each country, the vector text_country was created in this way with 250 entries, exactly the number of countries listed on the website. For reasons of clarity, it often makes sense not to put out the complete and often very long vectors, data frames or tibbles, but to use the function head() to list the number of entries specified by the argument n, starting with the first. 4.3.1.1 The pipe %&gt;% At this point, we should think again about the readability and structure of our R code. Let us consider the preceding code block: element_country &lt;- html_elements(website, css = &quot;h3&quot;) text_country &lt;- html_text(element_country, trim = TRUE) As we have seen, this achieves our goal. However, we have also created the element_country object to temporarily save the result of the first step – reading the &lt;h3&gt; tags. We will never need this object again. If we use the pipe %&gt;% from tidyverse instead, the need to cache partial results is eliminated and we write code that is more intuitive and easier to understand at the same time. country &lt;- website %&gt;% html_elements(css = &quot;h3&quot;) %&gt;% html_text(trim = TRUE) head(country, n = 10) ## [1] &quot;Andorra&quot; &quot;United Arab Emirates&quot; &quot;Afghanistan&quot; ## [4] &quot;Antigua and Barbuda&quot; &quot;Anguilla&quot; &quot;Albania&quot; ## [7] &quot;Armenia&quot; &quot;Angola&quot; &quot;Antarctica&quot; ## [10] &quot;Argentina&quot; The pipe passes the result of a work step along to the next function, which in the tidyverse as well as in many other R-functions (but not all!) takes data as the first argument, which we then do not have to define explicitly. For a better understanding, let’s look at the above example in detail. The first line passes the object website along to the function html_elements(). So we don’t have to tell html_elements() which object to apply to, because we already passed it along to the function with the pipe. The function is applied to the object website with all other defined arguments – here css – and the result is passed along again to the next line, where the html_text() function is applied to it. Here the pipe ends, and the final result is assigned to the object country. We now need three instead of two lines to get the same result, but the actual typing work has been reduced – especially if you create the pipe with the key combination CTRL+Shift+M – and we have created code that can be read and understood more intuitively with a little practice. Also we do not clutter our environment with unneeded objects. So should we always connect all steps with the pipe? No. In many cases it makes sense to save intermediate results in an object, namely whenever we will access it multiple times. In our example, we could also integrate the import of the website into the pipe: country &lt;- read_html(&quot;https://scrapethissite.com/pages/simple/&quot;) %&gt;% html_elements(css = &quot;h3&quot;) %&gt;% html_text(trim = TRUE) Overall, this saves us even more typing. However, since we still have to access the selected website multiple times later on, this would also mean that the parsing process has to be repeated each time. On the one hand, this can have a noticeable impact on the computing time for larger amounts of data. On the other hand, it also means accessing the website’s servers and downloading the data again each time. However, we should avoid data traffic generated without good reasons as part of a good practice of web scraping – see 9. So it makes perfect sense to save the result of the read_html() function in an R object so that it can be reused multiple times. We will see the pipe in action many more times over the course of this seminar. 4.3.2 Capitals, population and area Let us now turn to the further information for each country. These are located in the second block of the HTML code considered above: &lt;div class=&quot;country-info&quot;&gt; &lt;strong&gt;Capital:&lt;/strong&gt; &lt;span class=&quot;country-capital&quot;&gt;Andorra la Vella&lt;/span&gt;&lt;br&gt; &lt;strong&gt;Population:&lt;/strong&gt; &lt;span class=&quot;country-population&quot;&gt;84000&lt;/span&gt;&lt;br&gt; &lt;strong&gt;Area (km&lt;sup&gt;2&lt;/sup&gt;):&lt;/strong&gt; &lt;span class=&quot;country-area&quot;&gt;468.0&lt;/span&gt;&lt;br&gt; &lt;/div&gt; As we can see, both the name of the capital, the population of the country, and its size in square kilometers are enclosed by a &lt;span&gt; tag in lines 2–4 respectively. Like &lt;div&gt;, &lt;span&gt; defines groupings, but not across multiple lines but for one, or as here, part of a line. So let’s try to read the names of the capitals, using the &lt;span&gt; tag as a selector. website %&gt;% html_elements(css = &quot;span&quot;) %&gt;% html_text() %&gt;% head(n = 10) ## [1] &quot;Andorra la Vella&quot; &quot;84000&quot; &quot;468.0&quot; &quot;Abu Dhabi&quot; ## [5] &quot;4975593&quot; &quot;82880.0&quot; &quot;Kabul&quot; &quot;29121286&quot; ## [9] &quot;647500.0&quot; &quot;St. John&#39;s&quot; So we get the names of the capitals, but also the population and the size of the country. span was too unspecific as a selector. Since all three types of country data are enclosed with &lt;span&gt; tags, all three are also selected. So we have to tell html_elements() more precisely which &lt;span&gt; we are interested in. This is where the CSS classes we mentioned earlier come into play. These differ between the three countries’ information. For example, the &lt;span&gt; that includes the name of the capital city is assigned the class \"country-capital\". We can target this class with our CSS selector. To select a class, we can use the syntax .class-name. So, to select all &lt;span&gt; that have the class \"country-capital\", we can do as follows: capital &lt;- website %&gt;% html_elements(css = &quot;span.country-capital&quot;) %&gt;% html_text() head(capital, n = 10) ## [1] &quot;Andorra la Vella&quot; &quot;Abu Dhabi&quot; &quot;Kabul&quot; &quot;St. John&#39;s&quot; ## [5] &quot;The Valley&quot; &quot;Tirana&quot; &quot;Yerevan&quot; &quot;Luanda&quot; ## [9] &quot;None&quot; &quot;Buenos Aires&quot; We can repeat this in an analogue manner for the number of inhabitants with the class \"country-population\". population &lt;- website %&gt;% html_elements(css = &quot;span.country-population&quot;) %&gt;% html_text() head(population, n = 10) ## [1] &quot;84000&quot; &quot;4975593&quot; &quot;29121286&quot; &quot;86754&quot; &quot;13254&quot; &quot;2986952&quot; ## [7] &quot;2968000&quot; &quot;13068161&quot; &quot;0&quot; &quot;41343201&quot; If we take a closer look at the vector created in this way, we see that it is a character vector. For inspection we can use the function str(), which gives us the structure of an R object, including the data type used. str(population) ## chr [1:250] &quot;84000&quot; &quot;4975593&quot; &quot;29121286&quot; &quot;86754&quot; &quot;13254&quot; &quot;2986952&quot; ... So the numbers were not read out as numbers but as strings. Among other things, this does not allow for calculation with the numbers. Reminder: population[1] selects the first element of the vector. population[1] / 2 ## Error in population[1]/2: non-numeric argument to binary operator As you remember, we can tell R to interpret the “text” read from the HTML code as numbers using the function as.numeric(). population &lt;- website %&gt;% html_elements(css = &quot;span.country-population&quot;) %&gt;% html_text() %&gt;% as.numeric() str(population) ## num [1:250] 84000 4975593 29121286 86754 13254 ... population[1] / 2 ## [1] 42000 In the same way, the size in square kilometers can be read with the class \"country-area\". area &lt;- website %&gt;% html_elements(css = &quot;span.country-area&quot;) %&gt;% html_text() %&gt;% as.numeric() str(area) ## num [1:250] 468 82880 647500 443 102 ... 4.3.3 Merge into one tibble We have now created four vectors, which respectively contain the information about the name of the country, the associated capital, the number of population and the size of the country. For Andorra: country[1] ## [1] &quot;Andorra&quot; capital[1] ## [1] &quot;Andorra la Vella&quot; population[1] ## [1] 84000 area[1] ## [1] 468 We could already continue working with this, but for many applications it is more practical if we combine the data in tabular form. In the tidyverse, the form of the tibble is suitable for this purpose. countries &lt;- tibble( Land = country, Hauptstadt = capital, Bevoelkerung = population, Flaeche = area ) countries ## # A tibble: 250 × 4 ## Land Hauptstadt Bevoelkerung Flaeche ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Andorra Andorra la Vella 84000 468 ## 2 United Arab Emirates Abu Dhabi 4975593 82880 ## 3 Afghanistan Kabul 29121286 647500 ## 4 Antigua and Barbuda St. John&#39;s 86754 443 ## 5 Anguilla The Valley 13254 102 ## 6 Albania Tirana 2986952 28748 ## 7 Armenia Yerevan 2968000 29800 ## 8 Angola Luanda 13068161 1246700 ## 9 Antarctica None 0 14000000 ## 10 Argentina Buenos Aires 41343201 2766890 ## # ℹ 240 more rows This is not only more readable but also facilitates all further potential analysis steps. If we are sure that we do not need the individual vectors, we can also perform the reading of the data and the creation of the tibble in a single step. Below you can see how the complete scraping process can be completed in relatively few lines. website &lt;- &quot;https://scrapethissite.com/pages/simple/&quot; %&gt;% read_html() countries_2 &lt;- tibble( Land = website %&gt;% html_elements(css = &quot;h3&quot;) %&gt;% html_text(trim = TRUE), Hauptstadt = website %&gt;% html_elements(css = &quot;span.country-capital&quot;) %&gt;% html_text(), Bevoelkerung = website %&gt;% html_elements(css = &quot;span.country-population&quot;) %&gt;% html_text() %&gt;% as.numeric(), Flaeche = website %&gt;% html_elements(css = &quot;span.country-area&quot;) %&gt;% html_text() %&gt;% as.numeric() ) countries_2 ## # A tibble: 250 × 4 ## Land Hauptstadt Bevoelkerung Flaeche ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Andorra Andorra la Vella 84000 468 ## 2 United Arab Emirates Abu Dhabi 4975593 82880 ## 3 Afghanistan Kabul 29121286 647500 ## 4 Antigua and Barbuda St. John&#39;s 86754 443 ## 5 Anguilla The Valley 13254 102 ## 6 Albania Tirana 2986952 28748 ## 7 Armenia Yerevan 2968000 29800 ## 8 Angola Luanda 13068161 1246700 ## 9 Antarctica None 0 14000000 ## 10 Argentina Buenos Aires 41343201 2766890 ## # ℹ 240 more rows "],["css-selectors-developer-tools.html", "5 CSS selectors &amp; Developer Tools 5.1 CSS selectors 5.2 Developer Tools", " 5 CSS selectors &amp; Developer Tools 5.1 CSS selectors In the previous section, you already got to know the first CSS selectors. These are actually used in web design to select individual elements of a website and apply a CSS style to them, i.e. to define the display of the elements. So they were not developed with web scraping applications in mind, yet we can still make use of them, because we also want to select individual elements of a website in order to extract them. CSS selectors are used in rvest as an argument of the html_elements() function. As a second argument – the first one determines which data the function should be applied to – we specify a selector in the form css = \"selector\". This determines which elements of the HTML code we want to extract. It is important here that the entire selector – regardless of how many individual parts it consists of – is always passed along as a string to the argument, i.e. enclosed in \". In the following, the CSS selectors are applied to the website https://jakobtures.github.io/web-scraping/turnout.html for illustration. You can view the source code the usual way. Firstly, we load the rvest package and parse the website. library(rvest) website &lt;- &quot;https://jakobtures.github.io/web-scraping/turnout.html&quot; %&gt;% read_html() We have already learned about the simplest selector. \"tag\" selects all occurrences of the specified HTML tag. For example, we can select the title of the website – in the &lt;title&gt; tag – or the heading displayed in the browser window – in &lt;h3&gt;. website %&gt;% html_elements(css = &quot;title&quot;) ## {xml_nodeset (1)} ## [1] &lt;title&gt;Voter turnout for German federal state elections&lt;/title&gt;\\n website %&gt;% html_elements(css = &quot;h3&quot;) ## {xml_nodeset (1)} ## [1] &lt;h3&gt;Voter turnout for the last federal state elections in Germany:&lt;/h3&gt; 5.1.1 Classes We also got to know the selector for the argument class – \".class\" – in the previous section. We can select all &lt;span&gt; tags of the class \"state-name\" as follows: website %&gt;% html_elements(css = &quot;.state-name&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; website %&gt;% html_elements(css = &quot;span.state-name&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; The first variation of the selector says, select all elements of the class \"state-name\". The second variation says, select all &lt;span&gt; elements of the class \"state-name\". For this website, both variations are equivalent in result, since all elements assigned the class \"state-name\" are also &lt;span&gt; tags. However, this does not always have to be the case. Different tags with different content may very well have the same class in practice. Basically, constructing CSS selectors is always a balancing act between functionality and readability. The selector should select exactly only the elements we are interested in, and at the same time be understandable. The latter is especially important when others – or you yourself a few weeks later – read the code. Readability also means achieving a balance between length and clarity of the selector. To illustrate: website %&gt;% html_elements(css = &quot;body &gt; div#data &gt; div[class^=state-] &gt; span.state-name&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; This is the full path through the hierarchical HTML structure, which leads us to the result already seen above. You don’t need to fully understand this one yet. But you can see that the selector can get long and complicated, and here we are dealing with a very simply designed website. For me, the selector \"span.state-name\" is a good choice, because it allows us to reach our goal and has a balance between brevity and readability that is pleasant for me. But you must decide this for yourself in each individual case. 5.1.2 IDs Another common attribute in HTML elements, is the id. These identify individual HTML elements with a unique assigned name. Among other things, these are used for design purposes, as part of scripts for the dynamic design of websites or in HTML forms. We can also use them to extract individual elements specifically. In our example, the &lt;span&gt; tag, which includes the name of each state, has an id that identifies the state with a two-character abbreviation: &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; The selector for IDs is being written as \"#id\". website %&gt;% html_elements(css = &quot;#bw&quot;) ## {xml_nodeset (2)} ## [1] &lt;img id=&quot;bw&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/6/ ... ## [2] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; In this case, however, we have selected more than we wanted. Because the &lt;img&gt; tag, which represents the flags of the federal states, has the attribute id=\"bw\" as well. Strictly speaking, this is against the HTML rules, but this is also a reality. We can’t rely on the creators of a website to always write clean HTML code, so we have to be able to deal with “rule violations” and unexpected structures. The solution at this point is the combination of ID and tag in the selector, in the form \"tag#id\". website %&gt;% html_elements(css = &quot;span#bw&quot;) ## {xml_nodeset (1)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; A combination with the class attribute is also possible. website %&gt;% html_elements(css = &quot;span.state-name#bw&quot;) ## {xml_nodeset (1)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; In general, it should be noted that the CSS selectors are case-sensitive. This means that we also need to be case sensitive. In the entry for Hamburg, the name of the id is capitalized: &lt;span class=&quot;state-name&quot; id=&quot;HH&quot;&gt;Hamburg&lt;/span&gt; If we want to extract this element, we have to use this way of writing in the selector as well, \"#hh\" will not work. website %&gt;% html_elements(css = &quot;span#hh&quot;) ## {xml_nodeset (0)} website %&gt;% html_elements(css = &quot;span#HH&quot;) ## {xml_nodeset (1)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;HH&quot;&gt;Hamburg&lt;/span&gt; 5.1.3 Attributes ID and Class are attributes of HTML tags. Since both are particularly relevant in web design, the shortcuts presented above exist to select them quickly. However, we can use all attributes occurring in an HTML code to select elements. The corresponding CSS selector is written as tag[attribute]. At the bottom of our example HTML code, there are three &lt;a&gt; tags. These have, in addition to href=\"url\" – the linked page – the attribute target_=\"\". This specifies how the link should open. The value \"_blank\" opens the link in a new browser tab, \"_self\" in the active tab. For the second link no target=\"\" attribute is set. In our example, to select all &lt;a&gt; tags that have a target attribute, i.e. the first and third, we could proceed as follows: website %&gt;% html_elements(css = &quot;a[target]&quot;) ## {xml_nodeset (2)} ## [1] &lt;a href=&quot;https://www.wahlrecht.de/ergebnisse/index.htm&quot; target=&quot;_blank&quot;&gt;\\ ... ## [2] &lt;a href=&quot;https://jakobtures.github.io/web-scraping/css-selectors-develope ... With \"element[attribute='value']\" it is possible to select only attributes with a certain value. Note that the value is enclosed in single quotes. The manner of writing \"element[attribute=\"value\"]\" would be split into the two strings \"element[attribute=\" and \"]\" and the R object value, which cannot be interpreted by R in this combination. Instead, we use single quotes to define the value of the attribute. This is a convention used in many programming contexts. Inside “ we use ’, inside ‘ we use “ to be able to realize multiple levels of quotes. If we want to select only the link that opens in a new tab: website %&gt;% html_elements(css = &quot;a[target=&#39;_blank&#39;]&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;https://www.wahlrecht.de/ergebnisse/index.htm&quot; target=&quot;_blank&quot;&gt;\\ ... This can be further modified by specifying that the value of an attribute should have a certain beginning – \"element[attribute^='beginning']\" – a certain end – \"element[attribute$='end']\" – or contain a certain partial term – \"element[attribute*='partial term']\". Thus, we could select the links based on the beginning, the end, or any part of the URL assigned to the href=\"\" attribute. website %&gt;% html_elements(css = &quot;a[href^=&#39;https://www&#39;]&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;https://www.wahlrecht.de/ergebnisse/index.htm&quot; target=&quot;_blank&quot;&gt;\\ ... website %&gt;% html_elements(css = &quot;a[href$=&#39;Deutschland&#39;]&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;https://de.wikipedia.org/wiki/Flaggen_und_Wappen_der_L%C3%A4nder ... website %&gt;% html_elements(css = &quot;a[href*=&#39;web-scraping&#39;]&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;https://jakobtures.github.io/web-scraping/css-selectors-develope ... 5.1.4 Hierarchy levels We can think of the hierarchy of an HTML structure as analogous to a family tree. A simplified representation of our example page as a family tree could look like this: By clicking on the elements of the family tree, you can expand and collapse the hierarchy levels. Click once through the different levels and compare this with the HTML source code of the page. When you’re done with that, I suggest collapsing &lt;head&gt; and expanding only &lt;div id=\"data\"&gt; in &lt;body&gt; and the first &lt;div&gt; at the next level down. With that, you should see everything we need below. With the metaphor of the family tree and the associated terms “descendant”, “child/parent”, and “sibling”, we should have an easier time understanding the slightly more advanced Selector concepts that follow. 5.1.4.1 Descendant If element A is a “descendant” of element B, this means that A “descended” from B over any number of generations. This can be one generation – i.e. a direct child-parent relationship; it can also be any number of generations – i.e. grandchildren and grandparents with any number of “grand” prefixes. In the selector, we write this as \"B A\". For example, if we want to select all &lt;span&gt; tags that descend from the &lt;div id=\"data\"&gt; tag – that is, a grandchild-grandparent relationship: website %&gt;% html_elements(css = &quot;div#data span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; It should be noted that we can only select younger generations with CSS Selectors. So, for example, we can’t select the grandparents of the grandchildren by “flipping” the selector around. To select the grandparents – i.e. &lt;div id=\"data\"&gt;, we would need to look at which element they descended from, for example &lt;body&gt;. CSS selectors are not limited to mapping the relationship of two generations. A longer selector containing four generations and leading to the same result could look like this: website %&gt;% html_elements(css = &quot;body div#data div span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; 5.1.4.2 Child/Parent If we want to define direct child-parent relationships instead of lineages across any number of generations, we can do this with selectors in the form \"B &gt; A\". This means A is a direct child of B. Since the &lt;span&gt; tags selected earlier are not direct children of &lt;div id=\"data\"&gt;, \"div#data &gt; span\" would not achieve the desired goal here. However, the &lt;span&gt; tags are direct children of &lt;div&gt; with classes \"state-odd\" and \"state-even\" respectively. Since there is no other direct child-parent relationship of &lt;span&gt; and &lt;div&gt; tags in our example, the selector \"div &gt; span\" would already be sufficient. However, we could become more explicit to write less error-prone code. Both &lt;div&gt; tags are similar in that their classes start with \"state\" and we already know how to exploit this: website %&gt;% html_elements(css = &quot;div[class^=&#39;state&#39;] &gt; span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; If we do not want to select all children, but only certain ones, there are a number of options. \":first-child\" selects the first child of an element: website %&gt;% html_elements(css = &quot;div[class^=&#39;state&#39;] &gt; :first-child&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;img id=&quot;bw&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/6/ ... ## [2] &lt;img id=&quot;by&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/2/ ... However, these are not the &lt;span&gt; but the &lt;img&gt; tags, as a quick look at the family tree reminds us again. With \":nth-child(n)\" we can again specify more precisely which child – counted in the order in which they appear in the HTML code – we are interested in. To make it clearer again that we are only interested in &lt;span&gt; children, we combine \"span\" and \"nth-child(n)\": website %&gt;% html_elements(css = &quot;div[class^=&#39;state&#39;] &gt; span:nth-child(2)&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; So we want to select all &lt;span&gt; tags that are the second child of the &lt;div&gt; tags whose classes start with \"state\". If we would like to select only the last child, we could do this with :last-child: website %&gt;% html_elements(css = &quot;div[class^=&#39;state&#39;] &gt; span:last-child&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [2] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; 5.1.4.3 Sibling(s) The common children of a parent element can be seen as siblings within the metaphor of the family tree. On the lowest hierarchical level, we thus have four siblings per federal state in our example. One &lt;img&gt; and three &lt;span&gt; tags. The selector \"Element-A ~ Element-B\" selects all siblings B that follow sibling A: website %&gt;% html_elements(css = &quot;div[class^=&#39;state&#39;] img ~ span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; website %&gt;% html_elements(css = &quot;img ~ span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; The result for both notations is identical. If we had &lt;span&gt; tags that follow &lt;img&gt; tags in other places in the HTML code in our example, the second variant would no longer be explicit enough. The selector \"Element-A + Element-B\" works in the same way, but only selects the sibling B that follows directly after sibling A: website %&gt;% html_elements(css = &quot;img + span&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; website %&gt;% html_elements(css = &quot;span.state-name + span&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [2] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; The selection of siblings can be further restricted with \"element:first-of-type\", \"element:nth-of-type(n)\" and \"element:last-of-type\". In this way, the first, nth and last sibling of a certain type can be selected. The siblings are thus differentiated according to the type of element, with which our family tree metaphor is being somewhat overused. website %&gt;% html_elements(css = &quot;div[class^=&#39;state&#39;] &gt; span:first-of-type&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; website %&gt;% html_elements(css = &quot;div[class^=&#39;state&#39;] &gt; span:nth-of-type(2)&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [2] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; website %&gt;% html_elements(css = &quot;div[class^=&#39;state&#39;] &gt; span:last-of-type&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [2] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; 5.1.5 Further selectors The wildcard \"*\" selects all elements. For example, we can also select all children of an element in the following way: website %&gt;% html_elements(css = &quot;div[class^=&#39;state&#39;] &gt; *&quot;) %&gt;% head(n = 8) ## {xml_nodeset (8)} ## [1] &lt;img id=&quot;bw&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/6/ ... ## [2] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [3] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [4] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [5] &lt;img id=&quot;by&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/2/ ... ## [6] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [7] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; ## [8] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; If we want to exclude certain elements from the selection, this is possible with \":not(selector)\". Here, all elements are selected, except those that we have explicitly excluded. For example, all children except the &lt;img&gt; elements: website %&gt;% html_elements(css = &quot;div[class^=&#39;state&#39;] &gt; :not(img)&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; With \"selector-A, selector-B\", several selectors can be linked with “and/or”. This makes it possible, for example, to select two &lt;span&gt; classes in one step: website %&gt;% html_elements(css = &quot;div[class^=&#39;state&#39;] &gt; span.election-year, div[class^=&#39;state&#39;] &gt; span.election-turnout&quot;) %&gt;% head(n = 4) ## {xml_nodeset (4)} ## [1] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [2] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [3] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; ## [4] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; You now know the selectors most commonly used in web scraping. An overview of these and all other CSS selectors can be found on the W3 pages: https://www.w3schools.com/cssref/css_selectors.asp In addition to CSS selectors, there is another way to select elements of an HTML page: XPath. This method is even more flexible and allows, among other things, the selection of “ancestors”, i.e. higher hierarchical levels, starting from a lower one. The price for the higher flexibility, however, is an often longer and somewhat more complicated syntax. In most cases, the CSS selectors will suffice. And if at some point you reach a point where they are no longer sufficient to achieve your scraping goal, you will already be so proficient in using the CSS selectors that you will find it easy to switch to XPath. On the pages of the W3, you will find a suitable introduction.: https://www.w3schools.com/xml/xpath_intro.asp 5.2 Developer Tools Recognising the HTML structure and identifying functional selectors can be very difficult on complex websites, especially if the HTML code is not as clearly formatted as in our example. This is where it can be helpful to use the web developer tools built into modern browsers. On the following screenshots, you can see the application in Chromium for Linux on our running example page. However, the procedure is almost identical in other browsers such as Chrome, Firefox or Edge. In all cases, open the Developer Tools by right-clicking in the browser window and then selecting “Inspect Element”. Depending on the setting, the Developer Tools open in a horizontally or vertically separated area of the browser. We see a variety of tabs here, but for this introduction we will concentrate purely on the “Elements” tab. This shows us the HTML code in its hierarchical structure and allows us to expand and collapse individual elements. If we select an element in “Elements”, it will also be marked in the display in the browser window. We can also activate the “Inspector” and select elements directly in the browser window by clicking on it. This in turn selects the corresponding entry in the “Elements” tab. The bottom of the “elements” tab, shows us the full CSS selector belonging to the selected element. By right-clicking on an element and selecting “Copy” -&gt; “Copy selector”, we can also get a CSS selector in the clipboard, which we can then paste into RStudio with CTRL+V. For the element selected above, i.e. the state name for Baden-Württemberg, we get the selector \"#bw\". In this case, this short selector is sufficient to uniquely identify the element. However, as explained above, I recommend the construction of more unique and comprehensible selectors. However, the selector created by the Developer Tools, and especially the display of the full path in the “Elements” tab, can be very helpful in constructing these and allow us to quickly grasp the structure of a website. Also, although we learned how to select the name for Baden-Württemberg, we did not learn, for example, how to select all the names of the federal states. But here, too, the developer tools can be helpful. One approach would be to select several of the name elements one after the other and compare how the full CSS selector changes. If we identify similarities and differences between the name elements, this can give us starting points for formulating our own selector. Another possibility would be to use the SelectorGadget extension instead of the integrated developer tools. Among other things, this enables the selection of several elements at the same time. You can find information on how to use it at:https://selectorgadget.com/ Word of caution: For dynamically generated web sites – which many modern sites are – the developer tools will show us the dynamically generated HTML that the browser “sees”. So we may be presented with tags that are not “really” present in the actual HTML file and thus selectors based on these will not work. We should always also view the page source in the browser and confirm that the tags and their contents are actually “hardcoded” into the HTML file. If they are not, we can not scrape the information using the methods presented in this seminar. There are ways to scrape such sites, most notably the Selenium WebDriver and its R implementation RSelenium, but these methods are beyond the scope of the seminar. "],["rvest2.html", "6 Scraping of tables &amp; dynamic websites 6.1 Scraping of tables 6.2 Dynamic Websites", " 6 Scraping of tables &amp; dynamic websites 6.1 Scraping of tables In web scraping, we will often pursue the goal of transferring the extracted data into a tibble or data frame in order to be able to analyse it further. It is particularly helpful if the data we are interested in is already stored in an HTML table. Because rvest allows us to read out complete tables quickly and easily with the function html_table(). As a reminder, the basic structure of the HTML code for tables is as follows: &lt;table&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;Tag&lt;/th&gt; &lt;th&gt;Effect&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;&quot;b&quot;&lt;/td&gt; &lt;td&gt;bold&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&quot;i&quot;&lt;/td&gt; &lt;td&gt;italics&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; The &lt;table&gt; tag covers the entire table. Rows are defined by &lt;tr&gt;, column headings with &lt;th&gt; and cells with &lt;td&gt;. Before we start scraping, we load the necessary packages as usual: library(tidyverse) library(rvest) 6.1.1 Table with CSS selectors from Wikipedia On the Wikipedia page on “CSS”, there is also a table with CSS selectors. This is our scraping target. First we parse the website: website &lt;- &quot;https://en.wikipedia.org/wiki/CSS&quot; %&gt;% read_html() If we look at the source code and search – CTRL+F – for “&lt;table”, we see that this page contains a large number of HTML tables. These include not only the elements that are recognisable at first glance as “classic” tables, but also, among other things, the “info boxes” at the top right edge of the article or the fold-out lists of further links at the bottom. If you want to look at this more closely, the Web Developer Tools can be very helpful here. Instead of simply selecting all &lt;table&gt; elements on the page, one strategy might be to use the WDTs to create a CSS selector for that specific table: \"table.wikitable:nth-child(42)\". We thus select the table of class \"wikitable\" which is the 42nd child of the parent hierarchy level – &lt;div class=\"mw-parser-output\"&gt;. If we only want to select a single HTML element, it can be helpful to use the function html_element() instead of html_elements(). elements &lt;- website %&gt;% html_elements(css = &quot;table.wikitable:nth-child(42)&quot;) elements ## {xml_nodeset (1)} ## [1] &lt;table class=&quot;wikitable&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Pattern&lt;/th&gt;\\n&lt;th&gt;Matches&lt;/th ... element &lt;- website %&gt;% html_element(css = &quot;table.wikitable:nth-child(42)&quot;) element ## {html_node} ## &lt;table class=&quot;wikitable&quot;&gt; ## [1] &lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Pattern&lt;/th&gt;\\n&lt;th&gt;Matches&lt;/th&gt;\\n&lt;th&gt;First defined&lt;br&gt;i ... The difference is mainly in the output of the function. This is recognisable by the entry inside the { } in the output. In the first case, we get a list of HTML elements – an “xml_nodeset” – even if this list, as here, consists of only one entry. html_element() returns the HTML element itself – the “html_element” – as the function’s output. Why is this relevant? In many cases it can be easier to work directly with the HTML element instead of a list of HTML elements, for example when transferring tables into data frames and tibbles, but more on that later. To read out the table selected in this way, we only need to apply the function html_table() to the HTML element. css_table &lt;- element %&gt;% html_table() css_table %&gt;% head(n = 4) ## # A tibble: 4 × 3 ## Pattern Matches First definedin CSS …¹ ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 E an element of type E 1 ## 2 E:link an E element that is the source anchor o… 1 ## 3 E:active an E element during certain user actions 1 ## 4 E::first-line the first formatted line of an E element 1 ## # ℹ abbreviated name: ¹​`First definedin CSS level` The result is a tibble that contains the scraped contents of the HTML table and adopts the column names stored in the &lt;th&gt; tags for the columns. 6.1.2 Scraping multiple tables It could also be our scraping goal to scrape not only the first, but all four content tables of the Wikipedia article. If we look at the four tables in the source code and/or the WDTs, we see that they all carry the class \"wikitable\". This allows us to select them easily. Please note that the function html_elements() must be used again, as we no longer need just one element, but a list of several selected elements. tables &lt;- website %&gt;% html_elements(css = &quot;table.wikitable&quot;) %&gt;% html_table() The result is a list of four tibbles, each of which contains one of the four tables. If we want to select an individual tibble from the list, for example, to transfer it into a new object, we have to rely on subsetting. We have learned about basic subsetting for vectors using [#], in chapter 1. For lists, things can get a little bit more complicated. There are basically two ways of subsetting lists in R: list_name[#] and list_name[[#]]. The most relevant difference for us is what kind of object R returns to us. In the first case, the returned object is always a list, even if it may only consist of one element. Using double square brackets, on the other hand, returns a single element directly. So the difference is not dissimilar to that between html_elements() and html_element(). For example, if our goal is to select the third tibble from the list of four data frames, which subsetting should we use? tables[3] %&gt;% str() ## List of 1 ## $ : tibble [7 × 2] (S3: tbl_df/tbl/data.frame) ## ..$ Selectors : chr [1:7] &quot;h1 {color: white;}&quot; &quot;p em {color: green;}&quot; &quot;.grape {color: red;}&quot; &quot;p.bright {color: blue;}&quot; ... ## ..$ Specificity: chr [1:7] &quot;0, 0, 0, 1&quot; &quot;0, 0, 0, 2&quot; &quot;0, 0, 1, 0&quot; &quot;0, 0, 1, 1&quot; ... tables[[3]] %&gt;% str() ## tibble [7 × 2] (S3: tbl_df/tbl/data.frame) ## $ Selectors : chr [1:7] &quot;h1 {color: white;}&quot; &quot;p em {color: green;}&quot; &quot;.grape {color: red;}&quot; &quot;p.bright {color: blue;}&quot; ... ## $ Specificity: chr [1:7] &quot;0, 0, 0, 1&quot; &quot;0, 0, 0, 2&quot; &quot;0, 0, 1, 0&quot; &quot;0, 0, 1, 1&quot; ... In the first case, we see that we have a list of length 1, which contains a tibble with 7 lines and 2 variables, as well as further information about these variables. In the second case, we get the tibble directly, i.e. no longer as an element of a list. So we have to use list_name[[]] to directly select a single tibble from a list of tibbles. If we are interested in selecting several elements from a list instead, this is only possible with list_name[]. Instead of selecting an element with a single number, we can select several with a vector of numbers in one step. tables[c(1, 3)] %&gt;% str() ## List of 2 ## $ : tibble [42 × 3] (S3: tbl_df/tbl/data.frame) ## ..$ Pattern : chr [1:42] &quot;E&quot; &quot;E:link&quot; &quot;E:active&quot; &quot;E::first-line&quot; ... ## ..$ Matches : chr [1:42] &quot;an element of type E&quot; &quot;an E element that is the source anchor of a hyperlink whose target is either not yet visited (:link) or already&quot;| __truncated__ &quot;an E element during certain user actions&quot; &quot;the first formatted line of an E element&quot; ... ## ..$ First definedin CSS level: int [1:42] 1 1 1 1 1 1 1 1 1 1 ... ## $ : tibble [7 × 2] (S3: tbl_df/tbl/data.frame) ## ..$ Selectors : chr [1:7] &quot;h1 {color: white;}&quot; &quot;p em {color: green;}&quot; &quot;.grape {color: red;}&quot; &quot;p.bright {color: blue;}&quot; ... ## ..$ Specificity: chr [1:7] &quot;0, 0, 0, 1&quot; &quot;0, 0, 0, 2&quot; &quot;0, 0, 1, 0&quot; &quot;0, 0, 1, 1&quot; ... As a result, we get a list again that contains the two tibbles selected here. 6.1.3 Tabellen mit NAs What happens when we try to read a table with missing values? Consider the following example: https://jakobtures.github.io/web-scraping/table_na.html At first glance, it is already obvious that several cells of the table are unoccupied here. Values are missing. Let’s try to read in the table anyway. table_na &lt;- &quot;https://jakobtures.github.io/web-scraping/table_na.html&quot; %&gt;% read_html %&gt;% html_element(css = &quot;table&quot;) turnout &lt;- table_na %&gt;% html_table() turnout ## # A tibble: 16 × 3 ## Bundesland Wahljahr Wahlbeteiligung ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Baden-Württemberg 2016 NA ## 2 Bayern 2018 NA ## 3 Berlin NA 66.9 ## 4 Brandenburg 61.3 NA ## 5 Bremen 2019 64.1 ## 6 Hamurg 2020 63.2 ## 7 Hessen 2018 67.3 ## 8 Mecklenburg-Vorpommern 2016 61.6 ## 9 Niedersachsen 2017 63.1 ## 10 Nordrhein-Westfalen 2017 65.2 ## 11 Rheinland-Pfalz 2016 70.4 ## 12 Saarland 2017 69.7 ## 13 Sachsen 2019 66.6 ## 14 Sachsen-Anhalt 2016 61.1 ## 15 Schleswig-Holstein 2017 64.2 ## 16 Thüringen 2019 64.9 As we can see, html_table filled four cells with with NA. This stands for “Not Available” and represents missing values in R. However, there are different types of missing values in the HTML source code, which the automatic repair implemented in html_table() handles differently. Let’s first look at the source code of the first two lines: &lt;tr&gt; &lt;td&gt;Baden-Württemberg&lt;/td&gt; &lt;td&gt;2016&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Bayern&lt;/td&gt; &lt;td&gt;2018&lt;/td&gt; &lt;/tr&gt; In both cases the value for turnout is missing. For “Baden-Württemberg”, we see that the third column is created in the HTML code, but there is no content in this cell. html_table() knows, that this empty cell has to be filled with a NA. In contrast, for “Bayern” the cell is completely missing. This means that the second row of the table consists of only two columns, while the rest of the table has three columns. In this case, html_table() could draw the correct conclusion and filled the missing third column with an NA. But let’s also look at the third and fourth rows in the source code: &lt;tr&gt; &lt;td&gt;Berlin&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;66.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Brandenburg&lt;/td&gt; &lt;td&gt;61.3&lt;/td&gt; &lt;/tr&gt; The second column is missing in both cases. In the first case it is created but empty, in the second it does not exist. In the first case, html_table() can again handle it without any problems. For “Brandenburg”, however, the function reaches its limits. We, as human observers, quickly realise that the last state election in Brandenburg did not take place in 61.3 and that this must therefore be the turnout. R cannot distinguish this so easily and takes 61.3 as the value for the column “Election year” and inserts a NA in the third column. What to do? First of all, we should be aware that such problems exist. So we should check if such a problem exists and whether the option to have it fixed automatically will actually get us there. If this is not the case, we can at least correct the problems that arise after extraction. Our problem lies exclusively in row four. Its second column must be moved to the third and the second must then itself be set as NA. For this we need subsetting again. In the case of a tibble, we need to specify the row and column in the form tbl[row, column] to select a cell. So we can tell R: “Write in cell three the content of cell two, and then write in cell two NA”. turnout[4, 3] &lt;- turnout[4, 2] turnout[4, 2] &lt;- NA turnout %&gt;% head(n = 4) ## # A tibble: 4 × 3 ## Bundesland Wahljahr Wahlbeteiligung ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Baden-Württemberg 2016 NA ## 2 Bayern 2018 NA ## 3 Berlin NA 66.9 ## 4 Brandenburg NA 61.3 6.2 Dynamic Websites In the “reality” of the modern internet, we will increasingly encounter websites that are no longer based exclusively on static HTML files, but generate content dynamically. You know this, for example, in the form of timelines in social media offerings that are generated dynamically based on your user profile. Other websites may generate the displayed content with JavaScript functions or in response to input in HTML forms. In many of these cases, it is no longer sufficient from a web scraping perspective to parse an HTML page and extract the data you are looking for, as this is often not contained in the HTML source code but is loaded dynamically in the background. The good news is that there are usually ways of scraping the information anyway. Perhaps the operator of a page or service offers an API (Application Programming Interface). In this case, we can register for access to this interface and then get access to the data of interest. This is possible with Twitter, for example. In other cases, we may be able to identify in the embedded scripts how and from which database the information is loaded and access it directly. Or we use the Selenium WebDriver to “remotely control” a browser window and scrape what the browser “sees”. However, all of these approaches are advanced methods that are beyond the scope of this introduction. But in cases where an HTML file is dynamically generated based on input into a HTML form, we can often (not always) read it using the methods we already know. 6.2.1 HTML forms and HTML queries As an example, let’s first look at the OPAC catalogue of the Potsdam University Library https://opac.ub.uni-potsdam.de/ in the browser. If we enter the term “test” in the search field and click on Search, the browser window will show us the results of the search query. But what actually interests us here is the browser’s address bar. Instead of the URL “https://opac.ub.uni-potsdam.de/”, there is now a much longer URL. Note that the exact URL may very well differ for you, but the basic form should be similar to: “https://opac.ub.uni-potsdam.de/DB=1/LNG=DU/SID=3f0e2b15-1/CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test”. The first part is obviously still the URL of the website called up: “https://opac.ub.uni-potsdam.de/”. Let’s call this the base URL. However, the part “CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test” was added to the end of the URL. This is the HTML query we are interested in here. Between the base URL and the query there are one or more components, which in this case may also differ depending on your browser. However, these are also irrelevant for the actual search query. We can shorten the URL to “https://opac.ub.uni-potsdam.de/CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test” and receive the same result. A query is a request in which data from an HTML form is sent to the server. In response, the server generates a new website, which is sent back to the user and displayed in the browser. In this case, the query was triggered by clicking on the “Search” button. If we understand what the components of the query do, we could manipulate it and use it specifically to have a website of interest created and parsed. 6.2.2 HTML forms To do this, we first need to take a look at the HTML code of the search form. To understand this, you should display the source code of the page and search for “&lt;form” or use the WDTs to look at the form and its components. &lt;form action=&quot;CMD&quot; class=&quot;form&quot; name=&quot;SearchForm&quot; method=&quot;GET&quot;&gt; ... &lt;/form&gt; HTML forms are encompassed by the &lt;form&gt; tag. Within the tag, one or more form elements such as text entry fields, drop-down option lists, buttons, etc. can be placed. &lt;form&gt; itself carries a number of attributes in this example. The first attribute of interest to us is the method=\"GET\" attribute. This specifies the method of data transfer between client and server. It is important to note that the method “GET” uses queries in the URL for the transmission of data and the method “POST” does not. We can therefore only manipulate queries in this way, if the “GET” method is used. If no method is specified in the &lt;form&gt; tag, “GET” is also used as the default. The second attribute of interest to us is action=\"CMD\". This specifies which action should be triggered after the form has been submitted. Often the value of action= is the name of a file on the server to which the data will be sent and which then returns a dynamically generated HTML page back to the user. Let us now look at the elements of the form. For this, the rvest function html_form() can be helpful. &quot;https://opac.ub.uni-potsdam.de/&quot; %&gt;% read_html() %&gt;% html_element(css = &quot;form&quot;) %&gt;% html_form() ## &lt;form&gt; &#39;SearchForm&#39; (GET https://opac.ub.uni-potsdam.de/CMD) ## &lt;field&gt; (select) ACT: SRCHA ## &lt;field&gt; (select) IKT: ## &lt;field&gt; (select) SRT: ## &lt;field&gt; (checkbox) FUZZY: Y ## &lt;field&gt; (text) TRM: ## &lt;field&gt; (submit) : Suchen The output shows us in the first line the name of the form and the action that is performed on submit: “GET https://opac.ub.uni-potsdam.de/CMD”. The other six lines show the form components: The three drop-down selections for: type of search which fields should be searched how results should be ordered The Checkbox for “unscharfe Suche” The text field where we enter terms to be searched The search button itself We also see the names of these components as well as in some cases the default value that is sent when the form is submitted, as long as no other value is selected or entered. Let’s look at some of these elements. &lt;select&gt; elements are drop-down lists of options that can be selected. This is the source code for the first &lt;select&gt; element in our example: &lt;select name=&quot;ACT&quot;&gt; &lt;OPTION VALUE=&quot;SRCH&quot;&gt;suchen [oder] &lt;OPTION VALUE=&quot;SRCHA&quot; SELECTED&gt;suchen [und] &lt;OPTION value=&quot;AND&quot;&gt;eingrenzen &lt;OPTION value=&quot;OR&quot;&gt;erweitern &lt;OPTION value=&quot;NOT&quot;&gt;ausgenommen &lt;OPTION value=&quot;RLV&quot;&gt;neu ordnen &lt;OPTION value=&quot;BRWS&quot;&gt;Index bl&amp;auml;ttern &lt;/select&gt; The attribute name=\"ACT\" defines the elements name, which is used when transmitting the data from the form via the query. The &lt;option&gt; tags define the selectable options, i.e. the drop down menu. &lt;value=\"\"&gt; represents the value transmitted by the form. The user is being shown the text following the tag. The default selection is either the first value in the list or – like in this case – the option with the attribute selected is being explicitly chosen as the default. The three other elements are &lt;input&gt; tags. Input fields whose specific type is specified via the attribute type=\"\". These can be, for example, text boxes (type=\"text\") or checkboxes (input=\"checkbox\"), but there are many more options available. A comprehensive list can be found at: https://www.w3schools.com/html/html_form_input_types.asp. Here is the source code for two of the three &lt;input&gt; elements on the example page: &lt;input type=&quot;text&quot; name=&quot;TRM&quot; value=&quot;&quot; size=&quot;50&quot;&gt; ... &lt;input type=&quot;submit&quot; class=&quot;button&quot; value=&quot; Suchen &quot;&gt; The first tag is of the type “text”, i.e. a text field, in this case the text field into which the search term is entered. In addition to the name of the element, a default value of the field is specified via value=\"\". In this case, the default value is an empty field. The second tag is of the type “submit”. This is the “Search” button, which triggers the transmission of the form data via the query by clicking on it. 6.2.3 The query But what exactly is being transmitted? Let’s look again at the example query from above: CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test The value of the action=\"\" attribute forms the first part of the query and is appended after the base URL. The value of the attribute tells the server what to do with the other transmitted data. This is followed by a ?, which introduces the data to be transmitted as several pairs of name=\"\" and value=\"\" attributes of the individual elements. The pairs are connected with &amp;. ACT=SRCHA thus stands for the fact that the (default) value “SRCHA” has been selected in the element with the name “ACT”. What the values of the two other &lt;select&gt; elements “IKT” and “SRT” stand for, you can understand yourself with a look into the source code or the WDTs. These are not important for our endeavour. The text entered in the field is transmitted as the value of the &lt;input type=\"text\"&gt; tag with the name “TRM”. Here the value was “test”. The server receives the form data in this way, can then take a decision on the basis of the action=\"\" attribute, here “CMD”, how the data is to be processed and constructs the website accordingly, which it sends back to us and which is displayed in our browser. 6.2.4 Manipulating the query and scraping the result Now that we know what the components of the query mean, we can manipulate them. Instead of writing queries by hand, we should use R to combine them for us. We will also encounter the technique of manipulating URLs directly in the R code more often. So we should learn it early. The function str_c() from stringr (core tidyverse) combines the strings listed as arguments into a single string. Strings stored in other R objects can also be included. If we have the goal of manipulating both the search method and the search term, we could achieve this in this way: base_url &lt;- &quot;https://opac.ub.uni-potsdam.de/&quot; method &lt;- &quot;SRCHA&quot; term &lt;- &quot;test&quot; url &lt;- str_c(base_url, &quot;CMD?ACT=&quot;, method, &quot;&amp;IKT=1016&amp;SRT=YOP&amp;TRM=&quot;, term) url ## [1] &quot;https://opac.ub.uni-potsdam.de/CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test&quot; If we now change the strings stored in the method and term objects and generate the complete URL again, these components of the query are manipulated accordingly. method &lt;- &quot;SRCH&quot; term &lt;- &quot;web+scraping&quot; url &lt;- str_c(base_url, &quot;CMD?ACT=&quot;, method, &quot;&amp;IKT=1016&amp;SRT=YOP&amp;TRM=&quot;, term) url ## [1] &quot;https://opac.ub.uni-potsdam.de/CMD?ACT=SRCH&amp;IKT=1016&amp;SRT=YOP&amp;TRM=web+scraping&quot; The search method was set to the value “SRCH”, i.e. an “OR” search, the search term to “web scraping”. It is important to note that no spaces may appear in the query and that these are replaced by “+” when the form is submitted. So instead of “web scraping” we have to use the string “web+scraping”. As an example application, we can now have the server perform an “AND” search for the term “web scraping”, read out the HTML page generated by the server and extract the 10 titles displayed. base_url &lt;- &quot;https://opac.ub.uni-potsdam.de/&quot; method &lt;- &quot;SRCHA&quot; term &lt;- &quot;web+scraping&quot; url &lt;- str_c(base_url, &quot;CMD?ACT=&quot;, method, &quot;&amp;IKT=1016&amp;SRT=YOP&amp;TRM=&quot;, term) url ## [1] &quot;https://opac.ub.uni-potsdam.de/CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=web+scraping&quot; website &lt;- url %&gt;% read_html() The search results are displayed as tables in the generated HTML file. The &lt;table&gt; tag has the attribute-value combination summary=\"hitlist\", which we can use for our CSS selector: hits &lt;- website %&gt;% html_element(css = &quot;table[summary=&#39;hitlist&#39;]&quot;) %&gt;% html_table() %&gt;% as_tibble() hits %&gt;% head(n=10) ## # A tibble: 10 × 4 ## X1 X2 X3 X4 ## &lt;lgl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 NA NA &quot;&quot; NA ## 2 NA 1 &quot;Estimating House Prices in Emerging Markets and Developin… NA ## 3 NA NA &quot;&quot; NA ## 4 NA 2 &quot;Elgar encyclopedia of technology and politics/ Ceron, And… NA ## 5 NA NA &quot;&quot; NA ## 6 NA 3 &quot;The Beginner&#39;s Guide to Data Science/ Ball, Robert. - 1st… NA ## 7 NA NA &quot;&quot; NA ## 8 NA 4 &quot;From #Hashtags to Legislation : Engagement and Support fo… NA ## 9 NA NA &quot;&quot; NA ## 10 NA 5 &quot;Disaggregating China, Inc : State Strategies in the Liber… NA This worked, but we see that the table consists mainly of empty rows and cells. These are invisible on the website, but are used to format the display. Instead of repairing the table afterwards, it makes more sense to extract only the cells that contain the information we are looking for. These are the &lt;td&gt; tags with class=\"hit\" and the attribute-value combination align=\"left\". On this basis, we can construct a unique CSS selector. hits &lt;- website %&gt;% html_elements(css = &quot;td.hit[align=&#39;left&#39;]&quot;) %&gt;% html_text(trim = TRUE) hits %&gt;% head(n = 5) ## [1] &quot;Estimating House Prices in Emerging Markets and Developing Economies : A Big Data Approach/ Behr, Daniela M.. - Washington, D.C : The World Bank, 2023&quot; ## [2] &quot;Elgar encyclopedia of technology and politics/ Ceron, Andrea. - Cheltenham, UK : Edward Elgar Publishing, 2022&quot; ## [3] &quot;The Beginner&#39;s Guide to Data Science/ Ball, Robert. - 1st ed. 2022. - Cham : Springer International Publishing, 2022&quot; ## [4] &quot;From #Hashtags to Legislation : Engagement and Support for Economic Reforms in the Gulf Cooperation Council Countries/ Arezki, Rabah. - Washington, D.C : The World Bank, 2022&quot; ## [5] &quot;Disaggregating China, Inc : State Strategies in the Liberal Economic Order/ Tan, Yeling. - [Online-Ausgabe]. - Ithaca, NY : Cornell University Press, [2022]&quot; 6.2.5 Additional resources In order to process this information further and, for example, separate it into data on author, title, year, etc., advanced knowledge in dealing with strings is necessary, which unfortunately goes beyond the scope of this introduction. A good first overview can be found in the chapter “Strings” from “R for Data Science” by Wickham and Grolemund: https://r4ds.had.co.nz/strings.html The appropriate “cheat sheet” is also recommended: https://raw.githubusercontent.com/rstudio/cheatsheets/master/strings.pdf "],["rvest3.html", "7 Scraping of multi-page websites 7.1 Index-pages 7.2 Pagination", " 7 Scraping of multi-page websites In many cases, we do not want to scrape the content of a single website, but several sub-pages in one step. In this session we will look at two common variations. Index pages and pagination. 7.1 Index-pages An index page, in this context, is a website on which links to the various sub-pages are listed. We can think of this as a table of contents. The website for the tidyverse packages serves as an example: https://www.tidyverse.org/packages/. Under the point “Core tidyverse” the eight packages are listed, which are loaded in R with library(tidyverse). In addition to the name and icon, a short description of the package and a link to further information, are part of the list. Let’s look at one of the sub-pages for the core packages. Since they all have the same structure, you can choose any package as an example. It could be our scraping goal to create a table with the names of the core packages, the current version number, and the links to CRAN and the matching chapter in “R for Data Science” by Wickham and Grolemund. By now we have all the tools to extract this data from the websites. We could now “manually” scrape the individual sub-pages and merge the data. It would be more practical, however, if we could start from the index page and scrape all eight sub-pages and the data of interest they contain in one step. This is exactly what we will look at in the following. 7.1.1 Scraping of the index library(tidyverse) library(rvest) As a first step, we need to extract the links to the sub-pages from the source code of the index page. As always, we download the website and parse it. website &lt;- &quot;https://www.tidyverse.org/packages/&quot; %&gt;% read_html() In this case, the links are stored twice in the source code. In one case the image of the icon is linked, in the other the name of the package. You can follow this in the source code and/or with the WDTs yourself by now. However, we need each link only once. One of several ways to select them could be to select the &lt;a&gt; tags that directly follow the individual &lt;div class=\"package\"&gt; tags. a_elements &lt;- website %&gt;% html_elements(css = &quot;div.package &gt; a&quot;) a_elements ## {xml_nodeset (8)} ## [1] &lt;a href=&quot;https://ggplot2.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class ... ## [2] &lt;a href=&quot;https://dplyr.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class=&quot; ... ## [3] &lt;a href=&quot;https://tidyr.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class=&quot; ... ## [4] &lt;a href=&quot;https://readr.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class=&quot; ... ## [5] &lt;a href=&quot;https://purrr.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class=&quot; ... ## [6] &lt;a href=&quot;https://tibble.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class= ... ## [7] &lt;a href=&quot;https://stringr.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class ... ## [8] &lt;a href=&quot;https://forcats.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class ... Since we need the actual URLs to be able to read out the sub-pages in the following, we should now extract the values of the href=\"\" attributes. links &lt;- a_elements %&gt;% html_attr(name = &quot;href&quot;) links ## [1] &quot;https://ggplot2.tidyverse.org/&quot; &quot;https://dplyr.tidyverse.org/&quot; ## [3] &quot;https://tidyr.tidyverse.org/&quot; &quot;https://readr.tidyverse.org/&quot; ## [5] &quot;https://purrr.tidyverse.org/&quot; &quot;https://tibble.tidyverse.org/&quot; ## [7] &quot;https://stringr.tidyverse.org/&quot; &quot;https://forcats.tidyverse.org/&quot; 7.1.2 Iteration with map() Before starting to parse the sub-pages, we must think about how we can get R to apply these steps automatically to several URLs one after the other. One possibility from base R would be to apply a “For Loop”. However, I would like to introduce the map() functions family, from the tidyverse package purrr. These follow the basic logic of the tidyverse, can easily be included in pipes and have a short and intuitively understandable syntax. The map() function takes a vector or list as input, applies a function specified in the second argument to each of the elements of the input, and returns to us a list of the results of the applied function. x &lt;- c(1.28, 1.46, 1.64, 1.82) map(x, round) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 1 ## ## [[3]] ## [1] 2 ## ## [[4]] ## [1] 2 For each element of the numerical vector x, map() individually applies the function round(). round() does what the name suggests, and rounds the input up or down, depending on the numerical value. As a result, map() returns a list. If we want to have a vector as output, we can use specific variants of the map functions depending on the desired type – logical, integer, double or character. Here is a quote from the help on ?map: “map_lgl(), map_int(), map_dbl() and map_chr() return an atomic vector of the indicated type (or die trying).” For example, if we want to have a numeric vector instead of a list as output for the above example, we can use map_dbl(): x &lt;- c(1.28, 1.46, 1.64, 1.82) map_dbl(x, round) ## [1] 1 1 2 2 Or for a character vector we can apply map_chr(). The function toupper() used here, puts returns the input as uppercase letters. x &lt;- c(&quot;abc&quot;, &quot;def&quot;, &quot;gah&quot;) map_chr(x, toupper) ## [1] &quot;ABC&quot; &quot;DEF&quot; &quot;GAH&quot; If we want to change the arguments of the applied function, the arguments are listed after the name of the function. Here, the number of decimal places to be rounded is set from the default value of 0 to 1. x &lt;- c(1.28, 1.46, 1.64, 1.82) map_dbl(x, round, digits = 1) ## [1] 1.3 1.5 1.6 1.8 This gives us an overview of iteration with map(), but this can necessarily only be a first introduction. For a more detailed introduction to For Loops and the map functions, I recommend the chapter on “Iteration” from “R for Data Science”: https://r4ds.had.co.nz/iteration.html For a more interactive German introduction, I recommend the section “Schleifen” in the StartR app by Fabian Class: https://shiny.lmes.uni-potsdam.de/startR/#section-schleifen 7.1.3 Scraping the sub-pages We can now use map() to parse all sub-pages in one step. As input, we use the character vector that contains the URLs of the sub-pages, and as the function to be applied, the familiar read_html(). For each of the eight URLs, the function is applied to the respective URL one after the other. As output we get a list of the eight parsed sub-pages. pages &lt;- links %&gt;% map(read_html) If we look at the sub-pages in the browser, we can see that the HTML structure is identical for each sub-page in terms of the information we are interested in – name, version number and CRAN as well as “R for Data Science” links. We can therefore extract the data for each of them using the same CSS selectors. pages %&gt;% map(html_element, css = &quot;a.navbar-brand&quot;) %&gt;% map_chr(html_text) ## [1] &quot;ggplot2&quot; &quot;dplyr&quot; &quot;tidyr&quot; &quot;readr&quot; &quot;purrr&quot; &quot;tibble&quot; &quot;stringr&quot; ## [8] &quot;forcats&quot; The name of the package is displayed in the menu bar in the upper section of the pages. This is enclosed by an &lt;a&gt; tag. For example, for https://ggplot2.tidyverse.org/ this is: &lt;a class=\"navbar-brand\" href=\"index.html\"&gt;ggplot2&lt;/a&gt;. The CSS selector used here is one of the possible options to retrieve the desired information. So what happens in detail in the code shown? The input is the previously created list with the eight parsed websites. In the second line, by using map(), the function html_element() with the argument css = \"a.navbar-brand\" is applied to each of the parsed pages. For each of the eight pages, the corresponding HTML-element is selected in turn. These are passed through the pipe to the third line, where iteration is again performed over each element, this time using the familiar function html_text(). For each of the eight selected elements, the text between the start and end tag is extracted. Since map_chr() is used here, a character vector is returned as output. pages %&gt;% map(html_element, css = &quot;small.nav-text.text-muted.me-auto&quot;) %&gt;% map_chr(html_text) ## [1] &quot;3.4.4&quot; &quot;1.1.3&quot; &quot;1.3.0&quot; &quot;2.1.4&quot; &quot;1.0.2&quot; &quot;3.2.1&quot; &quot;1.5.0&quot; &quot;1.0.0&quot; The extraction of the current version number of the packages works the same way. For ggplot2, these are contained in the following tag: &lt;small class=\"nav-text text-muted me-auto\" data-bs-toggle=\"tooltip\" data-bs-placement=\"bottom\" title=\"Released version\"&gt;3.3.5&lt;/small&gt;. The &lt;small&gt; tag is used for smaller than normal text, which here results in the small version number written after the package’s name. Looking closely at the tag reveals an interesting detail. Namely, the class name contains spaces. This indicates that the &lt;small&gt; tag carries the classes nav-text, text-muted and me-auto. We can select the tag by attaching all class names to small in the selector, combined with dots between them. Strictly speaking, however, we do not need to do this here. Each class name by itself would be sufficient for selection, as they do not appear anywhere else on the website. In terms of the most explicit CSS selectors possible, I would still recommend to use all three class names, but this is also a matter of taste. pages %&gt;% map(html_element, css = &quot;ul.list-unstyled &gt; li:nth-child(1) &gt; a&quot;) %&gt;% map_chr(html_attr, name = &quot;href&quot;) ## [1] &quot;https://cloud.r-project.org/package=ggplot2&quot; ## [2] &quot;https://cloud.r-project.org/package=dplyr&quot; ## [3] &quot;https://cloud.r-project.org/package=tidyr&quot; ## [4] &quot;https://cloud.r-project.org/package=readr&quot; ## [5] &quot;https://cloud.r-project.org/package=purrr&quot; ## [6] &quot;https://cloud.r-project.org/package=tibble&quot; ## [7] &quot;https://cloud.r-project.org/package=stringr&quot; ## [8] &quot;https://cloud.r-project.org/package=forcats&quot; pages %&gt;% map(html_element, css = &quot;ul.list-unstyled &gt; li:nth-child(4) &gt; a&quot;) %&gt;% map_chr(html_attr, name = &quot;href&quot;) ## [1] &quot;https://r4ds.had.co.nz/data-visualisation.html&quot; ## [2] &quot;http://r4ds.had.co.nz/transform.html&quot; ## [3] &quot;https://r4ds.had.co.nz/tidy-data.html&quot; ## [4] &quot;http://r4ds.had.co.nz/data-import.html&quot; ## [5] &quot;http://r4ds.had.co.nz/iteration.html&quot; ## [6] &quot;https://r4ds.had.co.nz/tibbles.html&quot; ## [7] &quot;http://r4ds.had.co.nz/strings.html&quot; ## [8] &quot;http://r4ds.had.co.nz/factors.html&quot; The extraction of the links also follows the same basic principle. The selectors are a little more complicated, but can easily be understood looking in the source code and/or using the WDTs. We select the &lt;a&gt; tags of the first and fourth &lt;li&gt; children of the unordered list with the class list-unstyled. Here we apply the function html_attr() with the argument name = \"href\" to each of the eight selected elements to get the data of interest, the URLs of the links. If we are only interested in the final result, we can also extract the data of the sub-pages directly during the creation of a tibble: tibble( name = pages %&gt;% map(html_element, css = &quot;a.navbar-brand&quot;) %&gt;% map_chr(html_text), version = pages %&gt;% map(html_element, css = &quot;small.nav-text.text-muted.me-auto&quot;) %&gt;% map_chr(html_text), CRAN = pages %&gt;% map(html_element, css = &quot;ul.list-unstyled &gt; li:nth-child(1) &gt; a&quot;) %&gt;% map_chr(html_attr, name = &quot;href&quot;), Learn = pages %&gt;% map(html_element, css = &quot;ul.list-unstyled &gt; li:nth-child(4) &gt; a&quot;) %&gt;% map_chr(html_attr, name = &quot;href&quot;) ) ## # A tibble: 8 × 4 ## name version CRAN Learn ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 ggplot2 3.4.4 https://cloud.r-project.org/package=ggplot2 https://r4ds.had.… ## 2 dplyr 1.1.3 https://cloud.r-project.org/package=dplyr http://r4ds.had.c… ## 3 tidyr 1.3.0 https://cloud.r-project.org/package=tidyr https://r4ds.had.… ## 4 readr 2.1.4 https://cloud.r-project.org/package=readr http://r4ds.had.c… ## 5 purrr 1.0.2 https://cloud.r-project.org/package=purrr http://r4ds.had.c… ## 6 tibble 3.2.1 https://cloud.r-project.org/package=tibble https://r4ds.had.… ## 7 stringr 1.5.0 https://cloud.r-project.org/package=stringr http://r4ds.had.c… ## 8 forcats 1.0.0 https://cloud.r-project.org/package=forcats http://r4ds.had.c… 7.2 Pagination Another common form of dividing a website into several sub-pages is pagination. You all know this from everyday life on the internet. We enter a search term in Google and get results that are divided over several pages. These are accessible and navigable via the numbered links and the forward/back arrows at the bottom of the browser content. This is “pagination in action” and we encounter similar variants on many websites. In the press release archive of the website of the Brandenburg state parliament, pagination is used to distribute the releases over several sub-pages. To illustrate the scraping of such a website, we can, for example, aim at scraping the date, title and further link for all press releases from 2021 and summarise them in a tibble. You can find the website at: https://www.landtag.brandenburg.de/de/pressemitteilungen_2021/27025 7.2.1 The query Let’s first look at the page in the browser and examine what happens when we click through the sub-pages. If we select the second sub-page, the URL in the browser window changes. We see that the end of the URL is extended from “27025” to “27025?skip=12”. So a query is sent to the server and the corresponding sub-page is sent back and displayed. On the third sub-page, the query changes again to “27025?skip=24”. What could “skip=12” mean? If we look at the list of press releases, we see that exactly 12 releases are displayed on each sub-page. We can therefore assume that “skip=12” instructs the server to skip the first 12 releases and thus display entries 13-24. “skip=24” then skips the first 24 and so on. The principle can be further confirmed by testing what “skip=0” triggers. The first sub-page is displayed again. So “27025” is actually functionally equivalent to “27025?skip=0”. With this we already know that we will be able to manipulate the URLs directly from Rstudio and thus scrape the sub-pages. 7.2.2 Scraping the sub-pages Before we start scraping all the press releases, we first need to find out how many sub-pages there are. The easiest way would be to do this by eye. We can see in the browser that the highest selectable sub-page is “13”. But we can also find this out in the scraping process itself. This has several advantages. We might not only want to scrap the messages from 2021, but those from several or all years. To do this, we would have to check for each year in the browser how many sub-pages there are and adjust this accordingly. If we extract the page number in the R code, this can be easily generalised to other years with different page numbers. The pagination “buttons” are contained in the HTML code in an unordered list (&lt;ul&gt;) carrying the class pagination. The penultimate list element &lt;li&gt; contains the page number of the last page, here “13”. Please note that the last list element is the “Forward” button. With this information we can construct a selector and extract the highest page number, as the second to last list element. website &lt;- &quot;https://www.landtag.brandenburg.de/de/pressemitteilungen_2021/27025&quot; %&gt;% read_html() max &lt;- website %&gt;% html_element(css = &quot;ul.pagination &gt; li:nth-last-child(2)&quot;) %&gt;% html_text() %&gt;% as.numeric() In the last line of code above, you again see the function as.numeric(). Remember, that html_text() always returns a character vector. Since we need a numeric value to be able to calculate with it in R, we have to convert it into a number. Now we can start constructing the links to all sub-pages directly in our R script. To do this, we need two components that we can then combine to create the URLs. First we have to define the constant part of the URL, the base URL. In this case, this is the complete URL up to “?skip=” inclusive. In addition, we need the values that are inserted after “?skip=”. We can easily calculate these. Each sub-page contains 12 press releases. So we can multiply the number of the sub-page by 12, but then we have to subtract another 12, because the 12 press releases shown on the current sub-page should not be skipped. So for page 1 we calculate: \\(1 ∗ 12 - 12 = 0\\), for page 2: \\(2 ∗ 12 - 12 = 12\\) and so on. To do this in one step for all sub-pages, we can use 1:max * 12 - 12 to instruct R to repeat the calculation for all numbers from 1 to the maximum value – which we previously stored in the object max. : stands for “from-to”. In this way we get a numeric vector with the values for “?skip=”. In the third step we can combine the base URL and the calculated values with str_c() to complete URLs and parse them in the fourth step with map(). base_url &lt;- &quot;https://www.landtag.brandenburg.de/de/pressemitteilungen_2021/27025?skip=&quot; skips &lt;- 1:max * 12 - 12 skips ## [1] 0 12 24 36 48 60 72 84 96 108 120 132 144 links &lt;- str_c(base_url, skips) pages &lt;- links %&gt;% map(read_html) Now we can extract the data we are interested in. The press releases are part of an unordered list with the class list-entries. Each individual list entry contains the date of the release, the title and the link to the full text. Let’s start with extracting the date of the press release. This is enclosed by a &lt;div&gt; tag of the class list-entry-date. With map() we first select the corresponding elements and in the next step extract the text of these elements. As there is a lot of whitespace around the dates, trim = TRUE should be used. Since at this point a list of lists is returned, and this would be unnecessarily complicated in further processing, we can use unlist() to dissolve the list and receive a character vector as output. pages %&gt;% map(html_elements, css = &quot;div.list-entry-date&quot;) %&gt;% map(html_text, trim = TRUE) %&gt;% unlist() %&gt;% head(n = 5) ## [1] &quot;30.12.2021&quot; &quot;22.12.2021&quot; &quot;13.12.2021&quot; &quot;13.12.2021&quot; &quot;10.12.2021&quot; However, we can go one step further and store the data as a vector of the type “Date”. This could be advantageous for potential further analyses. The tidyverse package lubridate makes it easy to convert dates from character or numeric vectors into the “Date” format. The package is not part of the core tidyverse and has to be loaded explicitly. Among other things, it offers a number of functions in the form dmy(), mdy(), ymd() and so on. d stands for “day”, m for “month” and y for “year”. With the order in which the letters appear in the function name, we tell R which format the data we want to convert to the “date” format has. On the website of the Brandenburg state parliament, the dates are written in the form Day.Month.Year, which is typical in Germany. So we use the function dmy(). If, for example, they were in the form Month.Day.Year, which is typical in the USA, we would have to use mdy() accordingly. It is irrelevant whether the components of the date are separated with “.”, “/”, “-” or spaces. Even written out or abbreviated month names can be processed by lubridate. library(lubridate) pages %&gt;% map(html_elements, css = &quot;div.list-entry-date&quot;) %&gt;% map(html_text, trim = TRUE) %&gt;% unlist() %&gt;% dmy() %&gt;% head(n = 5) ## [1] &quot;2021-12-30&quot; &quot;2021-12-22&quot; &quot;2021-12-13&quot; &quot;2021-12-13&quot; &quot;2021-12-10&quot; More about the handling of dates and times in R as well as the further possibilities opened up by lubridate, can be found in the corresponding chapter in “R for Data Science”: https://r4ds.had.co.nz/dates-and-times.html Next, we can extract the titles of the press releases. These are also enclosed by a &lt;div&gt; tag, this time carrying the clas list-entry-title. pages %&gt;% map(html_elements, css = &quot;div.list-entry-title&quot;) %&gt;% map(html_text, trim = TRUE) %&gt;% unlist() %&gt;% head(n = 5) ## [1] &quot;Termine des Landtages Brandenburg in der Zeit vom 3. bis 7. Januar 2022&quot; ## [2] &quot;Hinweise für Medien zur Sondersitzung des Landtages am 23. Dezember 2021&quot; ## [3] &quot;Hinweise für Medien zu den Plenarsitzungen des Landtages vom 15. bis 17. Dezember 2021&quot; ## [4] &quot;Landtag Brandenburg trauert um seinen ersten Präsidenten Dr. Herbert Knoblich&quot; ## [5] &quot;Termine des Landtages Brandenburg in der Zeit vom 13. bis 19. Dezember 2021&quot; The last thing to extract, are the links to the individual messages. These are included in &lt;a&gt; tags with the class list-entry. pages %&gt;% map(html_elements, css = &quot;a.list-entry&quot;) %&gt;% map(html_attr, name = &quot;href&quot;) %&gt;% unlist() %&gt;% head(n = 5) ## [1] &quot;/de/meldungen/termine_des_landtages_brandenburg_in_der_zeit_vom_3._bis_7._januar_2022/27018&quot; ## [2] &quot;/de/meldungen/hinweise_fuer_medien_zur_sondersitzung_des_landtages_am_23._dezember_2021/26986&quot; ## [3] &quot;/de/meldungen/hinweise_fuer_medien_zu_den_plenarsitzungen_des_landtages_vom_15._bis_17._dezember_2021/26897&quot; ## [4] &quot;/de/meldungen/landtag_brandenburg_trauert_um_seinen_ersten_praesidenten_dr._herbert_knoblich/26894&quot; ## [5] &quot;/de/meldungen/termine_des_landtages_brandenburg_in_der_zeit_vom_13._bis_19._dezember_2021/26864&quot; But, the links stored in the HTML code only describe a part of the complete URL. We could now construct the complete URLs again with str_c(). However, we still need a new concept for this. The pipe passes the result of a step along to the next line. If we use str_c() within the pipe, it receives the extracted end part of the URLs as the first argument. str_c(\"https://www.landtag.brandenburg.de\") would therefore lead to the end part of the URL being appended before “https://www.landtag.brandenburg.de”. However, we want this to happen the other way round. To do this, we need to tell str_c(), to use the data passed through the pipe as the second argument. We can achieve this by using .. . refers to the data passed through the pipe. In this way we can combine the URLs correctly: pages %&gt;% map(html_elements, css = &quot;a.list-entry&quot;) %&gt;% map(html_attr, name = &quot;href&quot;) %&gt;% unlist() %&gt;% str_c(&quot;https://www.landtag.brandenburg.de&quot;, .) %&gt;% head(n = 5) ## [1] &quot;https://www.landtag.brandenburg.de/de/meldungen/termine_des_landtages_brandenburg_in_der_zeit_vom_3._bis_7._januar_2022/27018&quot; ## [2] &quot;https://www.landtag.brandenburg.de/de/meldungen/hinweise_fuer_medien_zur_sondersitzung_des_landtages_am_23._dezember_2021/26986&quot; ## [3] &quot;https://www.landtag.brandenburg.de/de/meldungen/hinweise_fuer_medien_zu_den_plenarsitzungen_des_landtages_vom_15._bis_17._dezember_2021/26897&quot; ## [4] &quot;https://www.landtag.brandenburg.de/de/meldungen/landtag_brandenburg_trauert_um_seinen_ersten_praesidenten_dr._herbert_knoblich/26894&quot; ## [5] &quot;https://www.landtag.brandenburg.de/de/meldungen/termine_des_landtages_brandenburg_in_der_zeit_vom_13._bis_19._dezember_2021/26864&quot; As always, we can perform the complete extraction of the data during the construction of a tibble: tibble( date = pages %&gt;% map(html_elements, css = &quot;div.list-entry-date&quot;) %&gt;% map(html_text, trim = TRUE) %&gt;% unlist() %&gt;% dmy(), name = pages %&gt;% map(html_elements, css = &quot;div.list-entry-title&quot;) %&gt;% map(html_text, trim = TRUE) %&gt;% unlist(), link = pages %&gt;% map(html_elements, css = &quot;a.list-entry&quot;) %&gt;% map(html_attr, name = &quot;href&quot;) %&gt;% unlist() %&gt;% str_c(&quot;https://www.landtag.brandenburg.de&quot;, .) ) ## # A tibble: 147 × 3 ## date name link ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2021-12-30 Termine des Landtages Brandenburg in der Zeit vom 3. bis 7.… http… ## 2 2021-12-22 Hinweise für Medien zur Sondersitzung des Landtages am 23. … http… ## 3 2021-12-13 Hinweise für Medien zu den Plenarsitzungen des Landtages vo… http… ## 4 2021-12-13 Landtag Brandenburg trauert um seinen ersten Präsidenten Dr… http… ## 5 2021-12-10 Termine des Landtages Brandenburg in der Zeit vom 13. bis 1… http… ## 6 2021-12-09 Hinweise für Medien zur Sondersitzung des Landtages am 13. … http… ## 7 2021-12-09 Übersichtlich, modern und bürgernah: Landtag Brandenburg mi… http… ## 8 2021-12-03 Termine des Landtages Brandenburg in der Zeit vom 6. bis 12… http… ## 9 2021-11-26 Termine des Landtages Brandenburg in der Zeit vom 28. Novem… http… ## 10 2021-11-25 Mit dem Hissen der UN-Women-Flagge setzt der Landtag ein Ze… http… ## # ℹ 137 more rows "],["files.html", "8 Downloading and saving files 8.1 CSV files 8.2 Downloading files 8.3 Saving data files", " 8 Downloading and saving files In this section we will learn to download files from within our R scripts, and save the scraping results locally to be reused without the need to scrape the data again. Most data files you will deal with online are CSV files. To begin we will talk briefly about what CSV files are and how we can use them in R. 8.1 CSV files CSV stands for “comma-separated values”. While they are limited to the display of two-dimensional tables, their simplicity and portability makes CSV files one of the most used formats for this type of data. You can open and write CSV files with statistical software, spreadsheet software like Excel, and even with the most basic text editor. A simple CSV file could look like this: column1, column2, column3 data1_1, data1_2, data1_3 data2_1, data2_2, data2_3 data3_1, data3_2, data3_3 In essence, CSV files are tables. The rows of the table are separated by line breaks, the columns are delimited by commas. The first row usually represents the column names, just as in the example above (column1, column2, column3). Be aware, that this isn’t necessarily always the case. A CSV file may not contain any column names at all, and start with a first row of data cells. This is one of the vital differences when it comes to parsing the file. 8.1.1 Parsing a CSV file To convert the data contained in the CSV file into a format we can use for data analysis we have to parse it. The function used for parsing “understands” the representation of a table in the CSV syntax, can discern column names from data cells, assigns data cells to the correct rows and columns and returns an R object representing the table. read_csv(), from the readr package that we will use here, returns a tibble, while read.csv() from base R returns a data frame. Since readr is part of the core tidyverse, we can just load the tidyverse package. Soon we will also need rvest, so let us begin with loading both libraries. library(tidyverse) library(rvest) read_csv() takes a CSV file as its first argument. To test it, we can also use literal data for the file argument. So we can pass the example defined above as a string – contained in \" – as the fileargument. We will not be using this in practice much, but it serves well as a first example. read_csv( &quot;column1, column2, column3 data1_1, data1_2, data1_3 data2_1, data2_2, data2_3 data3_1, data3_2, data3_3&quot; ) ## Rows: 3 Columns: 3 ## ── Column specification ──────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): column1, column2, column3 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 3 × 3 ## column1 column2 column3 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 data1_1 data1_2 data1_3 ## 2 data2_1 data2_2 data2_3 ## 3 data3_1 data3_2 data3_3 read_csv() guesses the column types by the data entered. All three columns were automatically defined as character vectors here. Automatic guessing works reliably in most situations. Let’s try a CSV with multiple different column types: read_csv( &quot;name, age, size, retired Peter, 42, 1.68, FALSE Paul, 84, 1.82, TRUE Mary, 24, 1.74, FALSE&quot; ) ## Rows: 3 Columns: 4 ## ── Column specification ──────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): name ## dbl (2): age, size ## lgl (1): retired ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 3 × 4 ## name age size retired ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Peter 42 1.68 FALSE ## 2 Paul 84 1.82 TRUE ## 3 Mary 24 1.74 FALSE read_csv() correctly guessed that the first column is of type character, the third of the type double and the fourth is a logical vector. It also guessed that the second column should be of the type double, i.e. floating point numbers. This will work, but we could redefine this vector as integer manually by using the col_types argument. This will not be necessary in most situations, but could increase computational speed with very large datasets, as integers can be saved more efficiently. More on the definition of column types can be found in the help file. What happens when we have CSV data without column names defined in the first line? read_csv( &quot;Peter, 42, 1.68, FALSE Paul, 84, 1.82, TRUE Mary, 24, 1.74, FALSE&quot; ) ## Rows: 2 Columns: 4 ## ── Column specification ──────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): Peter ## dbl (2): 42, 1.68 ## lgl (1): FALSE ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 2 × 4 ## Peter `42` `1.68` `FALSE` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Paul 84 1.82 TRUE ## 2 Mary 24 1.74 FALSE read_csv() by itself cannot discern if the authors of the CSV file intended the first line to be used for names or not. If there are no column names in the CSV data, as in this case, we have to explicitly tell read_csv() not to use the first line for column names. We can do this by specifying the col_names() argument as FALSE; in this case, read_csv() chooses default names for the columns, which can be changed later on. read_csv( &quot;Peter, 42, 1.68, FALSE Paul, 84, 1.82, TRUE Mary, 24, 1.74, FALSE&quot;, col_names = FALSE) ## Rows: 3 Columns: 4 ## ── Column specification ──────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): X1 ## dbl (2): X2, X3 ## lgl (1): X4 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 3 × 4 ## X1 X2 X3 X4 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Peter 42 1.68 FALSE ## 2 Paul 84 1.82 TRUE ## 3 Mary 24 1.74 FALSE 8.2 Downloading files Let us look at some real CSV files to download and parse. The website https://www.bundeswahlleiter.de/bundestagswahlen/2017/ergebnisse/repraesentative-wahlstatistik.html holds multiple CSV files containing inferential statistics on the German federal election held in 2017. Among these files are two files with statistics on the number of people eligible to vote and on the number that actually voted, both by binary gender, year of birth, and additional indicators. Your first impulse would be to download the files manually and then parse them. This will work just fine, but we could also download the files directly from our code. Due to the small number of files in this example, this may not even be the most efficient way, but when you start handling larger amounts of files or want to regularly update files, downloading them from within your code is a safer and more efficient option. To start, we will download the HTML file containing the links to the CSV files. Using the tools at our disposal, we will extract the first two links following the header “Tabellen zur Weiterverwendung”. To understand the construction of the css selector, read chapter 5.1. Note that this is only one of many possible selectors that can achieve the selection of both links of interest. website &lt;- &quot;https://www.bundeswahlleiter.de/bundestagswahlen/2017/ergebnisse/repraesentative-wahlstatistik.html&quot; %&gt;% read_html() a_elements &lt;- website %&gt;% html_elements(css = &quot;ul[style=&#39;list-style-type:none&#39;] &gt; li:first-child &gt; a, ul[style=&#39;list-style-type:none&#39;] &gt; li:nth-child(2) &gt; a&quot;) a_elements ## {xml_nodeset (2)} ## [1] &lt;a data-file-extension=&quot;CSV&quot; data-file-size=&quot;40,53 kB&quot; title=&quot;Link zur CS ... ## [2] &lt;a data-file-extension=&quot;CSV&quot; data-file-size=&quot;68,82 kB&quot; title=&quot;Link zur CS ... links &lt;- a_elements %&gt;% html_attr(name = &quot;href&quot;) links ## [1] &quot;../../../dam/jcr/38972966-dc3d-40fa-91d7-6599d913f5e9/btw17_rws_bw2.csv&quot; ## [2] &quot;../../../dam/jcr/a67208c0-2a2b-41aa-abb8-204b09e73b6b/btw17_rws_bst2.csv&quot; We succeeded in extracting the links, but we can also see that these links are not complete since they are missing their base URL. This works on the website, as the link is relative. To access the files directly though, we need an absolute link – the full path. As before (sub section 7.2.2) we can use the function str_c() to construct the complete URLs. If we give str_c() a vector as one of its input arguments, the function will repeat the string connection for each element in the vector. For this example we can ignore the “../../../” part of the link as it will work regardless; to remove this part we could apply some of the functions from the stringr package. For this we need some basic knowledge in string manipulation and regular expressions, which we will gain in chapter 13. So for now, we will just use the links as they come. links &lt;- str_c(&quot;https://www.bundeswahlleiter.de/&quot;, links) links ## [1] &quot;https://www.bundeswahlleiter.de/../../../dam/jcr/38972966-dc3d-40fa-91d7-6599d913f5e9/btw17_rws_bw2.csv&quot; ## [2] &quot;https://www.bundeswahlleiter.de/../../../dam/jcr/a67208c0-2a2b-41aa-abb8-204b09e73b6b/btw17_rws_bst2.csv&quot; Now that we have complete absolute links, we can download the files to our hard drive using the base R function download.file(). We specify a URL as its first argument and also specify a path and file name for the destfile argument. When no path is given, the file is saved in the directory where your R script is located. download.file(links[1], destfile = &quot;eligible.csv&quot;) download.file(links[2], destfile = &quot;turnout.csv&quot;) We could also have used read_csv() to parse the CSV files directly from the web. However it may still sometimes be a good idea to download the files to our hard drives, since we only have to do this once; with this we decrease traffic for the server, as well as increase the efficiency of our code. This is only a benefit if we download only once and not every time we rerun our script. So we have to comment out the lines where the download occurs after it was successful. 8.2.1 Parsing the CSV files Now we proceed to parse the downloaded CSV files into R objects. But first let’s have a look at the CSV files. You can open them in a text editor of your choosing or directly in RStudio. Importing them into Excel or a similar spreadsheet software is not a valid option here, as we want to see the raw contents of the file and not a representation of the data. Looking at eligible.csv, we notice at least two things: First, the delimiter used here is not a comma but a semicolon. The default CSV style discussed above is common in those countries where a “.” is used as the decimal point, thus the “,” is available to be used as a delimiter. In countries where the “,” is used as a decimal point, it is not available as a delimiter and therefore the “;” is used. The function read_csv2() can be used in this case. A more generalized function is read_delim(), because the delimiting character can freely be defined as an argument. Secondly, we see that the first 9 lines are neither column names nor data, but contain comments on the CSV file itself. We have to tell read_csv2() to ignore these lines. We could use the argument skip to tell R to ignore the first 9 lines; however, these comments are neatly introduced with an #, so we can use the comment argument instead and specify the # as the identifier for comments, which then will be ignored. eligible_tbl &lt;- read_csv2(&quot;eligible.csv&quot;, comment = &quot;#&quot;) ## ℹ Using &quot;&#39;,&#39;&quot; as decimal and &quot;&#39;.&#39;&quot; as grouping mark. Use `read_delim()` for more control. ## Rows: 627 Columns: 10 ## ── Column specification ──────────────────────── ## Delimiter: &quot;;&quot; ## chr (3): Land, Geschlecht, Geburtsjahresgruppe ## dbl (7): Wahlberechtigte, Wahlberechtigte ohne Wahlscheinvermerk, Wahlberech... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. eligible_tbl ## # A tibble: 627 × 10 ## Land Geschlecht Geburtsjahresgruppe Wahlberechtigte Wahlberechtigte ohne W…¹ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bund Summe Summe 61688485 47595800 ## 2 Bund Summe 1997 - 1999 2045501 1709333 ## 3 Bund Summe 1993 - 1996 2902998 2295723 ## 4 Bund Summe 1988 - 1992 4206014 3311417 ## 5 Bund Summe 1983 - 1987 4302277 3455795 ## 6 Bund Summe 1978 - 1982 4264347 3490155 ## 7 Bund Summe 1973 - 1977 3953481 3273575 ## 8 Bund Summe 1968 - 1972 5230865 4232089 ## 9 Bund Summe 1958 - 1967 12396299 9581842 ## 10 Bund Summe 1948 - 1957 9487267 6950185 ## # ℹ 617 more rows ## # ℹ abbreviated name: ¹​`Wahlberechtigte ohne Wahlscheinvermerk` ## # ℹ 5 more variables: `Wahlberechtigte mit Wahlscheinvermerk` &lt;dbl&gt;, ## # `Wähler/-innen` &lt;dbl&gt;, `Wähler/-innen ohne Wahlschein` &lt;dbl&gt;, ## # `Wähler/-innen mit Wahlschein` &lt;dbl&gt;, Wahlbeteiligung &lt;dbl&gt; turnout_tbl &lt;- read_csv2(&quot;turnout.csv&quot;, comment = &quot;#&quot;) ## ℹ Using &quot;&#39;,&#39;&quot; as decimal and &quot;&#39;.&#39;&quot; as grouping mark. Use `read_delim()` for more control. ## Rows: 798 Columns: 16── Column specification ──────────────────────── ## Delimiter: &quot;;&quot; ## chr (3): Land, Geschlecht, Geburtsjahresgruppe ## dbl (13): Erst-/Zweitstimme, Summe, Ungültig, CDU, SPD, DIE LINKE, GRÜNE, CS... ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. turnout_tbl ## # A tibble: 798 × 16 ## Land `Erst-/Zweitstimme` Geschlecht Geburtsjahresgruppe Summe Ungültig ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bund 1 Summe Summe 46976341 586726 ## 2 Bund 1 Summe 1993 - 1999 3369640 37704 ## 3 Bund 1 Summe 1983 - 1992 6039663 58687 ## 4 Bund 1 Summe 1973 - 1982 6227268 62068 ## 5 Bund 1 Summe 1958 - 1972 13918027 146749 ## 6 Bund 1 Summe 1948 - 1957 7693131 103820 ## 7 Bund 1 Summe 1947 und früher 9728612 177697 ## 8 Bund 1 m Summe 22757980 274037 ## 9 Bund 1 m 1993 - 1999 1694779 20330 ## 10 Bund 1 m 1983 - 1992 3001418 29800 ## # ℹ 788 more rows ## # ℹ 10 more variables: CDU &lt;dbl&gt;, SPD &lt;dbl&gt;, `DIE LINKE` &lt;dbl&gt;, GRÜNE &lt;dbl&gt;, ## # CSU &lt;dbl&gt;, FDP &lt;dbl&gt;, AfD &lt;dbl&gt;, Sonstige &lt;dbl&gt;, `dar. NPD` &lt;dbl&gt;, ## # `dar. FREIE WÄHLER` &lt;dbl&gt; 8.3 Saving data files In my opinion, the best approach to data analysis in terms of reproducibility, is to have one script in which you load your raw data, do all your data cleaning, transform your data, do all statistical and graphical analysis and also all exports of tables and graphics. This ensures that anyone who has access to your script and the raw data can reproduce every step you undertook in your analysis, which one of the cornerstones of transparency and reproducibility of scientific works. When we take this approach, we seldom need to save the results of our data analysis in a file. The script – the .R file – is enough. Yet, there are situations where saving results becomes reasonable or even necessary. One of these is Web Scraping, for at least two reasons. To begin with, we will try to minimize the traffic we create on the websites we scrape (see @ref(good_practice)). If we re-run the read_html() function every time we run our script, we also re-download all of the HTML data in each re-run. This puts unnecessary load on the servers and also slows down our script, which becomes more relevant the more data you are scraping from the web. Another point to consider, is that websites change all the time. The data you scraped today may be changed or even gone tomorrow. You should also keep in mind that if the structure of the site changes, your CSS selectors and URLs may not work anymore. The website you are viewing right now, is an updated version of a site that was originally written for a Seminar in the summer of 2021. In the course of updating it for 2022, virtually every example had to be rewritten at least in some parts because the scraped websites had changed, often only in small details, rendering most of the original CSS selectors useless. So for both of those reasons, it may be appropriate to save all of our downloaded HTML data locally, so that we can re-load it for re-runs of our script without having to worry about changes to the source. The most straightforward approach would be to use the download.file() function and apply it directly to the URL and thus copy the original HTML data exactly. download.file(&quot;https://webscraping-tures.github.io/hello_world.html&quot;, &quot;hello_world_local.html&quot;) We run this line once and afterwards just use read_html() on the local file. hello_world &lt;- read_html(&quot;hello_world_local.html&quot;) In most situations we don’t need to save the results of our transformations and analysis. Re-running the script on the original data – we have now saved locally – is enough to ensure that the work can be reproduced. But if we are still interested in saving the results of our analysis to a local file, we could use write_csv() from stringr, or save the data in the native R format .RData. Let’s create an example object we want to save first: data &lt;- tibble( name = c(&quot;Peter&quot;, &quot;Paul&quot;, &quot;Mary&quot;), age = c(42, 84, 24), size = c(1.68, 1.82, 1.74), retired = c(FALSE, TRUE, FALSE) ) write_csv() is an appropriate format for saving tibbles or data frames and on top of that has the advantage of being readable by most software that deals with data, not just by R. It takes the object to be saved as its first argument and the path and file name used for saving as its second argument. write_csv(data, &quot;peter_paul_mary.csv&quot;) The resulting file looks like this: name,age,size,retired Peter,42,1.68,FALSE Paul,84,1.82,TRUE Mary,24,1.74,FALSE If we want to use semicolons as a delimiter we could also use write_csv2(); to load the resulting file we would just use read_csv() or read_csv2(). The downside to using CSV files is that the definition of column types – e.g. character, numeric or integer – is lost and may have to be redefined when reloading the data. Also more complex objects like lists, can not be saved as a CSV, so we may use the data format native to R, .RData. These files also have the advantages of being neatly compressed, thus taking up less space on your drive, and R can read them in faster than CSV files. The save() function also takes the name of the object – or multiple objects – to be saved as its first argument. We also have to specify a file name (and possibly path) in the file = argument. save(data, file = &quot;peter_paul_mary.RData&quot;) We can load objects stored as .RData files by using the load() function on this file. load(&quot;peter_paul_mary.RData&quot;) Please note that save() does not work, when used on an object to which the result of read_html() is assigned to; it will save an object but if you load it, the object will be empty. The reasons for this are rather technical, but you should know that you will loose some of your work, if you do try something like this: hello_world &lt;- read_html(&quot;https://webscraping-tures.github.io/hello_world.html&quot;) hello_world save(hello_world, file = &quot;hello_world.RData&quot;) rm(hello_world) load(&quot;hello_world.RData&quot;) hello_world As you can see, the objext hello_world does not work after loading. Sidenote: The function rm() removes the object(s) defined between the parantheses from the environment. "],["goodpractice.html", "9 Good practice 9.1 We ain’t no web crawlers 9.2 Reduce traffic 9.3 Citation and an additional resource", " 9 Good practice In this following section, we will concern ourselves with questions regarding good practice of web scraping and how to scrape responsibly. There are no wrong or right answers to these questions. But in my view, it is a privilege that we are able to access this wealth of data and we should treat it as such. Why? We have a responsibility towards the people, institutions and companies whose data we collect to respect certain boundaries and guidelines of data collection. These boundaries might have been set by others or might be constraints we impose upon ourselves. I will not be talking about the legal situation concerning web scraping. First and foremost, I am not an expert on the legalities and do not feel equipped to give any responsible advice. Additionally, this is a relatively new field and legal rulings might change. On top of that, the specific rules you have to adhere to may very well depend on the context of your scraping project. It’s possible that it is allowed to collect certain data for use in a scientific project, but not for use in a commercial one. Certain types of data are not allowed to be collected at all. When in doubt, try to get advice from your superiors or experts on the legal questions surrounding web scraping and privacy laws. Nonetheless, if we follow some basic principles of good practice, we can design our web scraping projects in a way that is respectful to the owners of the data as well as true to our own standards of responsible data collection. So to boil it down: Rule #1 of Scrape Club: Scrape responsibly and with respect to the data sources. Rule #2 of Scrape Club: Scrape responsibly and with respect to the data sources! 9.1 We ain’t no web crawlers Web crawlers – also known as web spiders or robots – are pieces of software that aim at collecting and indexing all content on the internet. Web crawlers are for example, operated by search engines whose aim is to index the content of the web and make it accessible via their search interface. It is not our goal to indiscriminately and systematically collect every piece of data on the web. Web scraping is aimed at extracting some specific data as precisely as possible and analysing the collected data in a specific usage context, in our case scientific research. We are not web crawlers. What does this imply for a good practice of web scraping? Collect only what you need This means that you have to think hard about which pieces of data you actually have to collect to meet your goals, before you start collecting. Rarely are all subpages of a website needed, so you should only download those that contain data of interest. Use an API if there is one Some websites may give you access to an API, an application programming interface. APIs in the web context are mostly used for allowing third party sites and applications to access the services a website provides. For example, YouTube’s IFrame player API is used when a YouTube Video is embedded in a third party site. The API gives access to the video and much of YouTube’s functionality. Another example would be Twitter’s API that can be used by tweet reader apps to access your tweets. In many cases, you can use those APIs to directly access the data you are interested in instead of scraping the raw HTML code for it. Sometimes the APIs are openly accessible, sometimes you have to apply for access first. Using the latter as an example, if you want to use Twitter’s API to collect tweets for scientific analysis, you have to fill out a form, describing your project and planned data usage, and then await approval. This extra work could potentially give you a more direct access to cleaner data. Additionally, if you use an API you automatically play by the rules of the provider. So, if there is an API you can access and it also gives you access to the data you are interested in, it is a good idea to use the API instead of “traditional” scraping techniques. The technical details of accessing APIs are beyond the scope of this introduction. Also APIs can differ tremendously in their functionality. The first step should always be to read the APIs documentation. You can also check whether there already is an R package that simplifies access to an API, e.g. TwitteR or Rfacebook, before you start writing your own scripts to access an API. Anonymisation of personal data Whenever we as web scrapers are collecting personal data that could be used to identify people, we ought to anonymise any information that could lead to identification; this primarily concerns real names in customer reviews, forums and so on. I would argue that this should also extend to user names in general. Most of the time, we do not need the actual names and any simple numeric identifier constructed by us would suffice to discern users during the analysis. If we cannot directly anonymise the data during collection, we should do this after collection. Especially if results of the analysis or a constructed data set are to be publicly released: anonymisation is paramount! robots.txt Many websites have set up a file called robots.txt on their servers which is predominantly aimed at informing automatic web crawlers which parts of the website they are allowed to collect. While the robots.txt files may ask the crawlers to stay out of certain subfolders of a website, they have no power in actually stopping them. If the crawler is polite, it will follow the guidelines of the robots.txt. It may be up to debate whether web scrapers should follow the robots.txt guidelines, as we have already established that we are not web crawlers. In my opinion, we should at least take a look at these files when planning a project. If the data we plan to collect is excluded in the guidelines, we still have to decide if we want to go forward with the project: Is the collection of this specific piece of data really necessary? Do I have support by superiors or my employer? Can we contact the website operator, describe our project and kindly ask for permission? 9.1.1 On robots.txt files The robots.txt files are usually accessible by appending the URL of the mainpage of a website. If you do not find a robots.txt in this manner, there most probably is none; e.g. to access the robots.txt file for the website https://www.wahlrecht.de/ we can use the URL https://www.wahlrecht.de/robots.txt{target_blank}. Let us briefly look at the structure of this file. Its first block looks like this: User-agent: * Disallow: /cgi-bin/ Disallow: /temp/ Disallow: /ueberhang/wpb/ Disallow: /ueberhang/chronic/ The field “User-agent” defines to which crawlers the rules apply to. “*” is short for everyone. The “Disallow” fields define which subdirectories of the website may not be accessed by the particular User-agent. So in this case, no User-agent may access the subdirectories “/cgi-bin/”, “/temp/”, and so on. Let’s have a look at the second block: User-Agent: Pixray-Seeker Disallow: / Here a specific User-agent is defined, so the rules that follow only refer to the crawler “Pixray-Seeker”. Here there is only one Disallow rule, namely “/”. “/” is short for “everything”: therefore the Pixray-Seeker crawler is not allowed access to any content on the website. 9.2 Reduce traffic Every time a website is accessed, traffic is generated. Traffic here refers to the transfer of the HTML data from the website’s server to our computer, this produces monetary costs on the side of the providers of the website. Many websites are financed through advertisements. When downloading the website directly from R using read_html(), we are circumventing the ad display in the browser and thus the provider will get no revenue. Both may seem negligible in many cases, but if we aim to act respectful to the website providers, we should reduce unnecessary traffic when we are able to. Collect only once (if possible) The most simple step we can undertake is not to download the HTML code every time we re-run our script. This reduces traffic and also makes our code faster to execute. See chapter 8 on how to save the downloaded HTML files. Test, test, test We should also test our code on a small scale before finally running it on large amounts of data. If our aim is to download a large number of websites and/or files, we ought to make sure that our code actually works. While there are problems in our code – and there will be – we may create a lot of unnecessary traffic during bugfixing. If we test our code until we are sure it will run without errors on a single or a small set of subpages, instead of all, we will reduce traffic and again, also reduce the time we have to wait for the downloads to finish. Set waiting times If we are downloading many pages or files, it may also be a good idea to slow down the process on our end by setting a waiting time between each download, this will spread our traffic over time. If a server detects unusually large amounts of site requests, it may even block our IP which would make us unable to continue our downloads. Waiting a little while between each request is a way to circumvent this. A waiting time between 2-5 seconds should be more than enough in most cases. Sometimes the robots.txt also specifies desired waiting times for a specific site. 9.2.1 Waiting times in action Let us have another look on the first example from chapter 7 to see how to set waiting times in practice when downloading multiple pages with read_html(). First, let us generate the list of links we want to download like we did in chapter 7. library(tidyverse) library(rvest) links &lt;- &quot;https://www.tidyverse.org/packages/&quot; %&gt;% read_html() %&gt;% html_elements(css = &quot;div.package &gt; a&quot;) %&gt;% html_attr(name = &quot;href&quot;) To let R wait for several seconds, we can use the base R function Sys.sleep() which takes the seconds to wait as its only argument. Since we want the map() function to add the waiting time before each read_html() iteration, we have to include Sys.sleep() into the iteration process. We can achieve the addition of the waiting time by defining a multi-line formula using the ~ {...} notation. Each new line enclosed by the {} is run once for each iteration over the object we pass on to map. So for each element of the links object, R first waits for two seconds and then applies read_html() to the element of links. Note, that we have to refer to the element of links in this formula notation by writing . between the parentheses of read_html(). So on the first iteration the . stands for the first element of links, on the second for the second element and so on. pages &lt;- links %&gt;% map(~ { Sys.sleep(2) read_html(.) }) We successfully added a waiting time of two seconds between each iteration. Note that read_html() has to come last if we want this short notation to work because only the return value of the last function within the braces is assigned to pages. 9.3 Citation and an additional resource Some of the ideas expressed above are in part based on the subchapter “9.3 Web scraping: Good practice” from: Munzert et. al (2015). Automated Data Collection with R. A Practical Guide to Web Scraping and Text Mining. Chichester: Wiley. The authors also describe some guidelines of a good practice of web scraping, that in part overlap with the set of guidelines presented here, but go beyond in describing the technical implementation in R and also the legal situation by way of some examples. A read is highly recommended. "],["scraping.html", "10 Scraping Berlin police reports 10.1 Topic and data 10.2 Scraping the data", " 10 Scraping Berlin police reports Over the next three chapters we will follow a small sample project. In this chapter we will briefly outline the research topic and scrape the data. In the next two chapters we will concern ourselfes with cleaning, transforming and analysing the data. First statistically, then grapically. 10.1 Topic and data This sample project aims to analyse police reports in Berlin. Specifically, we will try to answer two questions: Does the number of reports differ by district? Does the number of reports differ over time? over years over months over days of the week over time of day Note that these are ad hoc questions constructed for this sample project. In a real research project, we would have to motivate the research question more clearly and develop hyotheses based on theoretical considerations and existing research. These steps are skipped here to keep the focus on scraping the data and basic methods of data analysis. Now that topic and questions are defined, we need some data to answer them. The website https://www.berlin.de/polizei/polizeimeldungen/archiv/ contains reports by the Berlin police that are open to the public, beginning with the year 2014. We will gather the links to all subpages, download them and extract the text of the report, as well as the date, time and district where it occurred. 10.2 Scraping the data 10.2.1 Gathering the links library(tidyverse) library(rvest) We begin by downloading the mainpage. website &lt;- &quot;https://www.berlin.de/polizei/polizeimeldungen/archiv/&quot; %&gt;% read_html() The next step is to extract the URLs for the yearly archives. All the &lt;a&gt; tags that contain those links, have a title= attribute whose value begins with “Link”. Also they are contained in a &lt;div&gt; tag with the class textile. We can use this in selector construction, extract the value of the href= attribute and, as they are incomplete URLs, append them to the base URL. links &lt;- website %&gt;% html_elements(css = &quot;div.textile a[title^=&#39;Link&#39;]&quot;) %&gt;% html_attr(name = &quot;href&quot;) %&gt;% str_c(&quot;https://www.berlin.de&quot;, .) Each yearly archive page contains several subpages for the reports in that year, divided by pagination; to gather the links to all the subpages, we have to understand how the pagination works. In this case, the URL for the yearly archive is simply appended with the query “?page_at_1_0=” and a value indicating the number of the subpage we want to access. So “https://www.berlin.de/polizei/polizeimeldungen/archiv/2022/?page_at_1_0=5” for example gives access to the fifth page for 2022. The number of police reports is not constant over the years; the number of subpages per year will also differ. We do not know in advance, how many subpages we will have to download, but we can scrape this information from the last pagination link for each year. We are lucky, as the &lt;a&gt; tag for the last subpage per year is a list item that can be identified by its two classes pager-item and last. We parse the first page for each year, select the &lt;a&gt; tag for the last subpage, extract the text from it and save it as a number. Note that I also added a waiting time of two seconds between each download, as shown in chapter 9. max_pages &lt;- links %&gt;% map(~ { Sys.sleep(2) read_html(.x) }) %&gt;% map(html_element, css = &quot;li.pager-item.last &gt; a&quot;) %&gt;% map(html_text) %&gt;% as.numeric() Now we can construct the links to all subpages; done by nesting two for loops in the script. For this to be understandable, we should have a brief look on for loops. Like map(), for loops are used for iteration. We can use them to repeat one or multiple commands for a fixed number of times. The following non-runable code block, shows the basic for loop syntax. for (variable in vector) { Code } We will replace variable with a name for our counter. Per convention lower case letters are used, typically i. In vector we will define the values the counter will take. If we define the vector 1:5, the counter will take on the value 1 in the first loop, 2 in the second and so on. After the loop runs with the value 5, it ends. The values in vector do not have to be ascending numbers though. We could also use a numeric vector like c(4, 8, 15, 16, 23, 42), a character vector like c(\"a\", \"s\", \"d\", \"f), or any other vector. Between the {} brackets, we write the commands that shall be executed in each iteration. The important thing is, that we can use the value the counter takes in each iteration. For example, the following code prints the value of i for each iteration into the console: for (i in 1:5) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 For loops can also be nested. Then, the inner loop iterates completely for each iteration of the outer loop. In the following example, the inner loop iterates three times for each of the two iterations of the outer loop. So we get six iterations in total. Note, that inner and outer loop have to have different variable names for this to work. The paste() function in print() is used to combine the values of i and j seperated by a - and print this combination into the console, so we can see exactly in which order the iterations occur. for (i in 1:2) { for (j in 1:3) { print(paste(i, j, sep = &quot;-&quot;)) } } ## [1] &quot;1-1&quot; ## [1] &quot;1-2&quot; ## [1] &quot;1-3&quot; ## [1] &quot;2-1&quot; ## [1] &quot;2-2&quot; ## [1] &quot;2-3&quot; With this knowledge, we can construct a nested for loop that builds a vector of the links to all subpages for all years. pag_links &lt;- character() for (i in 1:length(links)) { for (j in 1:max_pages[i]) { pag_links &lt;- append(pag_links, values = str_c(links[i], &quot;?page_at_1_0=&quot;, j)) } } The outer loop is a counter for the elements in the objects links; in this case the loop counts from 1 to 9, one step for each yearly archive page. For each of these iterations, the inner loop will count from 1 to the number of the last subpage for this year, which we assigned to the vector max_pages. Using subsetting with max_pages[i], we select the correct number of subpages for each year, i.e. each iteration of the outer loop. Using both counters together, we can construct the links to the subpages using str_c(). The function takes the link to a yearly archive indicated by the counter in the outer loop i, appends “?page_at_1_0=” and after this the value of the inner loop counter j, i.e. the subpages (1, 2, …, j) for this year. The resulting link is then added to the vector pag_links using the function append() which adds the data indicated by the argument values to the object specified in the first argument. For this to work, we first have to initialise the object as an empty object outside of the loop. Here I created it as an empty character vector, using pag_links &lt;- character(). If you want to create an empty object without any data type assigned, you can also use empty_vector &lt;- NULL. 10.2.2 Downloading the subpages Now that the links to all subpages are gathered, we can finally download them. Please note that the download will take up to 20 minutes due to the number of subpages (443 at the time of writing), and the additional waiting time of 2 seconds between each iteration. pages &lt;- pag_links %&gt;% map(~ { Sys.sleep(2) read_html(.x) }) 10.2.3 Extracting the data of interest The goal is to extract the text of the report, the date/time and the district the report refers to. Looking into the source code, we find that all reports are list items in an unordered list. Conveniently for us, all data fields we are interested in have distinct classes we can use in scraping. Date and time are enclosed by a &lt;div&gt; tag with the classes cell, nowrap and date. The report headline is also part of a &lt;div&gt;. Here the classes are cell and text. The same &lt;div&gt; also includes the district’s name, but we can discern between the two. The text is included in a &lt;a&gt; tag, that is a child of the &lt;div&gt; and the district is part of a &lt;span&gt; with the class category, which is also a child of the &lt;div&gt;. We can use this information to construct appropriate CSS selectors like this: reports &lt;- tibble( Date = pages %&gt;% map(html_elements, css = &quot;div.cell.nowrap.date&quot;) %&gt;% map(html_text) %&gt;% unlist(), Report = pages %&gt;% map(html_elements, css = &quot;div.cell.text &gt; a&quot;) %&gt;% map(html_text) %&gt;% unlist(), District = pages %&gt;% map(html_elements, css = &quot;div.cell.text &gt; span.category&quot;) %&gt;% map(html_text) %&gt;% unlist() ) ## Error in `tibble()`: ## ! Tibble columns must have compatible ## sizes. ## • Size 21952: Existing data. ## • Size 21525: Column `District`. ## ℹ Only values of size one are recycled. That did not work. But why? Let us look at the error message we received. It informs us that the column “District” we tried to create, is shorter in length compared to the other two. Since we can not create tibbles out of columns with different lengths, we get this error. The fact that the “District” column is shorter than the other two, must mean that there are police reports where the information on the district is not listed on the website, which we can confirm by browsing some of the subpages, e.g. https://www.berlin.de/polizei/polizeimeldungen/archiv/2014/. 10.2.3.1 Dealing with missing data in lists Some of the &lt;span&gt; tags that contain the district are missing. Using the approach presented above, html_elements() just extracts the &lt;span&gt; tags that are present. We tell R that we do want all &lt;span&gt; tags of the class “category”, and this is what R returns to us. For list items where the tag is missing, nothing is returned. But this is not what we want; what we actually want is, that R looks at every single police report and saves its text, date and time, as well as the district, if it is not missing. If it is missing, we want R to save a NA in the cell of the tibble, the representation of missing values in R. The &lt;div&gt; and &lt;span&gt; tags that contain the data of interest are nested in &lt;li&gt; tags in this case. The &lt;li&gt; tags thus contain the whole set of data, the text, the date/time and the district. The approach here is to make R examine every single &lt;li&gt; tag and extract the data that is present, as well as save an NA for every piece of data that is missing. To start, we have to extract all the &lt;li&gt; tags and their content from the subpages. Right now, the subpages are saved in the object pages. We use a for loop that takes every element of pages, extracts all the &lt;li&gt; tags from it and adds them to a new list using append(). Note, that we have to use pages[[]] to subset the list of subpages, as we want to access the actual list elements, i.e. the node sets for the sub pages. As with tibbles, pages[] would always return another list. The &lt;li&gt; tags are all children of an &lt;ul&gt; with the class list--tablelist, which we can use in our selector. append() takes an existing object as its first argument and adds the data indicated in the values = argument to it. For this to work, we have to initiate the new list as an empty object before the loop starts. list_items &lt;- NULL for (i in 1:length(pages)) { list_items &lt;- append(list_items, values = html_elements(pages[[i]], css = &quot;ul.list--tablelist &gt; li&quot;)) } The newly created list list_items contains a node set for each &lt;li&gt; tag from all subpages. Again, we have to use double brackets to access the node set. With single brackets a new list containing the node set as its first element is returned, as illustrated here: list_items[[1]] ## {html_node} ## &lt;li&gt; ## [1] &lt;div class=&quot;cell nowrap date&quot;&gt;16.10.2023 17:55 Uhr&lt;/div&gt;\\n ## [2] &lt;div class=&quot;cell text&quot;&gt;\\n&lt;a href=&quot;/polizei/polizeimeldungen/2023/pressemi ... list_items[1] ## [[1]] ## {html_node} ## &lt;li&gt; ## [1] &lt;div class=&quot;cell nowrap date&quot;&gt;16.10.2023 17:55 Uhr&lt;/div&gt;\\n ## [2] &lt;div class=&quot;cell text&quot;&gt;\\n&lt;a href=&quot;/polizei/polizeimeldungen/2023/pressemi ... We can now make R examine every element of this list one after the other and extract all the data they contain. But what happens when we try to extract a element that is not present in one of the elements? R returns an NA: html_element(list_items[[1]], css = &quot;span.notthere&quot;) ## {xml_missing} ## &lt;NA&gt; Using a for loop that iterates over the elements in list_items, we write a tibble row by row, filling the data cells with the extracted information and with NA if a element could not be found for this element of list_items. We have to initiate the tibble before the for loop starts: we define the column names, the type of data to be saved and also the length of the columns. The latter is not strictly necessary as we could also have created a tibble with a column length of \\(0\\), but pre-defining their length increases computational efficiency. Still, the for loop has to iterate over several thousand elements and extract the data contained, which will take several minutes to complete. reports &lt;- tibble( Date = character()[1:length(list_items)], Report = character()[1:length(list_items)], District = character()[1:length(list_items)] ) for (i in 1:length(list_items)) { reports[i, &quot;Date&quot;] &lt;- html_element(list_items[[i]], css = &quot;div.cell.nowrap.date&quot;) %&gt;% html_text() reports[i, &quot;Report&quot;] &lt;- html_element(list_items[[i]], css = &quot;div.cell.text &gt; a&quot;) %&gt;% html_text() reports[i, &quot;District&quot;] &lt;- html_element(list_items[[i]], css = &quot;div.cell.text &gt; span.category&quot;) %&gt;% html_text() } Let’s look at the tibble we just constructed: reports ## # A tibble: 21,952 × 3 ## Date Report District ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 16.10.2023 17:55 Uhr Verdacht eines Tötungsdeliktes - Mordkommissio… Ereigni… ## 2 16.10.2023 17:49 Uhr Gefährliche Körperverletzung durch Schuss - Du… Ereigni… ## 3 16.10.2023 14:43 Uhr Vorkommnisse im Zusammenhang mit dem Nahost-Ko… Ereigni… ## 4 16.10.2023 11:26 Uhr Festnahme nach räuberischem Diebstahl Ereigni… ## 5 16.10.2023 10:07 Uhr Verletzte Radfahrerin in Krankenhaus verstorben Ereigni… ## 6 16.10.2023 09:53 Uhr Brandstiftung an mehreren Fahrzeugen Ereigni… ## 7 15.10.2023 21:07 Uhr Verkehrsunfall mit Sonder- und Wegerechten Ereigni… ## 8 15.10.2023 15:46 Uhr Raub im Juweliergeschäft Ereigni… ## 9 15.10.2023 14:59 Uhr Einkaufswagen aus Hochhaus geworfen - Mordkomm… Ereigni… ## 10 15.10.2023 14:47 Uhr Vorkommnisse im Zusammenhang mit den weltweite… Ereigni… ## # ℹ 21,942 more rows This looks good, but we should also confirm, that NAs were handled correctly. We can examine the entry for “31.12.2014 13:21 Uhr” that we saw on https://www.berlin.de/polizei/polizeimeldungen/archiv/2014/, and for which the district was missing. We can use subsetting to just look at this one observation in our tibble. Remembet that when subsetting two dimensional objects like tibbles, we have to supply an index for the row(s) as well as for the column(s) we want to subset. Our goal is, to subset the row for which the column “Date” holds the value “31.12.2014 13:21 Uhr”. We can thus write our row index as reports$Date == \"31.12.2014 13:21 Uhr\", which reads as: the row(s) for which the value of the column “Date” in the object “reports” is equal to “31.12.2014 13:21 Uhr”. As we want to see all columns for this observation, we do not need to supply a column index. By writing nothing after the , we instruct R to show us all columns. reports[reports$Date == &quot;31.12.2014 13:21 Uhr&quot;, ] ## # A tibble: 1 × 3 ## Date Report District ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 31.12.2014 13:21 Uhr Alkoholisiert geflüchtet und die Kontrolle verl… &lt;NA&gt; This also looks good. We now have extracted the data we need to answer our questions. 10.2.4 Saving the data As discussed in chapter 8, we save the scraped data at this point. You have seen that we downloaded a lot of subpages, which took a considerable amount of time; if we repeat this for every instance of further data analysis, we create a lot of unnecessary traffic and waste a lot of our own time. save(reports, file = &quot;reports.RData&quot;) In the next chapter we will continue with cleaning the data, transforming it and calculating some descriptive statistics on it. "],["dplyr.html", "11 Transformation with dplyr 11.1 mutate() 11.2 select() 11.3 rename() 11.4 filter() 11.5 summarise() &amp; group_by() 11.6 Exporting tables", " 11 Transformation with dplyr In this chapter, we will use several functions from the tidyverse package dplyr to clean the data we scraped in chapter 10. We will then select the columns and filter the observations (rows) we need, as well as compute some grouped summaries of the data. First, let us load the required packages. dplyr is part of the core tidyverse; we will need the package lubridate as well, to be able to deal with dates and times. library(tidyverse) library(lubridate) 11.1 mutate() 11.1.1 Computing new variables from existing ones Before we continue working on our scraped reports data, we will look at some simple examples that will show how new variables can be computed in a tibble from existing ones, using mutate() from dplyr. For this purpose, let us create a new simple tibble on voter turnout. Note that the entered data is purely illustrational and has no meaning. exmpl_tbl &lt;- tibble( ybirth = c(&quot;1993 - 2002&quot;, &quot;1983 - 1992&quot;, &quot;1973 - 1982&quot;, &quot;1963 - 1972&quot;), eligible = c(100000, 100000, 100000, 100000), turnout_person = c(40000, 50000, 60000, 70000), turnout_mail = c (35000, 30000, 25000, 20000) ) exmpl_tbl ## # A tibble: 4 × 4 ## ybirth eligible turnout_person turnout_mail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1993 - 2002 100000 40000 35000 ## 2 1983 - 1992 100000 50000 30000 ## 3 1973 - 1982 100000 60000 25000 ## 4 1963 - 1972 100000 70000 20000 Here we have different columns for people who were eligible to vote, who voted in person and who voted by mail, by their year of birth. We do not actually care about the difference in the voting method in this example and want one column that combines both. We can get there by using mutate(). The function takes the data to be manipulated as its first argument, followed by one or multiple arguments defining the new columns to be created. We can create this new columns as computations from the columns already present. To calculate the total turnout, we write: exmpl_tbl %&gt;% mutate(turnout = turnout_person + turnout_mail) ## # A tibble: 4 × 5 ## ybirth eligible turnout_person turnout_mail turnout ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1993 - 2002 100000 40000 35000 75000 ## 2 1983 - 1992 100000 50000 30000 80000 ## 3 1973 - 1982 100000 60000 25000 85000 ## 4 1963 - 1972 100000 70000 20000 90000 We can also immediately start calculating with new columns in the same mutate() command. For this, we chain multiple operations in mutate() separated with ,. To calculate the turnout percentage in a second step, we write the following: exmpl_tbl %&gt;% mutate( turnout = turnout_person + turnout_mail, turnout_pct = turnout / eligible ) ## # A tibble: 4 × 6 ## ybirth eligible turnout_person turnout_mail turnout turnout_pct ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1993 - 2002 100000 40000 35000 75000 0.75 ## 2 1983 - 1992 100000 50000 30000 80000 0.8 ## 3 1973 - 1982 100000 60000 25000 85000 0.85 ## 4 1963 - 1972 100000 70000 20000 90000 0.9 Note that mutate() is not limited to basic arithmetic operations. Many functions can be applied within mutate(): sum() or mean() are two examples we already know. 11.1.2 Cleaning the data on police reports Now knowing the basic principle of working with mutate(), we can apply this to the data on police reports we scraped in chapter 10. First we have to load the data and examine what we are working with. load(&quot;reports.RData&quot;) reports ## # A tibble: 21,952 × 3 ## Date Report District ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 16.10.2023 17:55 Uhr Verdacht eines Tötungsdeliktes - Mordkommissio… Ereigni… ## 2 16.10.2023 17:49 Uhr Gefährliche Körperverletzung durch Schuss - Du… Ereigni… ## 3 16.10.2023 14:43 Uhr Vorkommnisse im Zusammenhang mit dem Nahost-Ko… Ereigni… ## 4 16.10.2023 11:26 Uhr Festnahme nach räuberischem Diebstahl Ereigni… ## 5 16.10.2023 10:07 Uhr Verletzte Radfahrerin in Krankenhaus verstorben Ereigni… ## 6 16.10.2023 09:53 Uhr Brandstiftung an mehreren Fahrzeugen Ereigni… ## 7 15.10.2023 21:07 Uhr Verkehrsunfall mit Sonder- und Wegerechten Ereigni… ## 8 15.10.2023 15:46 Uhr Raub im Juweliergeschäft Ereigni… ## 9 15.10.2023 14:59 Uhr Einkaufswagen aus Hochhaus geworfen - Mordkomm… Ereigni… ## 10 15.10.2023 14:47 Uhr Vorkommnisse im Zusammenhang mit den weltweite… Ereigni… ## # ℹ 21,942 more rows We can leave the column containing the text of the police report as it is, but the columns containing the districts, as well as the date and time, could use some work. The data in the column “District” includes the string “Ereignisort:” before listing the actual name of the district the report refers to. This unnecessarily clutters this column: we can use substr() to remove these leading characters within mutate(). substr() extracts a part of a string we want to keep, taking the data to be applied to, as its first argument – here the name of the column –, the starting character position where the extraction shall begin as its second, and the character position where it shall stop, as its third argument. Character position here refers to the numeric position of a character in a string: to begin extraction after “Ereignisort:” we have to count the length of this substring including the whitespace after “:”, which is \\(13\\) and thus use \\(14\\) as the starting position. For the position to stop extraction, we could either use an unrealistically high number or use length(District) to determine the exact length of each string. reports %&gt;% mutate(District = substr(District, 14, 99)) ## # A tibble: 21,952 × 3 ## Date Report District ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 16.10.2023 17:55 Uhr Verdacht eines Tötungsdeliktes - Mordkommissio… Treptow… ## 2 16.10.2023 17:49 Uhr Gefährliche Körperverletzung durch Schuss - Du… Charlot… ## 3 16.10.2023 14:43 Uhr Vorkommnisse im Zusammenhang mit dem Nahost-Ko… berlinw… ## 4 16.10.2023 11:26 Uhr Festnahme nach räuberischem Diebstahl Friedri… ## 5 16.10.2023 10:07 Uhr Verletzte Radfahrerin in Krankenhaus verstorben Marzahn… ## 6 16.10.2023 09:53 Uhr Brandstiftung an mehreren Fahrzeugen Friedri… ## 7 15.10.2023 21:07 Uhr Verkehrsunfall mit Sonder- und Wegerechten Mitte ## 8 15.10.2023 15:46 Uhr Raub im Juweliergeschäft Charlot… ## 9 15.10.2023 14:59 Uhr Einkaufswagen aus Hochhaus geworfen - Mordkomm… Neukölln ## 10 15.10.2023 14:47 Uhr Vorkommnisse im Zusammenhang mit den weltweite… berlinw… ## # ℹ 21,942 more rows reports %&gt;% mutate(District = substr(District, 14, length(District))) ## # A tibble: 21,952 × 3 ## Date Report District ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 16.10.2023 17:55 Uhr Verdacht eines Tötungsdeliktes - Mordkommissio… Treptow… ## 2 16.10.2023 17:49 Uhr Gefährliche Körperverletzung durch Schuss - Du… Charlot… ## 3 16.10.2023 14:43 Uhr Vorkommnisse im Zusammenhang mit dem Nahost-Ko… berlinw… ## 4 16.10.2023 11:26 Uhr Festnahme nach räuberischem Diebstahl Friedri… ## 5 16.10.2023 10:07 Uhr Verletzte Radfahrerin in Krankenhaus verstorben Marzahn… ## 6 16.10.2023 09:53 Uhr Brandstiftung an mehreren Fahrzeugen Friedri… ## 7 15.10.2023 21:07 Uhr Verkehrsunfall mit Sonder- und Wegerechten Mitte ## 8 15.10.2023 15:46 Uhr Raub im Juweliergeschäft Charlot… ## 9 15.10.2023 14:59 Uhr Einkaufswagen aus Hochhaus geworfen - Mordkomm… Neukölln ## 10 15.10.2023 14:47 Uhr Vorkommnisse im Zusammenhang mit den weltweite… berlinw… ## # ℹ 21,942 more rows The column “Date” includes the date and the time of each report as a character string. To be able to use this data in analysis, we have to extract the date and time in a format R can understand. This is easily achieved using the parsing functions from lubridate. As the character data are written in the format “day.month.year hour:minute”, we have to use the function dmy_hm() on the column. reports %&gt;% mutate(date_cmpl = dmy_hm(Date)) ## # A tibble: 21,952 × 4 ## Date Report District date_cmpl ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; ## 1 16.10.2023 17:55 Uhr Verdacht eines Tötungsdeli… Ereigni… 2023-10-16 17:55:00 ## 2 16.10.2023 17:49 Uhr Gefährliche Körperverletzu… Ereigni… 2023-10-16 17:49:00 ## 3 16.10.2023 14:43 Uhr Vorkommnisse im Zusammenha… Ereigni… 2023-10-16 14:43:00 ## 4 16.10.2023 11:26 Uhr Festnahme nach räuberische… Ereigni… 2023-10-16 11:26:00 ## 5 16.10.2023 10:07 Uhr Verletzte Radfahrerin in K… Ereigni… 2023-10-16 10:07:00 ## 6 16.10.2023 09:53 Uhr Brandstiftung an mehreren … Ereigni… 2023-10-16 09:53:00 ## 7 15.10.2023 21:07 Uhr Verkehrsunfall mit Sonder-… Ereigni… 2023-10-15 21:07:00 ## 8 15.10.2023 15:46 Uhr Raub im Juweliergeschäft Ereigni… 2023-10-15 15:46:00 ## 9 15.10.2023 14:59 Uhr Einkaufswagen aus Hochhaus… Ereigni… 2023-10-15 14:59:00 ## 10 15.10.2023 14:47 Uhr Vorkommnisse im Zusammenha… Ereigni… 2023-10-15 14:47:00 ## # ℹ 21,942 more rows At a later point we will do some analysis by year, month, weekday and time of day. lubridate includes functions that extract the subparts of date and time data. year() extracts the year, month() the month and wday() the day of the week. The argument label = TRUE tells the function to use the names of months and days instead of a numerical value ranging from \\(1\\) to \\(12\\) and \\(1\\) to \\(7\\) respectively. reports %&gt;% mutate(date_cmpl = dmy_hm(Date), year = year(date_cmpl), month = month(date_cmpl, label = TRUE), day = wday(date_cmpl, label = TRUE) ) ## # A tibble: 21,952 × 7 ## Date Report District date_cmpl year month day ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; ## 1 16.10.2023 17:55 Uhr Verdacht… Ereigni… 2023-10-16 17:55:00 2023 Okt Mo ## 2 16.10.2023 17:49 Uhr Gefährli… Ereigni… 2023-10-16 17:49:00 2023 Okt Mo ## 3 16.10.2023 14:43 Uhr Vorkommn… Ereigni… 2023-10-16 14:43:00 2023 Okt Mo ## 4 16.10.2023 11:26 Uhr Festnahm… Ereigni… 2023-10-16 11:26:00 2023 Okt Mo ## 5 16.10.2023 10:07 Uhr Verletzt… Ereigni… 2023-10-16 10:07:00 2023 Okt Mo ## 6 16.10.2023 09:53 Uhr Brandsti… Ereigni… 2023-10-16 09:53:00 2023 Okt Mo ## 7 15.10.2023 21:07 Uhr Verkehrs… Ereigni… 2023-10-15 21:07:00 2023 Okt So ## 8 15.10.2023 15:46 Uhr Raub im … Ereigni… 2023-10-15 15:46:00 2023 Okt So ## 9 15.10.2023 14:59 Uhr Einkaufs… Ereigni… 2023-10-15 14:59:00 2023 Okt So ## 10 15.10.2023 14:47 Uhr Vorkommn… Ereigni… 2023-10-15 14:47:00 2023 Okt So ## # ℹ 21,942 more rows To extract the time of day, we first have to extract the substring of “Date” that contains the time and then apply the lubridate function hm() to it. reports %&gt;% mutate(time = substr(Date, 12, length(Date)) %&gt;% hm() ) ## # A tibble: 21,952 × 4 ## Date Report District time ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;Period&gt; ## 1 16.10.2023 17:55 Uhr Verdacht eines Tötungsdeliktes - Mo… Ereigni… 17H 55M 0S ## 2 16.10.2023 17:49 Uhr Gefährliche Körperverletzung durch … Ereigni… 17H 49M 0S ## 3 16.10.2023 14:43 Uhr Vorkommnisse im Zusammenhang mit de… Ereigni… 14H 43M 0S ## 4 16.10.2023 11:26 Uhr Festnahme nach räuberischem Diebsta… Ereigni… 11H 26M 0S ## 5 16.10.2023 10:07 Uhr Verletzte Radfahrerin in Krankenhau… Ereigni… 10H 7M 0S ## 6 16.10.2023 09:53 Uhr Brandstiftung an mehreren Fahrzeugen Ereigni… 9H 53M 0S ## 7 15.10.2023 21:07 Uhr Verkehrsunfall mit Sonder- und Wege… Ereigni… 21H 7M 0S ## 8 15.10.2023 15:46 Uhr Raub im Juweliergeschäft Ereigni… 15H 46M 0S ## 9 15.10.2023 14:59 Uhr Einkaufswagen aus Hochhaus geworfen… Ereigni… 14H 59M 0S ## 10 15.10.2023 14:47 Uhr Vorkommnisse im Zusammenhang mit de… Ereigni… 14H 47M 0S ## # ℹ 21,942 more rows We can combine all these individual steps in one mutate() command and apply them to the object reports. reports &lt;- reports %&gt;% mutate(District = substr(District, 14, length(District)), date_cmpl = dmy_hm(Date), year = year(date_cmpl), month = month(date_cmpl, label = TRUE), day = wday(date_cmpl, label = TRUE), time = substr(Date, 12, length(Date)) %&gt;% hm() ) reports ## # A tibble: 21,952 × 8 ## Date Report District date_cmpl year month day time ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;Period&gt; ## 1 16.10.2023 … Verda… Treptow… 2023-10-16 17:55:00 2023 Okt Mo 17H 55M 0S ## 2 16.10.2023 … Gefäh… Charlot… 2023-10-16 17:49:00 2023 Okt Mo 17H 49M 0S ## 3 16.10.2023 … Vorko… berlinw… 2023-10-16 14:43:00 2023 Okt Mo 14H 43M 0S ## 4 16.10.2023 … Festn… Friedri… 2023-10-16 11:26:00 2023 Okt Mo 11H 26M 0S ## 5 16.10.2023 … Verle… Marzahn… 2023-10-16 10:07:00 2023 Okt Mo 10H 7M 0S ## 6 16.10.2023 … Brand… Friedri… 2023-10-16 09:53:00 2023 Okt Mo 9H 53M 0S ## 7 15.10.2023 … Verke… Mitte 2023-10-15 21:07:00 2023 Okt So 21H 7M 0S ## 8 15.10.2023 … Raub … Charlot… 2023-10-15 15:46:00 2023 Okt So 15H 46M 0S ## 9 15.10.2023 … Einka… Neukölln 2023-10-15 14:59:00 2023 Okt So 14H 59M 0S ## 10 15.10.2023 … Vorko… berlinw… 2023-10-15 14:47:00 2023 Okt So 14H 47M 0S ## # ℹ 21,942 more rows 11.2 select() As we have already extracted all we need from the “Date” and “date_cmpl” columns, and we will not work with the “Report” column in this chapter, to keep the tibble neat and free of clutter, we remove the three columns by using the function select(). select() is used for selecting columns from a tibbble. The function takes the data as its first argument – here provided by the pipe – and one or several names of columns that should be kept. If columns follow after each other in order, we can also use a “from:to” notation. reports %&gt;% select(District, year, month, day, time) ## # A tibble: 21,952 × 5 ## District year month day time ## &lt;chr&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;Period&gt; ## 1 Treptow-Köpenick 2023 Okt Mo 17H 55M 0S ## 2 Charlottenburg-Wilmersdorf 2023 Okt Mo 17H 49M 0S ## 3 berlinweit 2023 Okt Mo 14H 43M 0S ## 4 Friedrichshain-Kreuzberg 2023 Okt Mo 11H 26M 0S ## 5 Marzahn-Hellersdorf 2023 Okt Mo 10H 7M 0S ## 6 Friedrichshain-Kreuzberg 2023 Okt Mo 9H 53M 0S ## 7 Mitte 2023 Okt So 21H 7M 0S ## 8 Charlottenburg-Wilmersdorf 2023 Okt So 15H 46M 0S ## 9 Neukölln 2023 Okt So 14H 59M 0S ## 10 berlinweit 2023 Okt So 14H 47M 0S ## # ℹ 21,942 more rows reports %&gt;% select(District, year:time) ## # A tibble: 21,952 × 5 ## District year month day time ## &lt;chr&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;Period&gt; ## 1 Treptow-Köpenick 2023 Okt Mo 17H 55M 0S ## 2 Charlottenburg-Wilmersdorf 2023 Okt Mo 17H 49M 0S ## 3 berlinweit 2023 Okt Mo 14H 43M 0S ## 4 Friedrichshain-Kreuzberg 2023 Okt Mo 11H 26M 0S ## 5 Marzahn-Hellersdorf 2023 Okt Mo 10H 7M 0S ## 6 Friedrichshain-Kreuzberg 2023 Okt Mo 9H 53M 0S ## 7 Mitte 2023 Okt So 21H 7M 0S ## 8 Charlottenburg-Wilmersdorf 2023 Okt So 15H 46M 0S ## 9 Neukölln 2023 Okt So 14H 59M 0S ## 10 berlinweit 2023 Okt So 14H 47M 0S ## # ℹ 21,942 more rows Instead of telling select() which columns we want to keep, we can also tell it which ones not to keep by adding a - before the column names. If we want to exclude multiple columns in one step, we combine them with the c() function and write the - directly before it. In this last example we will assign the result to reports. reports &lt;- reports %&gt;% select(-c(Date, Report, date_cmpl)) reports ## # A tibble: 21,952 × 5 ## District year month day time ## &lt;chr&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;Period&gt; ## 1 Treptow-Köpenick 2023 Okt Mo 17H 55M 0S ## 2 Charlottenburg-Wilmersdorf 2023 Okt Mo 17H 49M 0S ## 3 berlinweit 2023 Okt Mo 14H 43M 0S ## 4 Friedrichshain-Kreuzberg 2023 Okt Mo 11H 26M 0S ## 5 Marzahn-Hellersdorf 2023 Okt Mo 10H 7M 0S ## 6 Friedrichshain-Kreuzberg 2023 Okt Mo 9H 53M 0S ## 7 Mitte 2023 Okt So 21H 7M 0S ## 8 Charlottenburg-Wilmersdorf 2023 Okt So 15H 46M 0S ## 9 Neukölln 2023 Okt So 14H 59M 0S ## 10 berlinweit 2023 Okt So 14H 47M 0S ## # ℹ 21,942 more rows 11.3 rename() As you may have noticed, all newly created columns are written in lower case, while “District” begins with an upper case letter. You may want to rename this column to a lower case name – or all the others to upper case names, depending on your preference. One approach to renaming is using the function rename() from dplyr: the function takes the data to be applied to as its first argument – again passed on by the pipe here – followed by one or more arguments in the form new_name = old_name. reports &lt;- reports %&gt;% rename(district = District) reports ## # A tibble: 21,952 × 5 ## district year month day time ## &lt;chr&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;Period&gt; ## 1 Treptow-Köpenick 2023 Okt Mo 17H 55M 0S ## 2 Charlottenburg-Wilmersdorf 2023 Okt Mo 17H 49M 0S ## 3 berlinweit 2023 Okt Mo 14H 43M 0S ## 4 Friedrichshain-Kreuzberg 2023 Okt Mo 11H 26M 0S ## 5 Marzahn-Hellersdorf 2023 Okt Mo 10H 7M 0S ## 6 Friedrichshain-Kreuzberg 2023 Okt Mo 9H 53M 0S ## 7 Mitte 2023 Okt So 21H 7M 0S ## 8 Charlottenburg-Wilmersdorf 2023 Okt So 15H 46M 0S ## 9 Neukölln 2023 Okt So 14H 59M 0S ## 10 berlinweit 2023 Okt So 14H 47M 0S ## # ℹ 21,942 more rows 11.4 filter() Some of our analysis requires us to have complete years in our data. If we want to compare the police reports by year, it makes no sense to compare the numbers for an ongoing year with those that are complete, since the former will obviously have fewer reports. Incomplete years would also impact analysis by month, as some months will have more observations than others; therefore, we should filter our data for complete years. To filter the observations, we can use the function filter(). As always in tidyverse functions, filter() takes the data to be filtered as its first argument and one or multiple expressions that specify the rules by which to filter. To write these expressions, we can make use of the comparison operators discussed in subchapter 1.3.2. To filter for all years that are not 2022, we can write: reports %&gt;% filter(year != 2022) ## # A tibble: 19,716 × 5 ## district year month day time ## &lt;chr&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;Period&gt; ## 1 Treptow-Köpenick 2023 Okt Mo 17H 55M 0S ## 2 Charlottenburg-Wilmersdorf 2023 Okt Mo 17H 49M 0S ## 3 berlinweit 2023 Okt Mo 14H 43M 0S ## 4 Friedrichshain-Kreuzberg 2023 Okt Mo 11H 26M 0S ## 5 Marzahn-Hellersdorf 2023 Okt Mo 10H 7M 0S ## 6 Friedrichshain-Kreuzberg 2023 Okt Mo 9H 53M 0S ## 7 Mitte 2023 Okt So 21H 7M 0S ## 8 Charlottenburg-Wilmersdorf 2023 Okt So 15H 46M 0S ## 9 Neukölln 2023 Okt So 14H 59M 0S ## 10 berlinweit 2023 Okt So 14H 47M 0S ## # ℹ 19,706 more rows Thus, only the observations where “year” does not equal “2022” remain in the tibble; more accurately, those observations for which the expression year != 2022 is returned as TRUE. Closer inspection of the data reveals, that 2014 (the first year that is available on the website) is not complete either. There also seems to be one single report for 2013. We should also exclude these years for analysis. We can chain multiple expressions when using filter() separated by commas. reports %&gt;% filter(year != 2022, year != 2014, year != 2013) ## # A tibble: 19,012 × 5 ## district year month day time ## &lt;chr&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;Period&gt; ## 1 Treptow-Köpenick 2023 Okt Mo 17H 55M 0S ## 2 Charlottenburg-Wilmersdorf 2023 Okt Mo 17H 49M 0S ## 3 berlinweit 2023 Okt Mo 14H 43M 0S ## 4 Friedrichshain-Kreuzberg 2023 Okt Mo 11H 26M 0S ## 5 Marzahn-Hellersdorf 2023 Okt Mo 10H 7M 0S ## 6 Friedrichshain-Kreuzberg 2023 Okt Mo 9H 53M 0S ## 7 Mitte 2023 Okt So 21H 7M 0S ## 8 Charlottenburg-Wilmersdorf 2023 Okt So 15H 46M 0S ## 9 Neukölln 2023 Okt So 14H 59M 0S ## 10 berlinweit 2023 Okt So 14H 47M 0S ## # ℹ 19,002 more rows Often there are multiple ways to formulate the filtering expressions. Here, we could tell R which values for “year” we want to keep, instead of which we do not want to keep. Instead of listing all those individual years, we can use %in% to define a range of numerical values which should be kept when filtering. We assign the result to a new object for later use. reports_fyears &lt;- reports %&gt;% filter(year %in% 2015:2022) While the , behaves like the logical operator &amp;, we can also use | for “or” when combining expressions to be filtered upon. 11.5 summarise() &amp; group_by() We can now begin computing some basic summary statistics for our data. You should note though, that we do not know how the data we scraped is actually created. Do the dates and times on the website refer to the instance in time when a crime occurred – there are indications that this is not the case –, when a report was filed, or even when the PR team of the Berlin police uploaded the report to the website – which might even be the most plausible scenario? Also, are all crimes reported on this website – the daily numbers are too low for this to be the case – or is there an internal selection process? And if so, on what criteria does the selection occur? If this was a real research project, we would absolutely need to gain clarity on these and many other questions, before we even begin with collecting the data. We do not have clarity here, but we have to keep in mind that we may not analyse the statistical distribution of crimes in Berlin, but rather the working practice of the PR team. 11.5.1 Number of reports by groups The dplyr function summarise() can be used to calculate summary statistics for a whole tibble; the syntax being similar to mutate(). The result is a new tibble, containing only the summary statistics we requested. The function we use for the summary statistic here is n(), which returns the number of observations. As each observation represents one police report, this equals the overall number of reports. reports_fyears %&gt;% summarise(reports = n()) ## # A tibble: 1 × 1 ## reports ## &lt;int&gt; ## 1 19508 This worked, but is not very interesting, as the result is the same as the length of the reports tibble. We are often more interested in summaries grouped by the value of one or more other variables. We might ask ourselves, if there are differences in the number of released police reports by year; for this purpose, we can group the data by the values of the “year” column using group_by() and then compute the summary statistic separately for each group. Here, n() returns the number of observations for each group, i.e. the number of reports per year. As always with dplyr functions, group_by() needs the data which will be grouped as the first argument, followed by one or multiple columns to group by. reports_fyears %&gt;% group_by(year) %&gt;% summarise(reports = n()) ## # A tibble: 8 × 2 ## year reports ## &lt;dbl&gt; &lt;int&gt; ## 1 2015 2369 ## 2 2016 2626 ## 3 2017 2233 ## 4 2018 2342 ## 5 2019 2619 ## 6 2020 2583 ## 7 2021 2500 ## 8 2022 2236 While there are differences between the years, there does not seem to be a systematic pattern to it. We can do the same kind of analysis grouped by months and weekdays. reports_fyears %&gt;% group_by(month) %&gt;% summarise(reports = n()) ## # A tibble: 12 × 2 ## month reports ## &lt;ord&gt; &lt;int&gt; ## 1 Jan 1735 ## 2 Feb 1530 ## 3 Mär 1702 ## 4 Apr 1657 ## 5 Mai 1717 ## 6 Jun 1579 ## 7 Jul 1645 ## 8 Aug 1648 ## 9 Sep 1593 ## 10 Okt 1627 ## 11 Nov 1550 ## 12 Dez 1525 reports_fyears %&gt;% group_by(day) %&gt;% summarise(reports = n()) ## # A tibble: 7 × 2 ## day reports ## &lt;ord&gt; &lt;int&gt; ## 1 So 2688 ## 2 Mo 2663 ## 3 Di 2790 ## 4 Mi 2909 ## 5 Do 2916 ## 6 Fr 2903 ## 7 Sa 2639 The analysis by month again shows no systematic variation. Looking at the reports by day of the week on the other hand, shows a slight increase in reports over the week, culminating on Friday. As stated above, we do not really know how the data is created, so the main point to take away from this analysis might be, that the PR team of the Berlin police also works on the weekend. In the same manner, we can analyse the number of released police reports by district. reports_fyears %&gt;% group_by(district) %&gt;% summarise(reports = n()) ## # A tibble: 22 × 2 ## district reports ## &lt;chr&gt; &lt;int&gt; ## 1 Charlottenburg - Wilmersdorf 513 ## 2 Charlottenburg-Wilmersdorf 1279 ## 3 Friedrichshain - Kreuzberg 577 ## 4 Friedrichshain-Kreuzberg 1528 ## 5 Lichtenberg 1075 ## 6 Marzahn - Hellersdorf 225 ## 7 Marzahn-Hellersdorf 758 ## 8 Mitte 2936 ## 9 Neukölln 1734 ## 10 Pankow 1438 ## # ℹ 12 more rows Before we interpret the result, let us examine the district names; districts with a dash are written in two ways: with whitespace around the dash and without one. There may have been a change in the way these names are recorded in the data over the years and we have to deal with it. We can use the function str_remove_all() from stringr to remove every occurrence of a pattern we specify as its argument. If we remove the pattern ” ” we basically delete all whitespace. This can again be combined with mutate() to transform the data in our tibble. reports_fyears &lt;- reports_fyears %&gt;% mutate(district = str_remove_all(district, pattern = &quot; &quot;)) reports_fyears %&gt;% group_by(district) %&gt;% summarise(reports = n()) %&gt;% arrange(desc(reports)) ## # A tibble: 16 × 2 ## district reports ## &lt;chr&gt; &lt;int&gt; ## 1 Mitte 2936 ## 2 Friedrichshain-Kreuzberg 2105 ## 3 Charlottenburg-Wilmersdorf 1792 ## 4 Neukölln 1734 ## 5 Tempelhof-Schöneberg 1542 ## 6 Pankow 1438 ## 7 Treptow-Köpenick 1197 ## 8 Lichtenberg 1075 ## 9 Reinickendorf 1038 ## 10 Spandau 1024 ## 11 Marzahn-Hellersdorf 983 ## 12 Steglitz-Zehlendorf 946 ## 13 bezirksübergreifend 921 ## 14 &lt;NA&gt; 382 ## 15 berlinweit 340 ## 16 bundesweit 55 For the grouped summary we also included arrange(desc()) which orders the values from highest to lowest. If you omit desc() the results are ordered ascending. We might also be interested in the relative share of reports by district. To compute those, one option is to add another column to the tibble resulting from summarise(), in which we calculate the relative share by dividing the “reports” columns values by the total of this column. reports_fyears %&gt;% group_by(district) %&gt;% summarise(reports = n()) %&gt;% arrange(desc(reports)) %&gt;% mutate(reports_rel = reports / sum(reports)) ## # A tibble: 16 × 3 ## district reports reports_rel ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Mitte 2936 0.151 ## 2 Friedrichshain-Kreuzberg 2105 0.108 ## 3 Charlottenburg-Wilmersdorf 1792 0.0919 ## 4 Neukölln 1734 0.0889 ## 5 Tempelhof-Schöneberg 1542 0.0790 ## 6 Pankow 1438 0.0737 ## 7 Treptow-Köpenick 1197 0.0614 ## 8 Lichtenberg 1075 0.0551 ## 9 Reinickendorf 1038 0.0532 ## 10 Spandau 1024 0.0525 ## 11 Marzahn-Hellersdorf 983 0.0504 ## 12 Steglitz-Zehlendorf 946 0.0485 ## 13 bezirksübergreifend 921 0.0472 ## 14 &lt;NA&gt; 382 0.0196 ## 15 berlinweit 340 0.0174 ## 16 bundesweit 55 0.00282 The results show clear differences in the number of reports by district, with “Mitte” leading by a substantial margin. Other districts with high counts seem to be those that at least in part lie within the inner ring. The outer districts all show lower numbers. Again, we do not know how the data is created, so maybe it is just the case that the crimes in the inner ring are more interesting than in other districts and that the data does not necessarily point to a higher number of crimes in central Berlin. To get a more complete picture, we would also have to relate the number of reports to the citizens or even tourists per district. For this we would need additional external data though. Using group_by() we can also group by multiple columns. “Mitte” and “Friedrichshain-Kreuzberg” showed the highest numbers among all districts. Let us analyse if these numbers changed over the years for these two districts. First we have to use filter() to use only the observations referring to those districts and then group the data by the remaining districts and year before we count the the number of released reports. reports_fyears %&gt;% filter(district == &quot;Mitte&quot; | district == &quot;Friedrichshain-Kreuzberg&quot;) %&gt;% group_by(district, year) %&gt;% summarise(reports = n()) ## `summarise()` has grouped output by &#39;district&#39;. ## You can override using the `.groups` argument. ## # A tibble: 16 × 3 ## # Groups: district [2] ## district year reports ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Friedrichshain-Kreuzberg 2015 277 ## 2 Friedrichshain-Kreuzberg 2016 269 ## 3 Friedrichshain-Kreuzberg 2017 234 ## 4 Friedrichshain-Kreuzberg 2018 237 ## 5 Friedrichshain-Kreuzberg 2019 285 ## 6 Friedrichshain-Kreuzberg 2020 301 ## 7 Friedrichshain-Kreuzberg 2021 312 ## 8 Friedrichshain-Kreuzberg 2022 190 ## 9 Mitte 2015 367 ## 10 Mitte 2016 448 ## 11 Mitte 2017 347 ## 12 Mitte 2018 363 ## 13 Mitte 2019 391 ## 14 Mitte 2020 374 ## 15 Mitte 2021 328 ## 16 Mitte 2022 318 There does not seem to be a clear pattern for the number of reports over time in both districts. For “Mitte” there is a considerable spike in 2016. To analyse this further, we had to go deeper into the data and look at the actual texts of the police reports. But we should briefly talk about a peculiarity to the way group_by() works with summarise() that can cause headaches, if we are not aware of it. In general, any summarise() function following a group_by() will calculate the summary statistic and then remove one level of grouping. In the examples where we only had one level of grouping, this essentially meant, that the data was ungrouped after summarise(). In the last example we had two levels of grouping. So, after the computation of the number of reports by “district” and “year” the grouping by “year” was removed, but the grouping by “district” remained in effect. We can see this in the output, where R informs us about the column by which the data are grouped and the number of groups in the output: # Groups: district [2]. Another summarise() function would compute the statistic by “district” and then remove this level as well. We can also use ungroup() to remove all grouping from a tibble. In the case of this example, it does not make a practical difference as we only compute the summary and then move on. But, if we assign the results of a summary to an object for later use, we have to decide if we want to remove the grouping or keep it in effect, depending on the goals of the analysis. In general, I would always ungroup the data and group it again if the need arises, as this is less error prone and just a minor increase in the amount of typing needed. 11.5.2 Summary statistics on the time The column “time” holds the time of day for each report in hours, minutes and seconds as a representation of the actual values, which are seconds from midnight. reports_fyears[[1, &#39;time&#39;]] ## [1] &quot;13H 31M 0S&quot; reports_fyears[[1, &#39;time&#39;]] %&gt;% as.numeric() ## [1] 48660 To calculate with the “time” column, we first have to tell R that we explicitly want to use the data as seconds, then compute the summary statistic and after this, transform the display of seconds back into the time of day. For the transformations we can use the functions period_to_seconds() and seconds_to_period(). We will calculate the mean and the median for the time of day over all police reports in one call of summarise(). reports %&gt;% summarise(mean = period_to_seconds(time) %&gt;% mean() %&gt;% seconds_to_period(), median = period_to_seconds(time) %&gt;% median() %&gt;% seconds_to_period() ) ## # A tibble: 1 × 2 ## mean median ## &lt;Period&gt; &lt;Period&gt; ## 1 11H 59M 37.1911443148711S 11H 13M 0S The mean time of day over all police reports is about 11:59 while the median is 11:13. This may indicate that the mean is biased towards a later time by a high number of very late reports. We will explore this further graphically in the next chapter. Again, the results may represent the time when crimes occur, but it is more likely, that we are actually analysing the time when the PR team posts their police reports. 11.6 Exporting tables Exporting the results of our analysis – e.g. summary statistics computed with summarise() – sadly is not as straightforward as one might think or hope. We can always copy and paste values from the output into Word or other pieces of software we might use for writing a report. But this is cumbersome and error prone. One way of directly exporting tables we already know about is writing them to a .csv file. For more on this please review section 8.1. These can be imported into Excel and also directly into Word. This may be a quick way of exporting the values of a table and importing them into Office, but we have to do all the formatting in the external software and may have to repeat the formatting procedure every time the values change. So we should make sure that all errors have been fixed and the analysis is final before exporting to .csv. In this example we save the number of police reports by year for the districts “Mitte” and “Friedrichshain-Kreuzberg” we computed earlier into a .csv file: reports_fyears %&gt;% filter(district == &quot;Mitte&quot; | district == &quot;Friedrichshain-Kreuzberg&quot;) %&gt;% group_by(district, year) %&gt;% summarise(reports = n()) %&gt;% write_csv(&quot;reports_year_M-FK.csv&quot;) ## `summarise()` has grouped output by &#39;district&#39;. ## You can override using the `.groups` argument. For reasons unknown to anyone but Microsoft, importing a .csv file that uses commas as the delimiter can actually be quite tricky on a German Windows installation if you are using Microsoft Office. Using Libre Office fixes this, but if you want to use Microsoft Office on a German system you might have to use semicolons as the delimiter by saving the file with write_csv2(). 11.6.1 Further resources A multitude of packages that provide functions for formatting and writing tables to different formats exist in the R world. To go into details is beyond the scope of this introduction, but I want to give you some pointers at least. If you want to skip importing a .csv into Microsoft Office, you may be interested in packages that allow writing directly to .xlsx files. Two options are: writexl: https://cran.r-project.org/web/packages/writexl/index.html openxlsx: https://cran.r-project.org/web/packages/openxlsx/index.html A way to directly produce tables in the .docx format is provided by the flextable package: https://davidgohel.github.io/flextable/ If your are working with LaTeX, the huxtable package can directly output .tex files, as well as HTML, Word, Excel and Powerpoint: https://hughjonesd.github.io/huxtable/ If you want to circumvent using external software for producing reports completely, R Markdown may be of interest to you. This allows you to write your report and your R code in one document and write it to .pdf or a range of other formats. The produced file includes all the output from your code that you want to report, i.e. code, plots and tables. Some resources on R Markdown can be found here: Introduction on the RStudio website: https://rmarkdown.rstudio.com/lesson-1.html{target_“blank”} The R Markdown cheat sheet: https://raw.githubusercontent.com/rstudio/cheatsheets/master/rmarkdown-2.0.pdf Chapter on R Markdown in “R for Data Science” by Hadley Wickham and Garrett Grolemund: https://r4ds.had.co.nz/r-markdown.html “R Markdown Cookbook” by Yihui Xie, Christophe Dervieux, Emily Riederer: https://bookdown.org/yihui/rmarkdown-cookbook/ The website your are viewing right now, as well as many of the linked resources, are built with the bookdown package, which extends the functionality of R Markdown and is suited for more large scale projects. “bookdown: Authoring Books and Technical Documents with R Markdown” by Yihui Xie: https://bookdown.org/yihui/bookdown/ "],["ggplot.html", "12 Graphical analysis with ggplot 12.1 ggplot2 syntax 12.2 Geoms and aesthetics 12.3 Exporting plots", " 12 Graphical analysis with ggplot Since we have cleaned our example data in the last chapter, we will now apply some graphical analysis to it, using the ggplot2 package. To follow most of the examples, we need the object reports_fyears from chapter 11. Either you can re-run the relevant code blocks from the last chapter, or the condensed block below. We also will need to load the tidyverse package – which includes ggplot2 – and lubridate. library(tidyverse) library(lubridate) load(&quot;reports.RData&quot;) reports_fyears &lt;- reports %&gt;% mutate(District = substr(District, 14, length(District)), date_cmpl = dmy_hm(Date), year = year(date_cmpl), month = month(date_cmpl, label = TRUE), day = wday(date_cmpl, label = TRUE), time = substr(Date, 12, length(Date)) %&gt;% hm() ) %&gt;% select(-c(Date, Report, date_cmpl)) %&gt;% rename(district = District) %&gt;% filter(year %in% 2015:2022) %&gt;% mutate(district = str_remove_all(district, pattern = &quot; &quot;)) 12.1 ggplot2 syntax The basic principle of ggplot2 is that we initiate an empty plot using ggplot() – note that while the package is called ggplot2, the function is written without the “2” – and then add one or multiple geoms, the graphical elements that shall be plotted – e.g. points, lines, bars and so on. We will look at some practical examples of geoms soon. ggplot() has to be provided the name of the object that contains the data we want to plot as its first argument. ggplot(data = reports_fyears) As you can see in the “Plots” tab located in the lower right of RStudio, a plot was created but it is still empty because we have not added any geoms yet. 12.2 Geoms and aesthetics Geoms represent the graphical objects we actually want to plot. All geom functions start with geom_ and end in a word describing the type of geom, e.g. geom_point() for scatter plots, geom_line() for lines or geom_col() for bar plots. To get an overview of the available geoms, I highly recommend the R cheat sheet for ggplot2, accessible here: https://raw.githubusercontent.com/rstudio/cheatsheets/master/data-visualization-2.1.pdf We add geoms to a plot by writing a + after ggplot() and adding the geom in a new line. Additional geoms can be added in the same manner. So the basic syntax, this is not runnable code, looks like this: ggplot(data = ...) + geom_1() + geom_2() + ... The aesthetics of a geom are used to assign the x and y variables that will be plotted on the coordinate system. Additionally, aesthetics can be used to influence the visual display of the plotted elements by the value of a variable, e.g. the colour of points or the thickness of a line. We will look at examples that show this later, and we will first focus on assigning variables to the axes, as these are the only aesthetics that have to be provided, everything else being optional. To define the aesthetics we use aes() as an argument of the geom function and assign variables to the horizontal x-axis and vertical y-axis of the plot, as shown below. Once more, the code block is used for illustration only and will not run. ggplot(data = ...) + geom_1(aes(x = x_variable, y = y_variable)) + geom_2(aes(x = x_variable, y = y_variable)) + ... If we are using the same x and y variables for all geoms, we can define them directly in the call of ggplot(). ggplot(data = ..., aes(x = x_variable, y = y_variable)) + geom_1() + geom_2() + ... 12.2.1 Continuous x, continuous y Variables are continuous, when they are able to take on every value – maybe limited by a maximum and minimum; they are also always numerical. Examples that are frequently used in the social sciences are income or monetary values in general, if they are measured accurately and not in a number of broad categories. The latter would be an example of a categorical variable, as the variable can only take on a number of defined categories. Income groups would be an example for this, as would be gender or the party that has been voted for in the last election. In our example data, we mostly find categorical variables like district or day, while time and year are the only continuous variable. Plotting time against year makes no inherent sense though. To be able to construct plots for continuous by continuous variables – which are important types of plots – we will thus construct a simple example dataset using random values. Let us create fictional data for the size and the price of apartments in a city. Let us assume, that the mean value for size in square meters is \\(70\\) and the mean value for the price in Euro per square meter is \\(10\\). The actual values should be evenly distributed around those means. There should also be about \\(20\\)% of premium apartments, that for some reason – maybe they are in an exceptionally expensive district – cost \\(10\\) Euros more per square meter. The following code block – which can not be explained in detail here – will generate the data just described. set.seed() assures that you will get the same random number generator results every time. set.seed(01042022) rent &lt;- tibble( size = rnorm(1000, mean = 70, sd = 15), premium = as.logical(rbinom(n = 1000, size = 1, prob = 0.2)), price = size * (10 + rnorm(1000, mean = 0, sd = 3) + (10 * premium)) ) rent %&gt;% head(n = 5) ## # A tibble: 5 × 3 ## size premium price ## &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 84.5 FALSE 881. ## 2 65.1 FALSE 445. ## 3 76.0 FALSE 727. ## 4 65.3 FALSE 892. ## 5 70.3 FALSE 461. We can now plot this data. The price of an apartment depends on its size, thus price should be plotted on the y-axis which is usually used for the dependent variable and the independent variable size on the x-axis. Our assumption – and we know it is true as we created the data this way – is that the size of an apartment explains its price. We will use the geom_point() to build a scatterplot, with each point representing one of the 1000s of combinations of size and price in the data. ggplot(data = rent, aes(x = size, y = price)) + geom_point() Looking at the data points, we clearly see a relationship. Overall, the bigger an apartment is, the higher is its price point. We know this is true in our data and the relationship is very clearly visible, but we can confirm this assumption by also plotting a regression line over the points. This will visualise the linear relationship between both variables in the way that best fits the data using a straight line. We can use geom_smooth() for this purpose, specifying that we want to use method = 'lm', which is a linear model. The second option se = FALSE means that we do not want to plot the confidence intervals around the regression line, which is a measure of the uncertainty of the estimated relationship. ggplot(data = rent, aes(x = size, y = price)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; The angle of the regression line shows us the correlation between size and price. Also we can use it to estimate the mean value of y for every given x. An apartment of 50 square meters in average costs about 600 Euros, for example. Remember that we added a third variable to the dataset, indicating whether it is a premium apartment or not, by using a logical variable. We can use this as an aesthetic to group the data in the plot by its values and in this way generate different regression lines per group using the aesthetic group = premium or we can use colour = premium to simultaneously group the data and colour the plotted objects by their group membership. ggplot(data = rent, aes(x = size, y = price, colour = premium)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; We see clearly now, that the steepness of both lines differs. For premium apartments the correlation between square meters and price is stronger than for non-premium apartments. Sidenote: Here the relationship between the three variables was known beforehand, as we have designed the data in this way. With real data, we would not know the relationship. Based on theoretical considerations we can assume a correlation between two variables and assess its presence in our actual data visually using a plot of this kind. 12.2.2 Graphical analysis of Berlin police reports We will now return to our scraped data on police reports in Berlin and follow up the summary statistics with additional graphical analysis, broadening our view on the patterns in the data. 12.2.2.1 Categorical x variables In the last chapter we saw that there seems to be a rise of police reports during the week, culminating on Fridays. If we want to visualise the number of police reports per day, one common approach is to use a bar plot. geom_bar()just needs to be supplied with the variable holding the categories – here day – and will then count the number of occurrences of the category which are plotted as bars with corresponding height. ggplot(data = reports_fyears, aes(x = day)) + geom_bar() We see that there is an increase over the course of the week, but that the relative differences between the days are minor. Combined with the summary statistics for the number of reports by year and month seen in chapter 11, we can conclude that there seem to be no systematic differences in the count of police reports by weekday, month or year it occurred. As we have established, we do not know how the data is created. That the number of reports is rather constant over time could point towards a quota of reports that are released by the Berlin police per day, not depending on whether much of interest happened on a specific day. Let us plot the report counts by the district, using the same approach. Let us plot the categories on the y-axis to make the long district names more readable. ggplot(data = reports_fyears, aes(y = district)) + geom_bar() This is nice, but it would be easier to interpret, if the bars were ordered by their height. We also have the category “NA” which has no meaning that we could interpret and should be removed before plotting. drop_na() will remove all observations for which the value of a specified variable is NA. We then group by district and calculate the group counts using summarize() with n(), as we have already done in chapter 11. The result is passed to ggplot() through the pipe. Here we use geom_col() which basically is a bar plot where the occurrences of each category are not counted automatically but are already present as a number in the data set. In our case we have summarised the number of reports in the column reports which is assigned to the x-axis in our plot. The y-axis variable is arranged in descending order using reorder(); the function sorts the variable that is specified in the first argument by using the values of the variable defined in the second argument. In this case ordering the categories of district by the values of reports. reports_fyears %&gt;% drop_na(district) %&gt;% group_by(district) %&gt;% summarize(reports = n()) %&gt;% ggplot(aes(x = reports, y = reorder(district, reports))) + geom_col() The plot underlines what we have already seen in chapter 11, that some districts, especially “Mitte”, are referred to by considerably more police reports than others. 12.2.2.2 Continuous x variables The time of day when a report occurred is a continuous variable; it can take all values between 00:00 and 23:59. If we want to visualise how its values are distributed across this range, we can plot a histogram. In a histogram the range of a variable is divided into bins; bins being an interval in the data. If we have 30 bins, which is the standard in geom_histogram(), the data is divided into 30 intervals of the same length. For each interval the number of actual values that fall into it is counted and displayed on the y-axis, which is therefore represented by the height of the bar. To be able to plot the values of time, we again have to convert the representation in hours, minutes and seconds to its numerical values – being the number of seconds since midnight – by using as.numeric() or period_to_seconds() (also see chapter 11). ggplot(data = reports_fyears, aes(x = period_to_seconds(time))) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better ## value with `binwidth`. We immediately see that the number of reports is not evenly distributed throughout the day. It is also not normally distributed, as the distribution is right-skewed, with high counts for low to medium values of time and lower counts for higher values that are spread out far to the right. We also see that there is a period on the left- and on the right-hand side of time where the counts are \\(0\\), meaning no reports are filed in the very early and very late hours. But the plot remains hard to interpret because the value labels that are shown on the x-axis do not immediately make sense to us. What is the time in hours and minutes when \\(25000\\) seconds have passed since midnight? As most humans – including myself – will not have an immediate answer to this question, our graphic is communicating the wrong measurement. We can use scale_x_continuous() to set options for the display of the x-axis, in this case to choose more appropriate labels. The argument breaks determines at which values the axis should be labelled; with labels we can further specify what the labels will be display. In the next code block, the values supplied to breaks are calculations of the seconds that since midnight for the hours of the day I chose, and for labels character strings using the familiar display of hours and minutes. While we are at it, we can tune the graphical output some more: with labs() we can set labels for several parts of the graphic. Here we display nice and clear labels on both axis – instead of using the name of the variables – and a title for the graphic. In chapter 11 we calculated the mean time of day over all police reports filed. We can include this in the plot by adding a vertical line using geom_vline(); with the aesthetic xintercept specifying the x-value at which we want to draw the line, in this case the mean we calculated and assigned to an object. We also use the colour argument outside of aes() to set a constant colour for the line – instead of using it as an aesthetic and thus colouring the data by the values of a third variable. Sidenote: For an overview of the available colours, have a look at: http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf. mean_time &lt;- reports_fyears$time %&gt;% period_to_seconds() %&gt;% mean() ggplot(data = reports_fyears, aes(x = period_to_seconds(time))) + geom_histogram() + scale_x_continuous(breaks = c(6 * 60 * 60, 9 * 60 * 60, 12 * 60 * 60, 15 * 60 * 60, 18 * 60 * 60, 21 * 60 * 60), labels = c(&quot;6:00&quot;, &quot;9:00&quot;, &quot;12:00&quot;, &quot;15:00&quot;, &quot;18:00&quot;, &quot;21:00&quot;) ) + labs(x = &quot;Time of Day&quot;, y = &quot;Reports&quot;, title = &quot;Distribution of police reports over time of day&quot;) + geom_vline(aes(xintercept = mean_time), colour = &quot;red&quot;) ## `stat_bin()` using `bins = 30`. Pick better ## value with `binwidth`. With readable labels and the line representing the mean as reference points, the distribution of the time of day becomes more apparent. We see that there are no reports filed beginning in the late evening to the early morning. This is another indication that the times we scraped do not actually represent the times when the event occurred but rather when the report was officially filed or uploaded to the website. From this data, we can not conclude that it is safest on the streets of Berlin at night time, but rather that the working hours of the people responsible for communicating the police reports range from about 08:00 to 19:00-20:00. We can also see that the mean we calculated in chapter 11 – 11:59 – is misleading. As a relevant number of reports are filed in the time span of late afternoon to early evening, the mean is biased to the right. 11:59 is not the peak of the distribution; actually, half of all reports are filed before 11:13, as indicated by the median, which in general is more robust to skewed distributions. There seems to be a short period of high activity in the early to late morning hours. From this data, we can again infer more about the working context of the police PR team than about crimes in Berlin. It seems reasonable that reports on the events that occurred in the night pile up and are then quickly published in the morning when office hours begin. After this, the number of reports gets considerably lower as events are reported at the rate they they occur at and quite possibly team members already finished their working hours. We can also ask if the distribution of the time of day when the reports are published, has changed over time. To compare plots grouped by the values of another variable – here year –, we can use facets. As the first argument of facet_wrap() we specify the variable by which we want subgroups to be formed, using the notation ~ variable. The argument nrows determines the number of rows to be displayed. ggplot(data = reports_fyears, aes(x = period_to_seconds(time))) + geom_histogram() + facet_wrap(~ year, nrow = 2) + scale_x_continuous(breaks = c(6 * 60 * 60, 12 * 60 * 60, 18 * 60 * 60), labels = c(&quot;6:00&quot;, &quot;12:00&quot;, &quot;18:00&quot;) ) + labs(x = &quot;Time of Day&quot;, y = &quot;Reports&quot;, title = &quot;Distribution of police reports over time of day by year&quot;) ## `stat_bin()` using `bins = 30`. Pick better ## value with `binwidth`. Looking at the high numbers of very early reports and their maximum, we can see a gradual and moderate shift from the early to the late morning, which is especially apparent from 2019 to 2021. Because there are still reports beginning from about 08:00, we can not necessarily infer that the working hours have changed, but maybe that the working process has. Maybe the writers started favouring the quality of the written reports over the quantity of early reports. Maybe new rules have been implemented that require reports to be approved by some central authority, which might slow down the rate. One also has to keep in mind, that the Corona pandemic started in early 2020, so the small shift to the right could be an effect of work in the context of home-office, child-minding and home-schooling. While some of these interpretations may seem reasonable, remember that we do not actually have enough information to draw any solid conclusions on either the number and distribution of crimes in Berlin or the working hours of the PR team. 12.3 Exporting plots Having created a beautiful plot, we have to start thinking about how to export it to a format we can use in an external piece of software, e.g. Word or LaTeX. The ggplot2 function ggsave() provides this functionality in an easy syntax. If we only provide a file name as its first argument, ggsave() defaults to saving the last plot created. Besides choosing a concise name for the file, we also have to decide on a format. In general I would advice on using vector graphics, e.g. .eps or .svg, for saving your plots, as these are scalable. The advantage being that you will be able to increase and decrease the size of the graphic in the software you chose for writing a document, without sacrificing image quality. Formats that are based on saving a graphic as pixels, like .png or .jpeg, will use compression, already lowering the image quality. Furthermore, increasing the size of such a graphic, will result in a blurry image. Decreasing the size works better, but will often also introduce graphical artefacts. We can set the format directly in the file name’s extension. Here we save the last created plot (Distribution of police reports over time of day by year) as an .eps file. We could also provide a path argument. If we do not, the file is saved in the current working directory. ggsave(&quot;distr_preports.eps&quot;) ## Saving 7 x 5 in image ## `stat_bin()` using `bins = 30`. Pick better ## value with `binwidth`. If we want to export graphics in this manner, we have to use ggsave() directly after creating a plot. An alternative would be to assign plots to objects which can then be exported at a later point in the code. Here we assign two plots to two different objects and use ggsave() and it’s plot argument to specify the objects to be saved. Note that you will not get any plotted output in this way. To see the plot, we can always call the created object by typing it’s name in the console. I would advice building the plot until you are satisfied and only then assign it to an object. distr_reports_day &lt;- ggplot(data = reports_fyears, aes(x = period_to_seconds(time))) + geom_histogram() + scale_x_continuous(breaks = c(6 * 60 * 60, 9 * 60 * 60, 12 * 60 * 60, 15 * 60 * 60, 18 * 60 * 60, 21 * 60 * 60), labels = c(&quot;6:00&quot;, &quot;9:00&quot;, &quot;12:00&quot;, &quot;15:00&quot;, &quot;18:00&quot;, &quot;21:00&quot;) ) + labs(x = &quot;Time of Day&quot;, y = &quot;Reports&quot;, title = &quot;Distribution of police reports over time of day&quot;) + geom_vline(aes(xintercept = mean_time), colour = &quot;red&quot;) n_reports_district &lt;- reports_fyears %&gt;% drop_na(district) %&gt;% group_by(district) %&gt;% summarize(reports = n()) %&gt;% ggplot(aes(x = reports, y = reorder(district, reports))) + geom_col() ggsave(&quot;reports_timeofday.eps&quot;, plot = distr_reports_day) ## Saving 7 x 5 in image ## `stat_bin()` using `bins = 30`. Pick better ## value with `binwidth`. ggsave(&quot;reports_per_district.eps&quot;, plot = n_reports_district) ## Saving 7 x 5 in image There are additional arguments that can be set. Arguments that control the size and resolution of the output may be of particular interest when saving a file in a format that is based on pixels. For more information see ?ggsave(). "],["regex.html", "13 Regular expressions 13.1 Cleaning the district column 13.2 Detecting robberies in police reports 13.3 Extracting details from a list of names", " 13 Regular expressions In web scraping, we are often dealing with text data that has been collected from a website and may include information that is not easily accessible and usable in data analysis. We are interested either in detecting the occurrence of certain words or in extracting some information from the strings in our data. For these purposes we will use some functions from the core tidyverse package stringr. We will also use regular expressions, a powerful and flexible formal representation of characters that we can use to select substrings by their content. This chapter will make use of some practical examples, introducing the basics of regular expressions and stringr. This can only be a starting point; if you want to follow the road of web scraping, you most probably will have to further your knowledge on regular expressions at some point. I would recommended reading the chapter on strings from “R for Data Science” by Wickham and Grolemund as a next step: https://r4ds.had.co.nz/strings.html, as well as the RStudio Cheat Sheet on stringr: https://raw.githubusercontent.com/rstudio/cheatsheets/master/strings.pdf. 13.1 Cleaning the district column We can use the data on Berlin police reports scraped earlier (10) for our examples. library(tidyverse) load(&quot;reports.RData&quot;) In our example the data on police reports contain the district a report refers to as a column which needs some cleaning to be usable. We have already done this in subchapter 11.1.2, but we will use some alternative and more advanced approaches here. reports %&gt;% select(District) %&gt;% head(n = 10) ## # A tibble: 10 × 1 ## District ## &lt;chr&gt; ## 1 Ereignisort: Treptow-Köpenick ## 2 Ereignisort: Charlottenburg-Wilmersdorf ## 3 Ereignisort: berlinweit ## 4 Ereignisort: Friedrichshain-Kreuzberg ## 5 Ereignisort: Marzahn-Hellersdorf ## 6 Ereignisort: Friedrichshain-Kreuzberg ## 7 Ereignisort: Mitte ## 8 Ereignisort: Charlottenburg-Wilmersdorf ## 9 Ereignisort: Neukölln ## 10 Ereignisort: berlinweit One problem we encounter here is that each string begins with “Ereignisort:”. We want to get rid of this substring, as it does not contain any useful information. With str_remove() we can remove substrings from a string. Like all stringr functions we will use here, str_remove() takes the string(s) to be applied to, as its first argument. The argument pattern = specifies a regular expression for the pattern that is to be matched and removed. In the code block below, the substring \"Ereignisort: is removed for all values in the “District” column. We are using the most basic form of a regular expression here which is the exact matching of characters. In general, the complete regular expression is enclosed by \". Please note, that the whitespace following : is part of the regular expression. reports %&gt;% mutate(District = str_remove(District, pattern = &quot;Ereignisort: &quot;)) %&gt;% select(District) %&gt;% head(n = 10) ## # A tibble: 10 × 1 ## District ## &lt;chr&gt; ## 1 Treptow-Köpenick ## 2 Charlottenburg-Wilmersdorf ## 3 berlinweit ## 4 Friedrichshain-Kreuzberg ## 5 Marzahn-Hellersdorf ## 6 Friedrichshain-Kreuzberg ## 7 Mitte ## 8 Charlottenburg-Wilmersdorf ## 9 Neukölln ## 10 berlinweit We could also keep the substring but translate it to English. For this, we use str_replace() which also matches a regular expression, but replaces it with a string specified in replacement =. Here, the pattern is shortened to \"Ereignisort\", as we want to keep the \": \". reports %&gt;% mutate(District = str_replace(District, pattern = &quot;Ereignisort&quot;, replacement = &quot;District&quot;)) %&gt;% select(District) %&gt;% head(n = 10) ## # A tibble: 10 × 1 ## District ## &lt;chr&gt; ## 1 District: Treptow-Köpenick ## 2 District: Charlottenburg-Wilmersdorf ## 3 District: berlinweit ## 4 District: Friedrichshain-Kreuzberg ## 5 District: Marzahn-Hellersdorf ## 6 District: Friedrichshain-Kreuzberg ## 7 District: Mitte ## 8 District: Charlottenburg-Wilmersdorf ## 9 District: Neukölln ## 10 District: berlinweit In the same way, we can replace the \": \" substring with a dash. reports %&gt;% mutate(District = str_replace(District, pattern = &quot;: &quot;, replacement = &quot;-&quot;)) %&gt;% select(District) %&gt;% head(n = 10) ## # A tibble: 10 × 1 ## District ## &lt;chr&gt; ## 1 Ereignisort-Treptow-Köpenick ## 2 Ereignisort-Charlottenburg-Wilmersdorf ## 3 Ereignisort-berlinweit ## 4 Ereignisort-Friedrichshain-Kreuzberg ## 5 Ereignisort-Marzahn-Hellersdorf ## 6 Ereignisort-Friedrichshain-Kreuzberg ## 7 Ereignisort-Mitte ## 8 Ereignisort-Charlottenburg-Wilmersdorf ## 9 Ereignisort-Neukölln ## 10 Ereignisort-berlinweit In chapter 11 we saw district names with two parts that are sometimes written with and sometimes without whitespace around the dash, as seen here: reports %&gt;% group_by(District) %&gt;% summarize(reports = n()) ## # A tibble: 22 × 2 ## District reports ## &lt;chr&gt; &lt;int&gt; ## 1 Ereignisort: Charlottenburg - Wilmersdorf 579 ## 2 Ereignisort: Charlottenburg-Wilmersdorf 1427 ## 3 Ereignisort: Friedrichshain - Kreuzberg 661 ## 4 Ereignisort: Friedrichshain-Kreuzberg 1701 ## 5 Ereignisort: Lichtenberg 1232 ## 6 Ereignisort: Marzahn - Hellersdorf 253 ## 7 Ereignisort: Marzahn-Hellersdorf 881 ## 8 Ereignisort: Mitte 3314 ## 9 Ereignisort: Neukölln 1952 ## 10 Ereignisort: Pankow 1629 ## # ℹ 12 more rows We can easily unify those names using str_replace and regular expressions. reports %&gt;% mutate(District = str_replace(District, pattern = &quot; - &quot;, replacement = &quot;-&quot;)) %&gt;% group_by(District) %&gt;% summarize(reports = n()) ## # A tibble: 16 × 2 ## District reports ## &lt;chr&gt; &lt;int&gt; ## 1 Ereignisort: Charlottenburg-Wilmersdorf 2006 ## 2 Ereignisort: Friedrichshain-Kreuzberg 2362 ## 3 Ereignisort: Lichtenberg 1232 ## 4 Ereignisort: Marzahn-Hellersdorf 1134 ## 5 Ereignisort: Mitte 3314 ## 6 Ereignisort: Neukölln 1952 ## 7 Ereignisort: Pankow 1629 ## 8 Ereignisort: Reinickendorf 1193 ## 9 Ereignisort: Spandau 1170 ## 10 Ereignisort: Steglitz-Zehlendorf 1045 ## 11 Ereignisort: Tempelhof-Schöneberg 1737 ## 12 Ereignisort: Treptow-Köpenick 1346 ## 13 Ereignisort: berlinweit 389 ## 14 Ereignisort: bezirksübergreifend 953 ## 15 Ereignisort: bundesweit 63 ## 16 &lt;NA&gt; 427 If we want to remove the whitespace and unify the names in the process, we use str_remove() and the regular expression \"\\\\s\", being a shortcut for matching all kinds of whitespace. At this point we have to talk about escape characters. Some characters used in regular expressions have a special meaning, e.g. \".\", \"?\" or \"*\". If we just write one of those characters in a regular expression, R will not interpret it as a literal character but by its special meaning. \".\" used in a regular expression for example, stands for any character. If we want to match a literal . in a string though, we have to escape the dot by using the escape character \\ or actually two, telling R to take the next character literally. So to match a dot, we write \"\\\\.\". If this seems confusing and needlessly complicated, save your rage for when you find out what you have to write to match one literal backslash in a string: \\\\\\\\. Some commentary on this: https://xkcd.com/1638/ Escape characters are hard to understand when starting out using regular expressions, but for now it is enough to know that some characters have to be escaped and that you find a full list of those in the cheat sheet linked above. In a similar way, we have to use two escape characters in \"\\\\s\" to make R interpret it not as an “s” but as the shortcut for whitespace characters we want to use: reports %&gt;% mutate(District = str_remove(District, pattern = &quot;\\\\s&quot;)) %&gt;% group_by(District) %&gt;% summarize(reports = n()) ## # A tibble: 22 × 2 ## District reports ## &lt;chr&gt; &lt;int&gt; ## 1 Ereignisort:Charlottenburg - Wilmersdorf 579 ## 2 Ereignisort:Charlottenburg-Wilmersdorf 1427 ## 3 Ereignisort:Friedrichshain - Kreuzberg 661 ## 4 Ereignisort:Friedrichshain-Kreuzberg 1701 ## 5 Ereignisort:Lichtenberg 1232 ## 6 Ereignisort:Marzahn - Hellersdorf 253 ## 7 Ereignisort:Marzahn-Hellersdorf 881 ## 8 Ereignisort:Mitte 3314 ## 9 Ereignisort:Neukölln 1952 ## 10 Ereignisort:Pankow 1629 ## # ℹ 12 more rows This did not work as expected. The whitespace following the \":\" was removed, but not the whitespace around the dashes. This is the case because most of the stringr functions stop after they found the first match for the pattern in a string. But there are versions of the functions that do not stop after their first match, ending in _all: e.g. str_replace_all(). So if we use str_remove_all() in our code, all occurrences of whitespace will be removed. reports %&gt;% mutate(District = str_remove_all(District, pattern = &quot;\\\\s&quot;)) %&gt;% group_by(District) %&gt;% summarize(reports = n()) ## # A tibble: 16 × 2 ## District reports ## &lt;chr&gt; &lt;int&gt; ## 1 Ereignisort:Charlottenburg-Wilmersdorf 2006 ## 2 Ereignisort:Friedrichshain-Kreuzberg 2362 ## 3 Ereignisort:Lichtenberg 1232 ## 4 Ereignisort:Marzahn-Hellersdorf 1134 ## 5 Ereignisort:Mitte 3314 ## 6 Ereignisort:Neukölln 1952 ## 7 Ereignisort:Pankow 1629 ## 8 Ereignisort:Reinickendorf 1193 ## 9 Ereignisort:Spandau 1170 ## 10 Ereignisort:Steglitz-Zehlendorf 1045 ## 11 Ereignisort:Tempelhof-Schöneberg 1737 ## 12 Ereignisort:Treptow-Köpenick 1346 ## 13 Ereignisort:berlinweit 389 ## 14 Ereignisort:bezirksübergreifend 953 ## 15 Ereignisort:bundesweit 63 ## 16 &lt;NA&gt; 427 13.2 Detecting robberies in police reports When collecting the data on police reports, we also scraped the short description of the report’s contents, but we did no analysis on these in chapters 11 &amp; 12. We could try to categorize the texts into types of crimes using regular expressions. Doing this properly, would require a lot of planning. We would have to decide which categories of crimes we want to extract and construct flexible regular expressions for each of these. Please understand the following merely as a first outline for how to approach such a project. Let us focus solely on robberies. To begin with, we want to detect all strings that contain the term “raub” in some form. str_detect() determines whether a pattern was found in a string, returning TRUE or FALSE. We can then count the number of strings for which the result is TRUE– remember TRUE is equal to \\(1\\), FALSE equal to \\(0\\) – using sum(). reports$Report %&gt;% str_detect(pattern = &quot;raub&quot;) %&gt;% sum() ## [1] 522 But what if a text begins with “Raub”. In this case the “R” will be upper case. Pattern matching is case sensitive and that means that “Raub” is not equal to “raub”. If we want both ways of writing the term to be counted as equal, we have to write a regular expression that allows for either an upper or lower case “r” as the first character. For this we can define a class which contains both characters enclosed by []. \"[Rr]aub\" means that we want to find either an “R” or an “r” as the first character and “aub” after this; only one of the characters from the class is matched. reports$Report %&gt;% str_detect(pattern = &quot;[Rr]aub&quot;) %&gt;% sum() ## [1] 1166 If we also want to detect terms like “Räuber” or “räuberische”, we have to allow for an “ä” instead of the “a” as the second character by defining a class for the second position in the term: reports$Report %&gt;% str_detect(pattern = &quot;[rR][aä]ub&quot;) %&gt;% sum() ## [1] 1498 Let us have a look at some of the strings which returned a TRUE match, using str_detect() in filter(): telling R we want to see only lines from reports for which the pattern could be found, and only the first \\(10\\) of those. reports %&gt;% filter(str_detect(Report, pattern = &quot;[Rr][aä]ub&quot;)) %&gt;% select(Report) %&gt;% head(n=10) ## # A tibble: 10 × 1 ## Report ## &lt;chr&gt; ## 1 Festnahme nach räuberischem Diebstahl ## 2 Raub im Juweliergeschäft ## 3 Räuber festgenommen ## 4 Seniorin in ihrer Wohnung beraubt und schwer verletzt zurückgelassen ## 5 Tankstelle beraubt ## 6 Gemeinschaftlicher Raub in Wohnung – Wer kennt diesen Mann? ## 7 Raub auf Tankstelle - Wer kennt diesen Mann? ## 8 Nachtportier beraubt ## 9 Ausgeraubt und mit Pistole bedroht ## 10 Raub auf Hotel - Rezeptionist wird bedroht und verletzt This looks good at a first glance, but consider this result: reports %&gt;% filter(str_detect(Report, pattern = &quot;[Rr][aä]ub&quot;)) %&gt;% filter(Date == &quot;18.06.2021 10:20 Uhr&quot;) %&gt;% select(Report) ## # A tibble: 1 × 1 ## Report ## &lt;chr&gt; ## 1 &quot;Mit dem Rettungshubschrauber ins Krankenhaus – Motorradfahrer schwer verletz… Why was a report on a motorcycle accident included? Because the word “Rettungshubschrauber” also has “raub” in it. You see, how reliably detecting a simple string such as “raub” gets complicated very quickly. At this point, we would have to put in the hours and define a list of phrases referring to robberies that we actually want to select while constructing regular expressions that select those phrases and only those. This is beyond the scope of this introductory example, but as a first approximation we could say that we want to find all instances in which the words begin with a capital “R”, as in “Raub” or “Räuber”, or where “aub” and “äub” are prefixed with “ge”, “be” or “zu”, as in “geraubt”, “beraubt”, or “auszurauben”. To achieve this, the regular expression has to allow for four optional substrings, using grouping and the “OR” operator |. Groups are defined by enclosing several expressions in parentheses. While groups have advanced functionality, for now you can interpret them like parentheses in mathematics. The enclosed expression is evaluated first and as a whole. If one of the prefixes, connected by |, is found in a string and is then followed up by either “a” or “ä” and “ub” after this, the pattern is matched. reports$Report %&gt;% str_detect(pattern = &quot;(R|ger|ber|zur)[aä]ub&quot;) %&gt;% sum() ## [1] 1294 There are other words in the German language that are used to describe the concept of robberies. Adding all of them and the several individual pre- and/or suffixes that have to be taken into account to reliably select the terms we want to select and none of those we do not want to select, is beyond the scope of this example. But we can at least add one further word to the regular expression. Using the | operator we extend the regular expression to also detect “dieb” with an upper or lower case “d”. The expression now selects all ways of writing “raub” and “dieb” we allowed for. reports$Report %&gt;% str_detect(pattern = &quot;(R|ger|ber|zur)[aä]ub|[Dd]ieb&quot;) %&gt;% sum() ## [1] 2061 Now that we have constructed a provisional regular expression that can detect reports that are related to robberies, we can also compute their relative proportion. Remember, we are dealing with a logical vector of ones and zeros: sum() counts all ones and thus returns the absolute number of reports and mean() divides the number of ones by the number of all reports, thus returning the relative proportion. reports$Report %&gt;% str_detect(pattern = &quot;(R|ger|ber|zur)[aä]ub|[Dd]ieb&quot;) %&gt;% mean() ## [1] 0.09388666 So about \\(9.4\\)% of police reports deal with robberies if we apply the regular expression constructed above. Remember, that we might still match some words that we do not want to match, while missing others that also relate to robberies. Thus the actual percentage may differ. 13.3 Extracting details from a list of names The website https://www.bundeswahlleiter.de/bundestagswahlen/2021/gewaehlte/bund-99/ lists all German members of parliament in tables, with the first column holding the full names and academic degrees in one character string. A common application of regular expressions in the context of web scraping is to extract substrings of interest from such text. We will first scrape the data. The members of parliament are divided into 26 subpages, one for each first letter of their last names, in the form “a.html”, “b.html” and so on. We construct links to all these pages using the base R object letters, a character vector conveniently containing all 26 letters of the alphabet in lower case. We then read the links and extract the table cells containing the names into a character vector, which is then transformed into a tibble. library(rvest) website &lt;- &quot;https://www.bundeswahlleiter.de/bundestagswahlen/2021/gewaehlte/bund-99/&quot; links &lt;- str_c(website, letters, &quot;.html&quot;) pages &lt;- links %&gt;% map(~ { Sys.sleep(2) read_html(.x) }) names &lt;- pages %&gt;% map(html_elements, css = &quot;th[scope=&#39;row&#39;]&quot;) %&gt;% map(html_text, trim = TRUE) %&gt;% unlist() names_tbl &lt;- tibble( full_name = names ) Our goal is to extract the academic degree from strings of full names. We first need to understand the pattern of how degrees are listed in those strings. Let us look at some members of parliament with and without degrees. To filter a tibble by row numbers, we can conveniently use slice() from dplyr: names_tbl %&gt;% slice(186:189) ## # A tibble: 4 × 1 ## full_name ## &lt;chr&gt; ## 1 Gramling, Fabian Benedikt Meinrad ## 2 Dr. Gräßle, Ingeborg Gabriele ## 3 Prof. Dr. Grau, Armin Jürgen ## 4 Gremmels, Timon If the members of parliament carry an academic degree, the string begins with it and the abbreviated titles always end in a dot. Based on this, we can start constructing a regular expression that will be used to detect and extract academic degrees. We know that if there is a degree, it is listed at the beginning of the string. We can refer to the beginning and ending of a string by using an anchor. Writing ^ in a regular expression refers to the beginning of a string, $ to the ending. Being a predefined class that includes all upper and lower case letters of the alphabet, we will use [:alpha:] to refer to letters in general. \"^[:alpha:]\" will select one upper or lower case letter that is the first character in a string. We do not want to select only one character but complete titles. For this we need to use quantifiers. These are used to define how often elements of an expression are allowed or required to be repeated. If we know the exact number of repetitions, we can set this number contained in {} after an element. \"^[:alpha:]{2}\" will select the first two letters in each string; always exactly two. The actual titles vary in the number of letters they contain. To allow for variation in the number of repetitions we use the quantifiers ?, * and +. ? stands for zero or one, * for zero or multiple and + for one or multiple repetitions, multiple meaning \\(n &gt;=1\\). \"^[:alpha:]+\" will select the word a string begins with. [:alpha:] does not contain any whitespace or punctuation characters. Therefore, the regular expression will begin selecting letters at the beginning of a string until it “hits” a whitespace or punctuation character. For members of parliament without a degree, this would select their last name. To make sure we only select titles, we can make use of the fact that all titles in this data end with a “.”. As the dot is a special character – standing for any character – we have to escape it by using two backslashes. The regular expression \"^[:alpha:]+\\\\.\" selects all words at the beginning of a string that end with a dot. We use it in the function str_extract() which will extract the substring that is specified by the regular expression if it could be matched in the string. We use the function inside of mutate() to assign the extracted degrees into a new column. To get an overview of the result, we group the data by this new column and count the occurrences per group. names_tbl %&gt;% mutate(academic = str_extract(full_name, pattern = &quot;^[:alpha:]+\\\\.&quot;)) %&gt;% group_by(academic) %&gt;% summarise(n()) ## # A tibble: 3 × 2 ## academic `n()` ## &lt;chr&gt; &lt;int&gt; ## 1 Dr. 111 ## 2 Prof. 5 ## 3 &lt;NA&gt; 620 We successfully identified a high percentage of doctors, as well as some professors. Let us also check, how it worked for the four members of parliament we examined above. names_tbl %&gt;% mutate(academic = str_extract(full_name, pattern = &quot;^[:alpha:]+\\\\.&quot;)) %&gt;% slice(186:189) ## # A tibble: 4 × 2 ## full_name academic ## &lt;chr&gt; &lt;chr&gt; ## 1 Gramling, Fabian Benedikt Meinrad &lt;NA&gt; ## 2 Dr. Gräßle, Ingeborg Gabriele Dr. ## 3 Prof. Dr. Grau, Armin Jürgen Prof. ## 4 Gremmels, Timon &lt;NA&gt; Members of parliament without a degree received an NA, which is correct. For observations with one title, it also worked, but for cases with multiple titles – i.e. “Prof. Dr.”, it did not. But we can extend the expression to allow for multiple titles as well. If there are two or more components to a title, they are separated by whitespace; which we can refer to using \\\\s. After the whitespace there will be another word ending in a dot. Thus we can reuse the first part of the expression, without the anchor: \"\\\\s[:alpha:]+\\\\.\". Some members of parliament have one title, others have multiple. We have to allow both. A quantifier that follows a part of an expression grouped by enclosure in parentheses will refer to the complete group. The regular expression \"(\\\\s[:alpha:]+\\\\.)*\" allows for no additional title or for multiple ones. Appending this to the regular expression used in the code above allows us to select and extract one or more titles. names_tbl %&gt;% mutate(academic = str_extract(full_name, pattern = &quot;^[:alpha:]+\\\\.(\\\\s[:alpha:]+\\\\.)*&quot;)) %&gt;% group_by(academic) %&gt;% summarise(n()) ## # A tibble: 3 × 2 ## academic `n()` ## &lt;chr&gt; &lt;int&gt; ## 1 Dr. 111 ## 2 Prof. Dr. 5 ## 3 &lt;NA&gt; 620 Let us confirm that it worked correctly: names_tbl %&gt;% mutate(academic = str_extract(full_name, pattern = &quot;^[:alpha:]+\\\\.(\\\\s[:alpha:]+\\\\.)*&quot;)) %&gt;% slice(186:189) ## # A tibble: 4 × 2 ## full_name academic ## &lt;chr&gt; &lt;chr&gt; ## 1 Gramling, Fabian Benedikt Meinrad &lt;NA&gt; ## 2 Dr. Gräßle, Ingeborg Gabriele Dr. ## 3 Prof. Dr. Grau, Armin Jürgen Prof. Dr. ## 4 Gremmels, Timon &lt;NA&gt; "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
